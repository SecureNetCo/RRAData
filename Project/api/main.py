"""
DataPage FastAPI ì„œë²„ - Vercel ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ ìµœì í™”
"""

# Vercel ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì—ì„œ ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
import sys
import os
from pathlib import Path
import urllib.request
import shutil
import threading

# í˜„ì¬ íŒŒì¼ì˜ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ Python pathì— ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(f"Python pathì— ì¶”ê°€ëœ ê²½ë¡œ: {project_root}")
print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
print(f"sys.path: {sys.path[:3]}...")  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, HTMLResponse, StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, Tuple, Union
import json
import asyncio
import logging
from datetime import datetime
# pandas removed to reduce serverless function size
import tempfile
from urllib.parse import urlparse, quote
from openpyxl import Workbook

# ë¡œê¹… ì„¤ì • - Vercel í™˜ê²½ì— ìµœì í™”
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s',
    force=True  # ê¸°ì¡´ ì„¤ì • ë®ì–´ì“°ê¸°
)

# íŠ¹ì • ëª¨ë“ˆì˜ ê³¼ë„í•œ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING)
logging.getLogger('config.display_config').setLevel(logging.WARNING)  # ì„¤ì • ë¡œë”© ë¡œê·¸ ì¤„ì´ê¸°

logger = logging.getLogger(__name__)

# DuckDB ê¸°ë³¸ ì‚¬ìš© (ëª¨ë“  ê²€ìƒ‰ì— parquet + DuckDB ì‚¬ìš©)
USE_DUCKDB = True  # í•­ìƒ DuckDB ì‚¬ìš©

# Blob íŒŒì¼ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ê²½ë¡œ ì„¤ì •
BLOB_PREFETCH_ROOT = Path("/tmp/datapage_blobs")
PREFETCH_LOCK = threading.Lock()
PREFETCHED_BLOB_FILES: Dict[Tuple[str, Optional[str], str], str] = {}

# ì½œë“œìŠ¤íƒ€íŠ¸ ì •ë³´ ì €ì¥ìš© ì „ì—­ ë³€ìˆ˜
_cold_start_info: Optional[Dict[str, Any]] = None

# ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ Blob í™˜ê²½ë³€ìˆ˜ ë§¤í•‘ (category, result_type, subcategory) â†’ env var
BLOB_ENV_PREFETCH_MAPPING: Dict[Tuple[str, Optional[str], str], str] = {
    # DataA (2025 ê²½ëŸ‰ ë°ì´í„°)
    ("dataA", None, "safetykorea"): "BLOB_URL_DATAA_1_SAFETYKOREA",
    ("dataA", None, "efficiency-rating"): "BLOB_URL_DATAA_3_EFFICIENCY",
    ("dataA", None, "high-efficiency"): "BLOB_URL_DATAA_4_HIGH_EFFICIENCY",
    ("dataA", None, "standby-power"): "BLOB_URL_DATAA_5_STANDBY_POWER",
    ("dataA", None, "approval"): "BLOB_URL_DATAA_6_APPROVAL",
    ("dataA", None, "declaration-details"): "BLOB_URL_DATAA_7_DECLARE",
    ("dataA", None, "kwtc"): "BLOB_URL_DATAA_8_KWTC",
    ("dataA", None, "recall"): "BLOB_URL_DATAA_9_RECALL",
    ("dataA", None, "safetykoreachild"): "BLOB_URL_DATAA_10_SAFETYKOREACHILD",
    ("dataA", None, "rra-cert"): "BLOB_URL_DATAA_11_RRA_CERT",
    ("dataA", None, "rra-self-cert"): "BLOB_URL_DATAA_12_RRA_SELF_CERT",
    ("dataA", None, "safetykoreahome"): "BLOB_URL_DATAA_13_SAFETYKOREAHOME",

    # DataB
    ("dataB", None, "wadiz-makers"): "BLOB_URL_DATAB_2_WADIZ",

    # DataC Success
    ("dataC", "success", "safetykorea"): "BLOB_URL_DATAC_SUCCESS_1_SAFETYKOREA",
    ("dataC", "success", "wadiz-makers"): "BLOB_URL_DATAC_SUCCESS_2_WADIZ",
    ("dataC", "success", "efficiency-rating"): "BLOB_URL_DATAC_SUCCESS_3_EFFICIENCY",
    ("dataC", "success", "high-efficiency"): "BLOB_URL_DATAC_SUCCESS_4_HIGH_EFFICIENCY",
    ("dataC", "success", "standby-power"): "BLOB_URL_DATAC_SUCCESS_5_STANDBY_POWER",
    ("dataC", "success", "approval"): "BLOB_URL_DATAC_SUCCESS_6_APPROVAL",
    ("dataC", "success", "declaration-details"): "BLOB_URL_DATAC_SUCCESS_7_DECLARE",
    ("dataC", "success", "kwtc"): "BLOB_URL_DATAC_SUCCESS_8_KWTC",
    ("dataC", "success", "recall"): "BLOB_URL_DATAC_SUCCESS_9_RECALL",
    ("dataC", "success", "safetykoreachild"): "BLOB_URL_DATAC_SUCCESS_10_SAFETYKOREACHILD",
    ("dataC", "success", "rra-cert"): "BLOB_URL_DATAC_SUCCESS_11_RRA_CERT",
    ("dataC", "success", "rra-self-cert"): "BLOB_URL_DATAC_SUCCESS_12_RRA_SELF_CERT",
    ("dataC", "success", "safetykoreahome"): "BLOB_URL_DATAC_SUCCESS_13_SAFETYKOREAHOME",

    # DataC Failed
    ("dataC", "failed", "safetykorea"): "BLOB_URL_DATAC_FAILED_1_SAFETYKOREA",
    ("dataC", "failed", "wadiz-makers"): "BLOB_URL_DATAC_FAILED_2_WADIZ",
    ("dataC", "failed", "efficiency-rating"): "BLOB_URL_DATAC_FAILED_3_EFFICIENCY",
    ("dataC", "failed", "high-efficiency"): "BLOB_URL_DATAC_FAILED_4_HIGH_EFFICIENCY",
    ("dataC", "failed", "standby-power"): "BLOB_URL_DATAC_FAILED_5_STANDBY_POWER",
    ("dataC", "failed", "approval"): "BLOB_URL_DATAC_FAILED_6_APPROVAL",
    ("dataC", "failed", "declaration-details"): "BLOB_URL_DATAC_FAILED_7_DECLARE",
    ("dataC", "failed", "kwtc"): "BLOB_URL_DATAC_FAILED_8_KWTC",
    ("dataC", "failed", "recall"): "BLOB_URL_DATAC_FAILED_9_RECALL",
    ("dataC", "failed", "safetykoreachild"): "BLOB_URL_DATAC_FAILED_10_SAFETYKOREACHILD",
    ("dataC", "failed", "rra-cert"): "BLOB_URL_DATAC_FAILED_11_RRA_CERT",
    ("dataC", "failed", "rra-self-cert"): "BLOB_URL_DATAC_FAILED_12_RRA_SELF_CERT",
    ("dataC", "failed", "safetykoreahome"): "BLOB_URL_DATAC_FAILED_13_SAFETYKOREAHOME",
}


SUBCATEGORY_ALIAS_MAP: Dict[str, str] = {
    "approval-details": "approval",
    "rra-certification": "rra-cert",
    "rra-self-conformity": "rra-self-cert",
}


def normalize_subcategory(subcategory: Optional[str]) -> Optional[str]:
    if not subcategory:
        return subcategory
    return SUBCATEGORY_ALIAS_MAP.get(subcategory, subcategory)


def _make_prefetch_key(category: str, subcategory: str, result_type: Optional[str] = None) -> Tuple[str, Optional[str], str]:
    normalized_subcategory = normalize_subcategory(subcategory)
    return (category, result_type, normalized_subcategory)


def _inspect_data_source(data_file_path: str) -> Tuple[str, bool, bool, float]:
    """Return path info, whether it's remote, tabular(parquet/duckdb), and size in MB."""

    data_file_str = str(data_file_path)
    is_remote = data_file_str.startswith(('https://', 'http://'))
    is_tabular = data_file_str.lower().endswith(('.parquet', '.duckdb'))

    if is_remote:
        # Blob/R2 URLì€ ì‹¤ì œ í¬ê¸°ë¥¼ ì¡°íšŒí•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì— ëŒ€ìš©ëŸ‰ìœ¼ë¡œ ì·¨ê¸‰
        return data_file_str, True, is_tabular, 100.0

    local_path = Path(data_file_str)
    size_mb = local_path.stat().st_size / (1024 * 1024) if local_path.exists() else 0.0
    return data_file_str, False, is_tabular, size_mb


def _store_prefetched_blob(category: str, subcategory: str, result_type: Optional[str], local_path: str) -> None:
    key = _make_prefetch_key(category, subcategory, result_type)
    with PREFETCH_LOCK:
        PREFETCHED_BLOB_FILES[key] = local_path


def get_prefetched_blob_path(category: str, subcategory: str, result_type: Optional[str] = None) -> Optional[str]:
    key = _make_prefetch_key(category, subcategory, result_type)
    with PREFETCH_LOCK:
        local_path = PREFETCHED_BLOB_FILES.get(key)
    if local_path and os.path.exists(local_path):
        return local_path
    return None


def _derive_blob_filename(url: str, fallback: str) -> str:
    parsed = urlparse(url)
    candidate = Path(parsed.path).name if parsed.path else ""
    if candidate:
        return candidate
    return fallback


def _prefetch_single_blob(category: str, subcategory: str, result_type: Optional[str], url: str) -> Optional[str]:
    try:
        BLOB_PREFETCH_ROOT.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    fallback_name = f"{category}_{result_type or 'default'}_{subcategory}.parquet"
    filename = _derive_blob_filename(url, fallback_name)
    dest_path = BLOB_PREFETCH_ROOT / filename

    if dest_path.exists() and dest_path.stat().st_size > 0:
        logger.info(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì¬ì‚¬ìš©: {filename}")
        _store_prefetched_blob(category, subcategory, result_type, str(dest_path))
        return str(dest_path)

    temp_path = dest_path.with_suffix(dest_path.suffix + ".download")

    try:
        logger.info(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url} â†’ {dest_path}")
        with urllib.request.urlopen(url) as response, open(temp_path, "wb") as out_file:
            shutil.copyfileobj(response, out_file)
        os.replace(temp_path, dest_path)
        _store_prefetched_blob(category, subcategory, result_type, str(dest_path))
        logger.info(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {dest_path}")
        return str(dest_path)
    except Exception as download_error:
        logger.warning(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({url}): {download_error}")
        if temp_path.exists():
            try:
                temp_path.unlink()
            except Exception:
                pass
        return None


async def prefetch_blob_files() -> None:
    global _cold_start_info
    import time

    start_time = time.time()
    data_mode = os.getenv("DATA_MODE", "full").lower()
    if data_mode != "2025":
        logger.info(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ (DATA_MODE={data_mode})")
        return

    if not BLOB_ENV_PREFETCH_MAPPING:
        logger.info("Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ ë§¤í•‘ ì—†ìŒ")
        return

    prefetch_tasks = []
    task_keys: List[Tuple[str, Optional[str], str]] = []

    for key, env_var in BLOB_ENV_PREFETCH_MAPPING.items():
        url = os.getenv(env_var)
        if not url:
            logger.debug(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ: í™˜ê²½ë³€ìˆ˜ {env_var} ë¯¸ì„¤ì •")
            continue
        category, result_type, subcategory = key
        task = asyncio.to_thread(_prefetch_single_blob, category, subcategory, result_type, url)
        prefetch_tasks.append(task)
        task_keys.append(key)

    if not prefetch_tasks:
        logger.info("Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰í•  í•­ëª© ì—†ìŒ")
        return

    results = await asyncio.gather(*prefetch_tasks, return_exceptions=True)
    success_count = 0
    total_files = len(results)

    for key, result in zip(task_keys, results):
        if isinstance(result, Exception):
            logger.warning(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì˜ˆì™¸ {key}: {result}")
        elif result:
            logger.info(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ {key}: {result}")
            success_count += 1
        else:
            logger.warning(f"Blob ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ë¯¸ì™„ë£Œ {key}")

    end_time = time.time()
    duration = end_time - start_time

    # ì½œë“œìŠ¤íƒ€íŠ¸ ì •ë³´ ì„¤ì •
    _cold_start_info = {
        "type": "cold_start_complete",
        "message": f"ğŸš€ ì½œë“œìŠ¤íƒ€íŠ¸ ì™„ë£Œ: {success_count}/{total_files}ê°œ íŒŒì¼, {duration:.2f}ì´ˆ ì†Œìš”",
        "stats": {
            "success_count": success_count,
            "total_files": total_files,
            "duration_seconds": round(duration, 2),
            "timestamp": datetime.now().isoformat()
        }
    }

    logger.info(_cold_start_info["message"])

# ë¡œì»¬ ëª¨ë“ˆ import
from config.search_config import search_config_manager
from config.display_config import display_config_manager, CategoryDisplayConfig, DisplayField, SearchField
from core.large_file_processor import get_processor, stream_search_large_file, SearchContext
from core.duckdb_processor import duckdb_search_large_file


app = FastAPI(title="DataPage API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# **ì„±ëŠ¥ ìµœì í™”: Startup Warming**
@app.on_event("startup")
async def startup_warming():
    """
    ì„œë²„ ì‹œì‘ì‹œ DuckDB ë° ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì‚¬ì „ ë¡œë“œ
    - DuckDB ì—°ê²° ë° httpfs í™•ì¥ ë¡œë“œ
    - ì£¼ìš” parquet íŒŒì¼ë“¤ ë©”íƒ€ë°ì´í„° ìºì‹±
    - ì²« ë²ˆì§¸ ì‚¬ìš©ìì˜ cold start ë°©ì§€
    """
    try:
        logger.info("ğŸ”¥ Startup Warming ì‹œì‘...")

        # ğŸ§¹ ëª¨ë“  ëª¨ë“œì—ì„œ startup ì‹œ /tmp ì •ë¦¬ ìˆ˜í–‰
        clear_success = clear_tmp_cache()
        if clear_success:
            logger.info("ğŸ§¹ Startup /tmp ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

        prefetch_config = get_prefetch_config()
        if prefetch_config["enabled"]:
            logger.info("ğŸ§­ 2025 ëª¨ë“œ: ìŠ¤ë§ˆíŠ¸ í”„ë¦¬í˜ì¹˜ í™œì„±í™”, ì´ˆê¸° ì¼ê´„ ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ")
        else:
            await prefetch_blob_files()

        # ì£¼ìš” ì¹´í…Œê³ ë¦¬ë“¤ì— ëŒ€í•´ ì‘ì€ ì¿¼ë¦¬ ì‹¤í–‰í•˜ì—¬ warming
        # **ì„±ëŠ¥ ìµœì í™”: Startup Warming - ì£¼ìš” ë°ì´í„°ì…‹ ì‚¬ì „ ë¡œë”©**
        warming_categories = [
            ("dataA", "safetykorea"),      # ê°€ì¥ í° íŒŒì¼
            ("dataA", "safetykoreachild"), # ë‘ ë²ˆì§¸ë¡œ í° íŒŒì¼
            ("dataB", "wadiz-makers"),     # ìì£¼ ì‚¬ìš©ë˜ëŠ” íŒŒì¼
        ]

        for category, subcategory in warming_categories:
            try:
                # ê° íŒŒì¼ì— ëŒ€í•´ ìµœì†Œí•œì˜ ì¿¼ë¦¬ ì‹¤í–‰ (limit=1)
                warming_request = SearchRequest(
                    keyword="test",
                    search_field="company_name",
                    page=1,
                    limit=1,
                    filters={}
                )
                warming_result = await search_category_data(category, subcategory, warming_request)
                logger.info(f"âœ… Warming ì™„ë£Œ: {category}_{subcategory}")
            except Exception as e:
                logger.warning(f"âš ï¸ Warming ì‹¤íŒ¨: {category}_{subcategory} - {e}")

        logger.info("ğŸš€ Startup Warming ì™„ë£Œ! ì²« ì‚¬ìš©ì ìš”ì²­ ìµœì í™”ë¨")

    except Exception as e:
        logger.error(f"âŒ Startup Warming ì „ì²´ ì‹¤íŒ¨: {e}")
        # Warming ì‹¤íŒ¨í•´ë„ ì„œë²„ëŠ” ì •ìƒ ì‹œì‘

# Static íŒŒì¼ ê²½ë¡œ ì„¤ì • - Vercelê³¼ ë¡œì»¬ í™˜ê²½ í˜¸í™˜
static_path = project_root / "public" / "static"

# ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ static íŒŒì¼ ë§ˆìš´íŠ¸ (Vercelì—ì„œëŠ” vercel.jsonì´ ì²˜ë¦¬)
if os.getenv("VERCEL") is None:  # ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ
    from fastapi.staticfiles import StaticFiles
    if static_path.exists():
        app.mount("/static", StaticFiles(directory=str(static_path)), name="static")


# ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸
class SearchRequest(BaseModel):
    keyword: Optional[str] = None
    search_field: Optional[str] = "product_name"  # ê²€ìƒ‰ í•„ë“œ: company_name, model_name, product_name ë“± ('all' ì œê±°ë¨)
    date_from: Optional[str] = None
    date_to: Optional[str] = None
    categories: Optional[List[str]] = None
    filters: Optional[Dict[str, Any]] = None  # ìœ ì—°í•œ ì¶”ê°€ í•„í„°
    # ì„œë²„ì‚¬ì´ë“œ í˜ì´ì§€ë„¤ì´ì…˜ íŒŒë¼ë¯¸í„°
    page: Optional[int] = 1  # í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
    limit: Optional[int] = 20  # í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜ (ê¸°ë³¸ 20ê°œ)
    # offsetì€ pageì™€ limitìœ¼ë¡œ ê³„ì‚°ë˜ë¯€ë¡œ ì œê±°
    # offset: Optional[int] = 0

# í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ëª¨ë¸
class PaginationInfo(BaseModel):
    total_count: int        # ì „ì²´ í•­ëª© ìˆ˜
    total_pages: int        # ì „ì²´ í˜ì´ì§€ ìˆ˜
    current_page: int       # í˜„ì¬ í˜ì´ì§€
    items_per_page: int     # í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜
    has_next: bool          # ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€
    has_prev: bool          # ì´ì „ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€

# ê²€ìƒ‰ ì‘ë‹µ ëª¨ë¸ (ì„œë²„ì‚¬ì´ë“œ í˜ì´ì§€ë„¤ì´ì…˜ìš©)
class SearchResponse(BaseModel):
    results: List[Dict[str, Any]]           # í˜„ì¬ í˜ì´ì§€ ê²°ê³¼
    pagination: PaginationInfo              # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´
    summary: Dict[str, Any]                 # ì²˜ë¦¬ ì •ë³´
    available_categories: List[str]         # ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ (í˜¸í™˜ì„±ìš©)

# ë‹¤ìš´ë¡œë“œ ìš”ì²­ ëª¨ë¸
class DownloadRequest(BaseModel):
    search_conditions: Dict[str, Any]
    file_format: str = "xlsx"  # ì—‘ì…€ë§Œ ì§€ì›
    user_session: Optional[str] = None
    filtered_data: Optional[List[Dict[str, Any]]] = None  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í•„í„°ë§ëœ ë°ì´í„°

# ê²€ìƒ‰ ê¸°ë°˜ ì „ì²´ ë‹¤ìš´ë¡œë“œ ìš”ì²­ ëª¨ë¸
class SearchDownloadRequest(BaseModel):
    keyword: str
    search_field: Optional[str] = "all"
    date_from: Optional[str] = None
    date_to: Optional[str] = None
    categories: Optional[List[str]] = None
    filters: Optional[Dict[str, Any]] = None
    file_format: str = "xlsx"
    user_session: Optional[str] = None

@app.get("/")
async def root():
    """ë©”ì¸ í˜ì´ì§€ - search.htmlë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url="/static/search.html", status_code=302)

@app.get("/api/system/status")
async def get_system_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ ë° ì„±ëŠ¥ ì„¤ì • ì •ë³´"""
    return {
        "api_version": "1.0.0",
        "duckdb_enabled": USE_DUCKDB,
        "supported_sources": "ëª¨ë“  ì†ŒìŠ¤ (Vercel Blob Parquet)",
        "performance_features": {
            "duckdb": "8-11ë°° ì„±ëŠ¥ í–¥ìƒ (ëŒ€ìš©ëŸ‰ íŒŒì¼)" if USE_DUCKDB else "ë¹„í™œì„±í™”ë¨",
            "ijson_streaming": "í™œì„±í™”ë¨",
            "large_file_threshold": "50MB"
        },
        "activation_guide": {
            "duckdb": "í™˜ê²½ë³€ìˆ˜ USE_DUCKDB=trueë¡œ ì„¤ì •",
            "github_releases": "1.6GB+ íŒŒì¼ì€ GitHub Releases ì—…ë¡œë“œ ê¶Œì¥"
        },
        "prefetch": get_prefetch_config()
    }


@app.get("/search/{category}/{subcategory}")
async def serve_search_page(category: str, subcategory: str):
    """ê²€ìƒ‰ í˜ì´ì§€ - search.html ë°˜í™˜ (ê¸°ì¡´ ê²½ë¡œ í˜¸í™˜ì„±)"""
    search_path = static_path / "search.html"
    if search_path.exists():
        with open(search_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return HTMLResponse(content=content)
    else:
        raise HTTPException(status_code=404, detail="ê²€ìƒ‰ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

@app.get("/search/dataA/{subcategory}")
async def serve_search_page_data_a(category: str = "dataA", subcategory: str = None):
    """ê²€ìƒ‰ í˜ì´ì§€ - dataA êµ¬ì¡°"""
    search_path = static_path / "search.html"
    if search_path.exists():
        with open(search_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return HTMLResponse(content=content)
    else:
        raise HTTPException(status_code=404, detail="ê²€ìƒ‰ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

@app.get("/search/dataB/{subcategory}")
async def serve_search_page_data_b(category: str = "dataB", subcategory: str = None):
    """ê²€ìƒ‰ í˜ì´ì§€ - dataB êµ¬ì¡°"""
    search_path = static_path / "search.html"
    if search_path.exists():
        with open(search_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return HTMLResponse(content=content)
    else:
        raise HTTPException(status_code=404, detail="ê²€ìƒ‰ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

@app.get("/search/dataC/{result_type}/{subcategory}")
async def serve_search_page_data_c(result_type: str, subcategory: str):
    """ê²€ìƒ‰ í˜ì´ì§€ - dataC êµ¬ì¡° (success/failed)"""
    search_path = static_path / "search.html"
    if search_path.exists():
        with open(search_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return HTMLResponse(content=content)
    else:
        raise HTTPException(status_code=404, detail="ê²€ìƒ‰ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

@app.get("/admin")
async def serve_admin_page():
    """ê´€ë¦¬ì í˜ì´ì§€ - admin.html ë°˜í™˜"""
    admin_path = static_path / "admin.html"
    if admin_path.exists():
        with open(admin_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return HTMLResponse(content=content)
    else:
        raise HTTPException(status_code=404, detail="ê´€ë¦¬ì í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ì½œë“œìŠ¤íƒ€íŠ¸ ì •ë³´ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/cold-start-info")
async def get_cold_start_info():
    """ì½œë“œìŠ¤íƒ€íŠ¸ ì •ë³´ ì¡°íšŒ (ê°œë°œì ë„êµ¬ ì½˜ì†” í‘œì‹œìš©)"""
    global _cold_start_info
    if _cold_start_info:
        return _cold_start_info
    else:
        return {
            "type": "cold_start_pending",
            "message": "â³ ì½œë“œìŠ¤íƒ„íŠ¸ ì§„í–‰ ì¤‘...",
            "stats": None
        }

def clear_tmp_cache():
    """2025 ëª¨ë“œìš© /tmp ìºì‹œ í´ë” ì •ë¦¬"""
    try:
        if BLOB_PREFETCH_ROOT.exists():
            import shutil
            shutil.rmtree(BLOB_PREFETCH_ROOT)
            logger.info(f"ğŸ§¹ /tmp ìºì‹œ í´ë” ì •ë¦¬ ì™„ë£Œ: {BLOB_PREFETCH_ROOT}")

        # ë©”ëª¨ë¦¬ ìƒì˜ ìºì‹œë„ ì •ë¦¬
        global PREFETCHED_BLOB_FILES
        with PREFETCH_LOCK:
            PREFETCHED_BLOB_FILES.clear()

        return True
    except Exception as e:
        logger.warning(f"âš ï¸ /tmp ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False

class SinglePrefetchRequest(BaseModel):
    category: str
    subcategory: str
    result_type: Optional[str] = None

@app.post("/api/prefetch-single")
async def prefetch_single_file(request: SinglePrefetchRequest):
    """2025 ëª¨ë“œìš© ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    import time

    config = get_prefetch_config()
    if not config["enabled"]:
        raise HTTPException(status_code=400, detail="2025 ëª¨ë“œì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤")

    start_time = time.time()

    # 1. /tmp í´ë” ì •ë¦¬
    clear_success = clear_tmp_cache()

    # 2. í™˜ê²½ë³€ìˆ˜ì—ì„œ URL ì°¾ê¸° (ìŠ¬ëŸ¬ê·¸ ì •ê·œí™”)
    normalized_subcategory = normalize_subcategory(request.subcategory)
    key = (request.category, request.result_type, normalized_subcategory)
    env_var = BLOB_ENV_PREFETCH_MAPPING.get(key)

    url = os.getenv(env_var) if env_var else None

    if not url:
        # BLOB í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ R2 URLë¡œ fallback ì‹œë„
        fallback_url = None
        try:
            if request.category == "dataC" and request.result_type:
                fallback_candidate = get_data_file_path_c(
                    request.category,
                    request.result_type,
                    request.subcategory,
                    prefer_r2=True
                )
            else:
                fallback_candidate = get_data_file_path(
                    request.category,
                    request.subcategory,
                    prefer_r2=True
                )

            if isinstance(fallback_candidate, str) and fallback_candidate.startswith("http"):
                fallback_url = fallback_candidate
                logger.info(
                    f"BLOB URL ì—†ìŒ: {request.category}/{request.result_type}/{request.subcategory} â†’ R2 URLë¡œ í”„ë¦¬í˜ì¹˜ ëŒ€ì²´"
                )
        except Exception as fallback_error:
            logger.warning(
                f"í”„ë¦¬í˜ì¹˜ fallback URL í™•ì¸ ì‹¤íŒ¨: {fallback_error}"
            )

        if fallback_url:
            url = fallback_url

    if not url:
        raise HTTPException(
            status_code=404,
            detail=f"í”„ë¦¬í˜ì¹˜ ê°€ëŠ¥í•œ ì›ê²© URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.category}/{request.subcategory}"
        )

    # 3. ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    try:
        logger.info(f"ğŸ¯ ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {request.category}/{request.subcategory}")
        result = await asyncio.to_thread(_prefetch_single_blob, request.category, request.subcategory, request.result_type, url)

        end_time = time.time()
        duration = end_time - start_time

        if result:
            return {
                "success": True,
                "message": f"âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {request.subcategory}",
                "stats": {
                    "category": request.category,
                    "subcategory": request.subcategory,
                    "result_type": request.result_type,
                    "local_path": result,
                    "duration_seconds": round(duration, 2),
                    "cache_cleared": clear_success,
                    "timestamp": datetime.now().isoformat()
                }
            }
        else:
            raise HTTPException(status_code=500, detail="íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

    except Exception as e:
        logger.error(f"âŒ ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@app.get("/api/prefetch/config")
async def get_prefetch_api_config():
    """í”„ë¦¬í˜ì¹˜ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€(2025 ëª¨ë“œ) ì œê³µ"""
    return get_prefetch_config()

@app.post("/api/search/{category}/{subcategory}")
async def search_category_data(category: str, subcategory: str, request: SearchRequest):
    """
    ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ - DuckDB + Parquet ì „ìš©
    """
    try:
        # Parquet ë°ì´í„° íŒŒì¼ URL ê°€ì ¸ì˜¤ê¸°
        data_file_path = get_data_file_path(category, subcategory)
        if not data_file_path:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„° íŒŒì¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {category}/{subcategory}")
        
        # R2 URLì¸ì§€ í™•ì¸í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
        is_r2_url = data_file_path.startswith('https://') 
        if is_r2_url:
            # R2 URLì´ë©´ í•­ìƒ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ (DuckDB) ì‚¬ìš©
            file_size_mb = 100.0  # ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ë¡œì§ì„ íƒ€ë„ë¡ ì„¤ì •
        else:
            # ë¡œì»¬ íŒŒì¼ì´ë©´ ì‹¤ì œ í¬ê¸° í™•ì¸
            from pathlib import Path
            local_path = Path(data_file_path)
            file_size_mb = local_path.stat().st_size / (1024 * 1024) if local_path.exists() else 0
        
        logger.info(f"DuckDB Parquet ì²˜ë¦¬ ì‹œì‘: {category}/{subcategory} ({'R2 URL' if is_r2_url else f'{file_size_mb:.1f}MB'})")

        # DuckDBë¡œ Parquet íŒŒì¼ ê²€ìƒ‰ (í˜ì´ì§€ë„¤ì´ì…˜)
        effective_subcategory = normalize_subcategory(subcategory)

        search_result = await duckdb_search_large_file(
            file_path=str(data_file_path),
            keyword=request.keyword,
            search_field=request.search_field,
            limit=request.limit,
            page=request.page,
            filters=request.filters,
            category=category,
            subcategory=effective_subcategory
        )
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
        if "error" in search_result:
            raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì²˜ë¦¬ ì‹¤íŒ¨: {search_result.get('message')}")
        
        # ìš”ì•½ ì •ë³´ ìƒì„±
        summary = {
            "processing_method": "duckdb_pagination",
            "file_size_mb": round(file_size_mb, 2),
            "processing_stats": search_result.get("stats", {}),
            "duckdb_enabled": True,
            "performance_note": "ì„œë²„ì‚¬ì´ë“œ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ìµœì í™”ëœ ì²˜ë¦¬"
        }

        # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€ (search_resultì—ì„œ ê°€ì ¸ì˜´)
        if "debug_info" in search_result and search_result["debug_info"]:
            summary["debug_info"] = search_result["debug_info"]

        # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ìƒì„±
        pagination_data = search_result.get("pagination", {})
        pagination_info = PaginationInfo(
            total_count=pagination_data.get("total_count", 0),
            total_pages=pagination_data.get("total_pages", 1),
            current_page=pagination_data.get("current_page", 1),
            items_per_page=pagination_data.get("items_per_page", 20),
            has_next=pagination_data.get("has_next", False),
            has_prev=pagination_data.get("has_prev", False)
        )

        return SearchResponse(
            results=search_result.get("results", []),
            pagination=pagination_info,
            summary=summary,
            available_categories=[]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/search/dataA/{subcategory}")
async def search_data_a(subcategory: str, request: SearchRequest):
    """
    dataA ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ - ìƒˆ êµ¬ì¡°
    """
    return await search_category_data("dataA", subcategory, request)

@app.post("/api/search/dataB/{subcategory}")
async def search_data_b(subcategory: str, request: SearchRequest):
    """
    dataB ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ - ìƒˆ êµ¬ì¡°
    """
    return await search_category_data("dataB", subcategory, request)

@app.post("/api/search/dataC/{result_type}/{subcategory}")
async def search_data_c(result_type: str, subcategory: str, request: SearchRequest):
    """
    dataC ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ - ìƒˆ êµ¬ì¡° (success/failed)
    """
    return await search_category_data_c("dataC", result_type, subcategory, request)

async def search_category_data_c(category: str, result_type: str, subcategory: str, request: SearchRequest):
    """
    dataC ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ - DuckDB + Parquet ì „ìš© (3-parameter structure)
    """
    try:
        # Parquet ë°ì´í„° íŒŒì¼ URL ê°€ì ¸ì˜¤ê¸° (3-parameter structure)
        data_file_path = get_data_file_path_c(category, result_type, subcategory)
        if not data_file_path:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„° íŒŒì¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {category}/{result_type}/{subcategory}")
        
        # R2 URLì¸ì§€ í™•ì¸í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
        is_r2_url = data_file_path.startswith('https://') 
        if is_r2_url:
            # R2 URLì´ë©´ í•­ìƒ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ (DuckDB) ì‚¬ìš©
            file_size_mb = 100.0  # ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ë¡œì§ì„ íƒ€ë„ë¡ ì„¤ì •
        else:
            # ë¡œì»¬ íŒŒì¼ì´ë©´ ì‹¤ì œ í¬ê¸° í™•ì¸
            from pathlib import Path
            local_path = Path(data_file_path)
            file_size_mb = local_path.stat().st_size / (1024 * 1024) if local_path.exists() else 0
        
        logger.info(f"DuckDB Parquet ì²˜ë¦¬ ì‹œì‘: {category}/{result_type}/{subcategory} ({'R2 URL' if is_r2_url else f'{file_size_mb:.1f}MB'})")

        # DuckDBë¡œ Parquet íŒŒì¼ ê²€ìƒ‰ (í˜ì´ì§€ë„¤ì´ì…˜)
        effective_subcategory = normalize_subcategory(subcategory)

        search_result = await duckdb_search_large_file(
            file_path=str(data_file_path),
            keyword=request.keyword,
            search_field=request.search_field,
            limit=request.limit,
            page=request.page,
            filters=request.filters,
            category=category,
            subcategory=effective_subcategory,
            result_type=result_type
        )
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
        if "error" in search_result:
            raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì²˜ë¦¬ ì‹¤íŒ¨: {search_result.get('message')}")
        
        # ìš”ì•½ ì •ë³´ ìƒì„±
        summary = {
            "processing_method": "duckdb_pagination",
            "file_size_mb": round(file_size_mb, 2),
            "processing_stats": search_result.get("stats", {}),
            "duckdb_enabled": True,
            "performance_note": "ì„œë²„ì‚¬ì´ë“œ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ìµœì í™”ëœ ì²˜ë¦¬"
        }

        # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€ (search_resultì—ì„œ ê°€ì ¸ì˜´)
        if "debug_info" in search_result and search_result["debug_info"]:
            summary["debug_info"] = search_result["debug_info"]

        # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ìƒì„±
        pagination_data = search_result.get("pagination", {})
        pagination_info = PaginationInfo(
            total_count=pagination_data.get("total_count", 0),
            total_pages=pagination_data.get("total_pages", 1),
            current_page=pagination_data.get("current_page", 1),
            items_per_page=pagination_data.get("items_per_page", 20),
            has_next=pagination_data.get("has_next", False),
            has_prev=pagination_data.get("has_prev", False)
        )

        return SearchResponse(
            results=search_result.get("results", []),
            pagination=pagination_info,
            summary=summary,
            available_categories=[]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/search")
async def search_data(request: SearchRequest):
    """
    ê¸°ë³¸ ê²€ìƒ‰ (í•˜ìœ„ í˜¸í™˜ì„±) - dataA/safetykorea ë°ì´í„° ì‚¬ìš©
    """
    return await search_category_data("dataA", "safetykorea", request)

@app.get("/api/categories")
async def get_categories():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜
    ì„¤ì • íŒŒì¼ì—ì„œ ë™ì ìœ¼ë¡œ ë¡œë“œ ê°€ëŠ¥
    """
    try:
        # ì¹´í…Œê³ ë¦¬ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì¶”í›„ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •)
        categories_config_path = "/tmp/categories_config.json"
        
        # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ (ì„¤ì • íŒŒì¼ì´ ì—†ì„ ê²½ìš°)
        default_categories = {
            "categories": [
                {"id": "tech", "name": "ê¸°ìˆ ", "description": "ê¸°ìˆ  ê´€ë ¨ ë°ì´í„°"},
                {"id": "economy", "name": "ê²½ì œ", "description": "ê²½ì œ ê´€ë ¨ ë°ì´í„°"},
                {"id": "society", "name": "ì‚¬íšŒ", "description": "ì‚¬íšŒ ê´€ë ¨ ë°ì´í„°"},
                {"id": "culture", "name": "ë¬¸í™”", "description": "ë¬¸í™” ê´€ë ¨ ë°ì´í„°"}
            ],
            "searchable_fields": [
                {"field": "title", "name": "ì œëª©", "type": "text"},
                {"field": "content", "name": "ë‚´ìš©", "type": "text"},
                {"field": "date", "name": "ë‚ ì§œ", "type": "date"},
                {"field": "tags", "name": "íƒœê·¸", "type": "array"}
            ]
        }
        
        if os.path.exists(categories_config_path):
            with open(categories_config_path, 'r', encoding='utf-8') as f:
                categories_config = json.load(f)
        else:
            categories_config = default_categories
        
        return categories_config
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¹´í…Œê³ ë¦¬ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/download")
async def request_download(request: DownloadRequest):
    """í”„ë¡ íŠ¸ì—ì„œ ì „ë‹¬í•œ ë°ì´í„°ë¥¼ ì¦‰ì‹œ Excelë¡œ ë°˜í™˜"""
    try:
        if request.filtered_data:
            filtered_data = request.filtered_data
        else:
            filtered_data = await filter_data_by_conditions(request.search_conditions)

        if not filtered_data:
            raise HTTPException(status_code=400, detail="ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

        filename = f"datapage_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        return _stream_excel_from_records(filtered_data, filename=filename)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/download-search/{category}/{subcategory}")
async def request_search_download(
    category: str,
    subcategory: str,
    request: SearchDownloadRequest,
):
    """ê²€ìƒ‰ ì¡°ê±´ ê¸°ë°˜ Excel íŒŒì¼ì„ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë°"""
    try:
        return await _stream_excel_from_query(
            category=category,
            subcategory=subcategory,
            result_type=None,
            keyword=request.keyword,
            search_field=request.search_field,
            filters=request.filters,
            date_from=request.date_from,
            date_to=request.date_to,
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/download-search/dataA/{subcategory}")
async def request_search_download_data_a(
    subcategory: str,
    request: SearchDownloadRequest,
):
    return await request_search_download("dataA", subcategory, request)


@app.post("/api/download-search/dataB/{subcategory}")
async def request_search_download_data_b(
    subcategory: str,
    request: SearchDownloadRequest,
):
    return await request_search_download("dataB", subcategory, request)


@app.post("/api/download-search/dataC/{result_type}/{subcategory}")
async def request_search_download_data_c(
    result_type: str,
    subcategory: str,
    request: SearchDownloadRequest,
):
    """DataC ì¹´í…Œê³ ë¦¬ Excel ë‹¤ìš´ë¡œë“œ (success/failed)"""
    try:
        return await _stream_excel_from_query(
            category="dataC",
            subcategory=subcategory,
            result_type=result_type,
            keyword=request.keyword,
            search_field=request.search_field,
            filters=request.filters,
            date_from=request.date_from,
            date_to=request.date_to,
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"DataC ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


@app.get("/api/file-info/{category}/{subcategory}")
async def get_file_info(category: str, subcategory: str):
    """
    íŒŒì¼ ì •ë³´ ë° ë©”íƒ€ë°ì´í„° ì¡°íšŒ
    """
    try:
        data_file_path = get_data_file_path(category, subcategory)
        if not data_file_path:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„° íŒŒì¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {category}/{subcategory}")
        
        data_file_str, is_r2_url, is_tabular, file_size_mb = _inspect_data_source(data_file_path)

        total_records_value: Union[int, str] = "unknown"

        if is_tabular:
            from core.duckdb_processor import DuckDBProcessor

            effective_subcategory = normalize_subcategory(subcategory)
            processor = DuckDBProcessor(
                data_file_str,
                category=category,
                subcategory=effective_subcategory
            )

            fields: List[str] = []
            try:
                fields = await asyncio.to_thread(processor.get_available_fields)
                sample_result = await processor.search_streaming(limit=3)
                pagination_info = sample_result.get("pagination", {})

                metadata = {
                    "total_records": pagination_info.get("total_count", 0),
                    "fields": fields,
                    "file_size_mb": round(file_size_mb, 2),
                    "sample_data": sample_result.get("results", [])[:3],
                    "is_large_file": True if is_r2_url or file_size_mb > 50 else False,
                    "data_source": "r2_url" if is_r2_url else "parquet"
                }
            except Exception as e:
                logger.warning(f"DuckDB ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                metadata = {
                    "total_records": 0,
                    "fields": fields if 'fields' in locals() and fields else [],
                    "file_size_mb": round(file_size_mb, 2),
                    "sample_data": [],
                    "is_large_file": True if is_r2_url or file_size_mb > 50 else False,
                    "data_source": "r2_url" if is_r2_url else "parquet",
                    "error": str(e)
                }
            finally:
                processor.close()

        elif file_size_mb > 50:
            from core.large_file_processor import get_large_file_metadata
            metadata = await get_large_file_metadata(data_file_path)
            metadata["is_large_file"] = True
        else:
            with open(data_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            metadata = {
                "file_info": {
                    "size_mb": round(file_size_mb, 2),
                    "modified_time": datetime.now().isoformat()
                },
                "estimated_record_count": len(data.get("data", [])),
                "available_fields": list(data.get("data", [{}])[0].keys()) if data.get("data") else [],
                "is_large_file": False
            }
        
        return metadata
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/field-samples/{category}/{subcategory}/{field_name}")
async def get_field_samples(category: str, subcategory: str, field_name: str, limit: int = 100):
    """
    íŠ¹ì • í•„ë“œì˜ ìƒ˜í”Œ ê°’ë“¤ ì¡°íšŒ (í•„í„° ì˜µì…˜ ìƒì„±ìš©)
    """
    try:
        data_file_path = get_data_file_path(category, subcategory)
        if not data_file_path:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„° íŒŒì¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {category}/{subcategory}")
        
        data_file_str, is_r2_url, is_tabular, file_size_mb = _inspect_data_source(data_file_path)

        if is_tabular:
            from core.duckdb_processor import DuckDBProcessor

            effective_subcategory = normalize_subcategory(subcategory)
            processor = DuckDBProcessor(
                data_file_str,
                category=category,
                subcategory=effective_subcategory
            )
            try:
                samples = processor.get_distinct_values(field_name, limit)
            except Exception as e:
                logger.warning(f"DuckDB í•„ë“œ ìƒ˜í”Œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                samples = []
            finally:
                processor.close()
        elif file_size_mb > 50:
            processor = get_processor(data_file_path)
            samples = await processor.get_field_samples(field_name, limit)
        else:
            with open(data_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            samples = set()
            for item in data.get("data", []):
                if field_name in item and item[field_name] is not None:
                    samples.add(item[field_name])
                    if len(samples) >= limit:
                        break
            samples = sorted(list(samples))
        
        return {
            "field_name": field_name,
            "sample_count": len(samples),
            "samples": samples[:limit]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í•„ë“œ ìƒ˜í”Œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/clear-cache")
async def clear_processor_cache():
    """
    ëŒ€ìš©ëŸ‰ íŒŒì¼ í”„ë¡œì„¸ì„œ ìºì‹œ í´ë¦¬ì–´
    """
    try:
        from core.large_file_processor import clear_all_processors
        clear_all_processors()
        return {"message": "ìºì‹œê°€ ì„±ê³µì ìœ¼ë¡œ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìºì‹œ í´ë¦¬ì–´ ì‹¤íŒ¨: {str(e)}")

# ====================================
# í‘œì‹œ ì„¤ì • ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸ë“¤
# ====================================

@app.get("/api/config/{category}/{subcategory}")
async def get_display_config(category: str, subcategory: str):
    """
    ì¹´í…Œê³ ë¦¬ë³„ í‘œì‹œ ì„¤ì • ì¡°íšŒ
    """
    try:
        # get_configëŠ” ì´ì œ í•­ìƒ ì„¤ì •ì„ ë°˜í™˜í•¨ (None ë°˜í™˜ ì—†ìŒ)
        config = display_config_manager.get_config(category, subcategory)
        return display_config_manager.export_client_config(category, subcategory)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.put("/api/config/{category}/{subcategory}")
async def update_display_config(category: str, subcategory: str, config: CategoryDisplayConfig):
    """
    ì¹´í…Œê³ ë¦¬ë³„ í‘œì‹œ ì„¤ì • ì—…ë°ì´íŠ¸
    """
    try:
        display_config_manager.save_config(category, subcategory, config)
        
        return {
            "message": "ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤",
            "config": display_config_manager.export_client_config(category, subcategory)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.patch("/api/config/{category}/{subcategory}")
async def patch_display_config(category: str, subcategory: str, updates: Dict[str, Any]):
    """
    ì¹´í…Œê³ ë¦¬ë³„ í‘œì‹œ ì„¤ì • ë¶€ë¶„ ì—…ë°ì´íŠ¸
    """
    try:
        updated_config = display_config_manager.update_config(category, subcategory, updates)
        
        return {
            "message": "ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤",
            "config": display_config_manager.export_client_config(category, subcategory)
        }
        
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.delete("/api/config/{category}/{subcategory}")
async def delete_display_config(category: str, subcategory: str):
    """
    ì¹´í…Œê³ ë¦¬ë³„ í‘œì‹œ ì„¤ì • ì‚­ì œ (ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ìƒì„±ë¨)
    """
    try:
        display_config_manager.delete_config(category, subcategory)
        
        return {"message": "ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/configs")
async def list_all_configs():
    """
    ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ í‘œì‹œ ì„¤ì • ëª©ë¡ ì¡°íšŒ
    """
    try:
        configs = display_config_manager.list_configs()
        
        result = {}
        for key, config in configs.items():
            category, subcategory = key.split('/', 1)
            result[key] = {
                "category": category,
                "subcategory": subcategory,
                "display_name": config.display_name,
                "description": config.description,
                "display_fields_count": len(config.display_fields),
                "download_fields_count": len(config.download_fields),
                "search_fields_count": len(config.search_fields)
            }
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/config/{category}/{subcategory}/validate")
async def validate_config_against_data(category: str, subcategory: str):
    """
    ì„¤ì •ì´ ì‹¤ì œ ë°ì´í„°ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦
    """
    try:
        validation_result = display_config_manager.validate_fields_against_data(category, subcategory)
        return validation_result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ê²€ì¦ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/config/{category}/{subcategory}/preview")
async def preview_config(category: str, subcategory: str, limit: int = 5):
    """
    ì„¤ì • ê¸°ë°˜ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    """
    try:
        # ì„¤ì • ì¡°íšŒ (get_configëŠ” í•­ìƒ ì„¤ì •ì„ ë°˜í™˜í•¨)
        config = display_config_manager.get_config(category, subcategory)
        
        # ì‹¤ì œ ë°ì´í„° URL ê°€ì ¸ì˜¤ê¸°
        data_file_path = get_data_file_path(category, subcategory)
        if not data_file_path:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„° íŒŒì¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {category}/{subcategory}")
        
        data_file_str, is_r2_url, is_tabular, file_size_mb = _inspect_data_source(data_file_path)

        if is_tabular:
            search_result = await duckdb_search_large_file(
                file_path=data_file_str,
                keyword=None,
                search_field="all",
                limit=limit,
                page=1,
                filters=None,
                category=category,
                subcategory=normalize_subcategory(subcategory)
            )
            preview_data = search_result.get("results", [])
        elif file_size_mb > 50:
            search_result = await stream_search_large_file(
                file_path=data_file_path,
                keyword=None,
                search_field="all",
                filters=None,
                limit=limit,
                offset=0
            )
            preview_data = search_result.get("results", [])
        else:
            # ì¼ë°˜ íŒŒì¼ ì²˜ë¦¬
            with open(data_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            preview_data = data.get("data", [])[:limit]
        
        # ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§ëœ ë°ì´í„° ë°˜í™˜
        filtered_preview = []
        display_field_names = [field.field for field in config.display_fields]
        
        for item in preview_data:
            # resultData êµ¬ì¡° ì²˜ë¦¬
            if "resultData" in item:
                item_data = item["resultData"]
            else:
                item_data = item
            
            # í‘œì‹œ í•„ë“œë§Œ ì¶”ì¶œ
            filtered_item = {}
            for field in config.display_fields:
                filtered_item[field.field] = item_data.get(field.field, "")
            
            filtered_preview.append(filtered_item)
        
        return {
            "config": display_config_manager.export_client_config(category, subcategory),
            "preview_data": filtered_preview,
            "total_preview_count": len(filtered_preview)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/api/config/{category}/{subcategory}/generate")
async def generate_config_from_data(category: str, subcategory: str):
    """
    ì‹¤ì œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì„¤ì • ìƒì„±
    """
    try:
        # ê¸°ì¡´ ì„¤ì • ì‚­ì œ (ìˆë‹¤ë©´)
        try:
            display_config_manager.delete_config(category, subcategory)
        except:
            pass
        
        # ìƒˆ ì„¤ì • ìƒì„± (ìë™ ë¶„ì„)
        new_config = display_config_manager.get_config(category, subcategory)
        
        return {
            "message": "ì„¤ì •ì´ ìë™ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
            "config": display_config_manager.export_client_config(category, subcategory)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìë™ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/api/settings/{category}/{subcategory}")
async def get_field_settings(category: str, subcategory: str):
    """
    í•„ë“œ ì„¤ì • ì¡°íšŒ (í´ë¼ì´ì–¸íŠ¸ìš©) - 2-parameter
    """
    try:
        # get_configëŠ” ì´ì œ í•­ìƒ ì„¤ì •ì„ ë°˜í™˜í•¨ (None ë°˜í™˜ ì—†ìŒ)
        config = display_config_manager.get_config(category, subcategory)
        return display_config_manager.export_client_config(category, subcategory)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/settings/{category}/{result_type}/{subcategory}")
async def get_field_settings_3param(category: str, result_type: str, subcategory: str):
    """
    í•„ë“œ ì„¤ì • ì¡°íšŒ (í´ë¼ì´ì–¸íŠ¸ìš©) - 3-parameter for dataC
    """
    try:
        # dataCì˜ ê²½ìš°: category=dataC, result_type=success, subcategory=safetykorea
        # DisplayConfigManagerê°€ ì´ì œ dataC/success/safetykorea 3-level keyë¥¼ ì§€ì›í•¨
        
        if category == 'dataC':
            # dataCì˜ ê²½ìš°: dataC/success/safetykorea key ì‚¬ìš©
            config = display_config_manager.get_config(f"{category}/{result_type}", subcategory)
            return display_config_manager.export_client_config(f"{category}/{result_type}", subcategory)
        else:
            # ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ëŠ” ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            config = display_config_manager.get_config(category, f"{result_type}/{subcategory}")
            return display_config_manager.export_client_config(category, f"{result_type}/{subcategory}")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨ (3-param): {str(e)}")

@app.put("/api/settings/{category}/{subcategory}")
async def update_field_settings(category: str, subcategory: str, settings: Dict[str, Any]):
    """
    í•„ë“œ ì„¤ì • ì—…ë°ì´íŠ¸
    """
    try:
        # ì„¤ì • ë³€í™˜ ë° ì—…ë°ì´íŠ¸
        config_data = {
            "display_name": settings.get("displayName", ""),
            "description": settings.get("description", ""),
            "display_fields": [],
            "download_fields": settings.get("downloadFields", []),
            "search_fields": [],
            "default_sort_field": settings.get("defaultSort", {}).get("field"),
            "default_sort_order": settings.get("defaultSort", {}).get("order", "asc"),
            "items_per_page": settings.get("pagination", {}).get("itemsPerPage", 20),
            "enable_export": settings.get("features", {}).get("enableExport", True),
            "show_summary": settings.get("features", {}).get("showSummary", True),
            "show_pagination": settings.get("features", {}).get("showPagination", True)
        }
        
        # í‘œì‹œ í•„ë“œ ë³€í™˜
        for field_data in settings.get("displayFields", []):
            display_field = DisplayField(**field_data)
            config_data["display_fields"].append(display_field)
        
        # ê²€ìƒ‰ í•„ë“œ ë³€í™˜
        for field_data in settings.get("searchFields", []):
            search_field = SearchField(**field_data)
            config_data["search_fields"].append(search_field)
        
        # ì„¤ì • ì €ì¥
        config = CategoryDisplayConfig(**config_data)
        display_config_manager.save_config(category, subcategory, config)
        
        return {
            "message": "ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤",
            "config": display_config_manager.export_client_config(category, subcategory)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/settings/preview/{category}/{subcategory}")
async def get_settings_preview(category: str, subcategory: str, limit: int = 5):
    """
    ì„¤ì • ë¯¸ë¦¬ë³´ê¸° (ì‹¤ì œ ë°ì´í„°ì™€ í•¨ê»˜)
    """
    try:
        # ê¸°ì¡´ ë¯¸ë¦¬ë³´ê¸° API í™œìš©
        return await preview_config(category, subcategory, limit)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/api/field-info/{category}/{subcategory}")
async def get_field_information(category: str, subcategory: str):
    """
    ë°ì´í„° íŒŒì¼ì—ì„œ í•„ë“œ ì •ë³´ ì¶”ì¶œ (ì„¤ì • UIìš©)
    """
    try:
        data_file_path = get_data_file_path(category, subcategory)
        if not data_file_path:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„° íŒŒì¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {category}/{subcategory}")
        
        data_file_str, is_r2_url, is_tabular, file_size_mb = _inspect_data_source(data_file_path)

        if is_tabular:
            from core.duckdb_processor import DuckDBProcessor

            effective_subcategory = normalize_subcategory(subcategory)
            processor = DuckDBProcessor(
                data_file_str,
                category=category,
                subcategory=effective_subcategory
            )

            try:
                available_fields = processor.get_available_fields()
                sample_result = await processor.search_streaming(limit=3)
                sample_data = sample_result.get("results", [])
                total_records_value = sample_result.get("pagination", {}).get("total_count", "unknown")
            finally:
                processor.close()
        elif file_size_mb > 50:
            from core.large_file_processor import get_large_file_metadata
            metadata = await get_large_file_metadata(data_file_path)
            available_fields = metadata.get("available_fields", [])
            sample_data = []
            total_records_value = metadata.get("estimated_record_count", "unknown")
        else:
            # JSON êµ¬ì¡° ì²˜ë¦¬
            with open(data_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if data.get("data") and len(data["data"]) > 0:
                first_record = data["data"][0]
                # resultData êµ¬ì¡° ì²˜ë¦¬
                if "resultData" in first_record:
                    first_record = first_record["resultData"]
                available_fields = list(first_record.keys())
                sample_data = data["data"][:3]  # ìƒ˜í”Œ 3ê°œ
            else:
                available_fields = []
                sample_data = []
            total_records_value = len(data.get("data", [])) if data.get("data") else 0
        
        # í•„ë“œ íƒ€ì… ì¶”ë¡ 
        field_types = {}
        for field in available_fields:
            field_types[field] = infer_field_type(sample_data, field)
        
        return {
            "available_fields": available_fields,
            "field_types": field_types,
            "sample_data": sample_data[:3],  # ìƒ˜í”Œ 3ê°œë§Œ
            "total_records": total_records_value,
            "is_large_file": file_size_mb > 50 or is_r2_url,
            "file_size_mb": round(file_size_mb, 2)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í•„ë“œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def get_category_name(category_id: str, categories: List[Dict]) -> str:
    """ì¹´í…Œê³ ë¦¬ IDë¡œ ì´ë¦„ ì°¾ê¸°"""
    for cat in categories:
        if cat.get("id") == category_id:
            return cat.get("name", category_id)
    return category_id

def get_korean_field_mapping(category: str, subcategory: str) -> Dict[str, str]:
    """ì¹´í…Œê³ ë¦¬/ì„œë¸Œì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ ì˜ì–´->í•œê¸€ í•„ë“œëª… ë§¤í•‘ ë°˜í™˜"""
    try:
        normalized_subcategory = normalize_subcategory(subcategory)
        config_path = Path(__file__).parent.parent / "config" / "field_settings.json"
        with open(config_path, 'r', encoding='utf-8') as f:
            field_settings = json.load(f)

        # ì¹´í…Œê³ ë¦¬ë³„ ì„¤ì • í™•ì¸
        if category in field_settings and normalized_subcategory in field_settings[category]:
            display_fields = field_settings[category][normalized_subcategory].get("display_fields", [])
            # field â†’ name ë§¤í•‘ ìƒì„±
            field_mapping = {}
            for field_config in display_fields:
                english_field = field_config.get("field")
                korean_name = field_config.get("name")
                if english_field and korean_name:
                    field_mapping[english_field] = korean_name
            
            return field_mapping
    except Exception as e:
        logger.error(f"í•„ë“œëª… ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    return {}  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

def get_download_fields(category: str, subcategory: str, result_type: Optional[str] = None) -> List[str]:
    """ì¹´í…Œê³ ë¦¬/ì„œë¸Œì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ download_fields ëª©ë¡ ë°˜í™˜"""
    try:
        normalized_subcategory = normalize_subcategory(subcategory)
        config_path = Path(__file__).parent.parent / "config" / "field_settings.json"
        with open(config_path, 'r', encoding='utf-8') as f:
            field_settings = json.load(f)

        category_config = field_settings.get(category, {})

        if category == "dataC":
            bucket_key = result_type or "success"
            category_config = category_config.get(bucket_key, {})

        if normalized_subcategory in category_config:
            download_fields = category_config[normalized_subcategory].get("download_fields", [])
            return download_fields
    except Exception as e:
        logger.error(f"download_fields ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def filter_data_by_download_fields(data: List[Dict], download_fields: List[str]) -> List[Dict]:
    """ë°ì´í„°ë¥¼ download_fieldsì— ì§€ì •ëœ í•„ë“œë§Œ í¬í•¨í•˜ë„ë¡ í•„í„°ë§"""
    if not download_fields or not data:
        return data

    filtered_data = []
    for item in data:
        filtered_item = {}
        for field in download_fields:
            if field in item:
                filtered_item[field] = item[field]
        filtered_data.append(filtered_item)

    return filtered_data


def _normalize_excel_value(value: Any) -> Any:
    """ì—‘ì…€ ì¶œë ¥ìš© ê°’ ì „ì²˜ë¦¬"""
    if value is None:
        return ""
    if isinstance(value, list):
        return ", ".join(str(v) for v in value)
    if isinstance(value, dict):
        try:
            return json.dumps(value, ensure_ascii=False)
        except Exception:
            return str(value)
    return value


async def _stream_excel_from_query(
    *,
    category: str,
    subcategory: str,
    result_type: Optional[str],
    keyword: Optional[str],
    search_field: Optional[str],
    filters: Optional[Dict[str, Any]],
    date_from: Optional[str],
    date_to: Optional[str],
) -> StreamingResponse:
    """DuckDB ì¡°íšŒ ê²°ê³¼ë¥¼ ì¦‰ì‹œ Excelë¡œ ìŠ¤íŠ¸ë¦¬ë°"""

    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸
    if category == "dataC" and result_type:
        data_file_path = get_data_file_path_c(category, result_type, subcategory)
    else:
        data_file_path = get_data_file_path(category, subcategory)

    effective_subcategory = normalize_subcategory(subcategory)

    korean_field_mapping = get_korean_field_mapping(category, subcategory)
    download_fields = get_download_fields(category, subcategory, result_type)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
        temp_path = Path(tmp.name)

    total_records = 0
    header_written = False
    selected_columns: List[str] = []

    workbook = Workbook(write_only=True)
    worksheet = workbook.create_sheet(title="ê²€ìƒ‰ ê²°ê³¼")

    def handle_chunk(chunk: List[Dict[str, Any]], total_processed: int):
        nonlocal header_written, selected_columns, total_records
        if not chunk:
            return

        if not header_written:
            if download_fields:
                selected_columns = download_fields
            else:
                selected_columns = list(chunk[0].keys())

            header_labels = [korean_field_mapping.get(col, col) for col in selected_columns]
            worksheet.append(header_labels)
            header_written = True

        for record in chunk:
            row = [_normalize_excel_value(record.get(col)) for col in selected_columns]
            worksheet.append(row)

        total_records = total_processed

    query_result = await duckdb_search_large_file(
        file_path=str(data_file_path),
        keyword=keyword,
        search_field=search_field,
        limit=None,
        page=1,
        filters=filters,
        category=category,
        subcategory=effective_subcategory,
        result_type=result_type,
        collect_results=False,
        chunk_callback=handle_chunk,
        chunk_size=1000,
        required_fields=download_fields
    )

    if query_result.get("error"):
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {query_result.get('message')}")

    if not header_written:
        try:
            temp_path.unlink(missing_ok=True)
        except Exception:
            pass
        raise HTTPException(status_code=404, detail="ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    workbook.save(temp_path)
    workbook.close()

    filename = f"datapage_{effective_subcategory}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

    def file_iterator():
        try:
            with temp_path.open('rb') as f:
                for chunk in iter(lambda: f.read(64 * 1024), b''):
                    yield chunk
        finally:
            try:
                temp_path.unlink(missing_ok=True)
            except Exception:
                pass

    response = StreamingResponse(
        file_iterator(),
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
    response.headers["Content-Disposition"] = f"attachment; filename*=UTF-8''{quote(filename)}"
    response.headers["X-Total-Count"] = str(query_result.get("pagination", {}).get("total_count", total_records))
    return response


def _stream_excel_from_records(records: List[Dict[str, Any]], *, filename: Optional[str] = None) -> StreamingResponse:
    """í”„ë¡ íŠ¸ì—ì„œ ì „ë‹¬ëœ ì†ŒëŸ‰ ë°ì´í„°ë¥¼ ì¦‰ì‹œ Excelë¡œ ë°˜í™˜"""
    if not records:
        raise HTTPException(status_code=400, detail="ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
        temp_path = Path(tmp.name)

    workbook = Workbook(write_only=True)
    worksheet = workbook.create_sheet(title="ê²€ìƒ‰ ê²°ê³¼")

    headers = list(records[0].keys())
    worksheet.append(headers)

    for record in records:
        row = [_normalize_excel_value(record.get(col)) for col in headers]
        worksheet.append(row)

    workbook.save(temp_path)
    workbook.close()

    if not filename:
        filename = f"datapage_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

    def file_iterator():
        try:
            with temp_path.open('rb') as f:
                for chunk in iter(lambda: f.read(64 * 1024), b''):
                    yield chunk
        finally:
            try:
                temp_path.unlink(missing_ok=True)
            except Exception:
                pass

    response = StreamingResponse(
        file_iterator(),
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
    response.headers["Content-Disposition"] = f"attachment; filename*=UTF-8''{quote(filename)}"
    response.headers["X-Total-Count"] = str(len(records))
    return response

def get_data_file_path_c(category: str, result_type: str, subcategory: str, prefer_r2: bool = False) -> str:
    """DataC ì¹´í…Œê³ ë¦¬ìš© 3-parameter ë°ì´í„° íŒŒì¼ ê²½ë¡œ ìƒì„± (DATA_MODE í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ë“€ì–¼ ëª¨ë“œ)"""

    normalized_subcategory = normalize_subcategory(subcategory)
    if normalized_subcategory != subcategory:
        logger.info(
            f"ì„œë¸Œì¹´í…Œê³ ë¦¬ ì •ê·œí™”: {category}/{result_type}/{subcategory} â†’ {normalized_subcategory}"
        )
        subcategory = normalized_subcategory

    # ğŸ¯ DATA_MODE í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë“œ ê²°ì •
    data_mode = os.getenv("DATA_MODE", "full").lower()

    # Vercel Blob URL ë§¤í•‘ (DataC 2025ë…„ í•„í„°ë§ëœ ë°ì´í„°) - í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    blob_env_mapping_c = {
        # DataC Success ë§¤í•‘
        ("success", "safetykorea"): "BLOB_URL_DATAC_SUCCESS_1_SAFETYKOREA",
        ("success", "wadiz-makers"): "BLOB_URL_DATAC_SUCCESS_2_WADIZ",
        ("success", "efficiency-rating"): "BLOB_URL_DATAC_SUCCESS_3_EFFICIENCY",
        ("success", "high-efficiency"): "BLOB_URL_DATAC_SUCCESS_4_HIGH_EFFICIENCY",
        ("success", "standby-power"): "BLOB_URL_DATAC_SUCCESS_5_STANDBY_POWER",
        ("success", "approval"): "BLOB_URL_DATAC_SUCCESS_6_APPROVAL",
        ("success", "declaration-details"): "BLOB_URL_DATAC_SUCCESS_7_DECLARE",
        ("success", "kwtc"): "BLOB_URL_DATAC_SUCCESS_8_KWTC",
        ("success", "recall"): "BLOB_URL_DATAC_SUCCESS_9_RECALL",
        ("success", "safetykoreachild"): "BLOB_URL_DATAC_SUCCESS_10_SAFETYKOREACHILD",
        ("success", "rra-cert"): "BLOB_URL_DATAC_SUCCESS_11_RRA_CERT",
        ("success", "rra-self-cert"): "BLOB_URL_DATAC_SUCCESS_12_RRA_SELF_CERT",
        ("success", "safetykoreahome"): "BLOB_URL_DATAC_SUCCESS_13_SAFETYKOREAHOME",

        # DataC Failed ë§¤í•‘
        ("failed", "safetykorea"): "BLOB_URL_DATAC_FAILED_1_SAFETYKOREA",
        ("failed", "wadiz-makers"): "BLOB_URL_DATAC_FAILED_2_WADIZ",
        ("failed", "efficiency-rating"): "BLOB_URL_DATAC_FAILED_3_EFFICIENCY",
        ("failed", "high-efficiency"): "BLOB_URL_DATAC_FAILED_4_HIGH_EFFICIENCY",
        ("failed", "standby-power"): "BLOB_URL_DATAC_FAILED_5_STANDBY_POWER",
        ("failed", "approval"): "BLOB_URL_DATAC_FAILED_6_APPROVAL",
        ("failed", "declaration-details"): "BLOB_URL_DATAC_FAILED_7_DECLARE",
        ("failed", "kwtc"): "BLOB_URL_DATAC_FAILED_8_KWTC",
        ("failed", "recall"): "BLOB_URL_DATAC_FAILED_9_RECALL",
        ("failed", "safetykoreachild"): "BLOB_URL_DATAC_FAILED_10_SAFETYKOREACHILD",
        ("failed", "rra-cert"): "BLOB_URL_DATAC_FAILED_11_RRA_CERT",
        ("failed", "rra-self-cert"): "BLOB_URL_DATAC_FAILED_12_RRA_SELF_CERT",
        ("failed", "safetykoreahome"): "BLOB_URL_DATAC_FAILED_13_SAFETYKOREAHOME"
    }

    # ë¡œì»¬ parquet íŒŒì¼ ê²½ë¡œ ë§¤í•‘ (2025ë…„ í•„í„°ë§ëœ ë°ì´í„°)
    local_file_mapping = {
        "safetykorea": "./parquet/1_safetykorea_flattened.parquet",
        "kwtc": "./parquet/8_kwtc_flattened.parquet",
        "rra-cert": "./parquet/11_rra_cert_flattened.parquet",
        "rra-self-cert": "./parquet/12_rra_self_cert_flattened.parquet",
        "efficiency-rating": "./parquet/3_efficiency_flattened.parquet",
        "high-efficiency": "./parquet/4_high_efficiency_flattened.parquet",
        "standby-power": "./parquet/5_standby_power_flattened.parquet",
        "approval": "./parquet/6_approval_flattened.parquet",
        "declaration-details": "./parquet/7_declare_flattened.parquet",
        "recall": "./parquet/9_recall_flattened.parquet",
        "safetykoreachild": "./parquet/10_safetykoreachild_flattened.parquet",
        "safetykoreahome": "./parquet/13_safetykoreahome_flattened.parquet",
        "wadiz-makers": "./parquet/2_wadiz_flattened.parquet",
    }

    # ğŸŸ¢ 2025ë…„ ëª¨ë“œ: Vercel Blob URL ìš°ì„  ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
    if not prefer_r2 and data_mode == "2025":
        # 1. Vercel Blob URL ì‚¬ìš© (í™˜ê²½ë³€ìˆ˜ì—ì„œ)
        blob_env_var = blob_env_mapping_c.get((result_type, subcategory))
        if blob_env_var:
            blob_url = os.getenv(blob_env_var)
            if blob_url:
                logger.info(f"2025ë…„ ëª¨ë“œ (Blob): {category}/{result_type}/{subcategory} â†’ {blob_url}")
                return blob_url
            else:
                logger.warning(f"Blob í™˜ê²½ë³€ìˆ˜ ì—†ìŒ: {blob_env_var}, prefetchë¡œ fallback")

        # 2. Prefetch ì‹œìŠ¤í…œ fallback
        prefetched_path = get_prefetched_blob_path(category, subcategory, result_type)
        if prefetched_path:
            logger.info(f"2025ë…„ ëª¨ë“œ (Blob-prefetch): {category}/{result_type}/{subcategory} â†’ {prefetched_path}")
            return prefetched_path

        local_parquet_path = local_file_mapping.get(subcategory, "./parquet/1_safetykorea_flattened.parquet")

        # Vercel í™˜ê²½ì—ì„œ ì ˆëŒ€ê²½ë¡œë„ ì‹œë„
        if not os.path.exists(local_parquet_path):
            # ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ ì ˆëŒ€ê²½ë¡œ ì‹œë„
            abs_parquet_path = os.path.abspath(local_parquet_path)
            if os.path.exists(abs_parquet_path):
                logger.info(f"2025ë…„ ëª¨ë“œ (DataC-ì ˆëŒ€ê²½ë¡œ): {category}/{result_type}/{subcategory} â†’ {abs_parquet_path}")
                return abs_parquet_path

            # Project/ í•˜ìœ„ ê²½ë¡œ ì‹œë„
            project_parquet_path = f"Project/{local_parquet_path}"
            if os.path.exists(project_parquet_path):
                logger.info(f"2025ë…„ ëª¨ë“œ (DataC-Project/): {category}/{result_type}/{subcategory} â†’ {project_parquet_path}")
                return project_parquet_path

        if os.path.exists(local_parquet_path):
            logger.info(f"2025ë…„ ëª¨ë“œ (DataC): {category}/{result_type}/{subcategory} â†’ {local_parquet_path}")
            return local_parquet_path
        else:
            # 2025ë…„ ëª¨ë“œì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ R2ë¡œ fallback
            logger.warning(f"2025ë…„ ëª¨ë“œ (DataC): íŒŒì¼ ì—†ìŒ {local_parquet_path}, R2 ëª¨ë“œë¡œ fallback")
            # R2 ëª¨ë“œë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì•„ë˜ R2 ë¡œì§ìœ¼ë¡œ ì§„í–‰

    # ğŸ”µ ì „ì²´ ë°ì´í„° ëª¨ë“œ: R2 URL ì‚¬ìš© (ê¸°ë³¸ê°’, í”„ë¡œë•ì…˜)
    r2_url_mapping = {
        # DataC Success êµ¬ì¡° ë§¤í•‘
        ("dataC", "success", "safetykorea"): os.getenv("R2_URL_DATAC_SUCCESS_1_SAFETYKOREA"),
        ("dataC", "success", "wadiz-makers"): os.getenv("R2_URL_DATAC_SUCCESS_2_WADIZ"),
        ("dataC", "success", "efficiency-rating"): os.getenv("R2_URL_DATAC_SUCCESS_3_EFFICIENCY"),
        ("dataC", "success", "high-efficiency"): os.getenv("R2_URL_DATAC_SUCCESS_4_HIGH_EFFICIENCY"),
        ("dataC", "success", "standby-power"): os.getenv("R2_URL_DATAC_SUCCESS_5_STANDBY_POWER"),
        ("dataC", "success", "approval"): os.getenv("R2_URL_DATAC_SUCCESS_6_APPROVAL"),
        ("dataC", "success", "declaration-details"): os.getenv("R2_URL_DATAC_SUCCESS_7_DECLARE"),
        ("dataC", "success", "kwtc"): os.getenv("R2_URL_DATAC_SUCCESS_8_KWTC"),
        ("dataC", "success", "recall"): os.getenv("R2_URL_DATAC_SUCCESS_9_RECALL"),
        ("dataC", "success", "safetykoreachild"): os.getenv("R2_URL_DATAC_SUCCESS_10_SAFETYKOREACHILD"),
        ("dataC", "success", "safetykoreahome"): os.getenv("R2_URL_DATAC_SUCCESS_13_SAFETYKOREAHOME"),
        ("dataC", "success", "rra-cert"): os.getenv("R2_URL_DATAC_SUCCESS_11_RRA_CERT"),
        ("dataC", "success", "rra-self-cert"): os.getenv("R2_URL_DATAC_SUCCESS_12_RRA_SELF_CERT"),

        # DataC Failed êµ¬ì¡° ë§¤í•‘
        ("dataC", "failed", "safetykorea"): os.getenv("R2_URL_DATAC_FAILED_1_SAFETYKOREA"),
        ("dataC", "failed", "wadiz-makers"): os.getenv("R2_URL_DATAC_FAILED_2_WADIZ"),
        ("dataC", "failed", "efficiency-rating"): os.getenv("R2_URL_DATAC_FAILED_3_EFFICIENCY"),
        ("dataC", "failed", "high-efficiency"): os.getenv("R2_URL_DATAC_FAILED_4_HIGH_EFFICIENCY"),
        ("dataC", "failed", "standby-power"): os.getenv("R2_URL_DATAC_FAILED_5_STANDBY_POWER"),
        ("dataC", "failed", "approval"): os.getenv("R2_URL_DATAC_FAILED_6_APPROVAL"),
        ("dataC", "failed", "declaration-details"): os.getenv("R2_URL_DATAC_FAILED_7_DECLARE"),
        ("dataC", "failed", "kwtc"): os.getenv("R2_URL_DATAC_FAILED_8_KWTC"),
        ("dataC", "failed", "recall"): os.getenv("R2_URL_DATAC_FAILED_9_RECALL"),
        ("dataC", "failed", "safetykoreachild"): os.getenv("R2_URL_DATAC_FAILED_10_SAFETYKOREACHILD"),
        ("dataC", "failed", "safetykoreahome"): os.getenv("R2_URL_DATAC_FAILED_13_SAFETYKOREAHOME"),
        ("dataC", "failed", "rra-cert"): os.getenv("R2_URL_DATAC_FAILED_11_RRA_CERT"),
        ("dataC", "failed", "rra-self-cert"): os.getenv("R2_URL_DATAC_FAILED_12_RRA_SELF_CERT"),
    }

    r2_url = r2_url_mapping.get((category, result_type, subcategory))

    # ë¡œì»¬ ê°œë°œ í™˜ê²½ fallback (VERCEL í™˜ê²½ë³€ìˆ˜ ì—†ì„ ë•Œ)
    if not r2_url and os.getenv("VERCEL") is None:
        local_parquet_path = local_file_mapping.get(subcategory, "./parquet/1_safetykorea_flattened.parquet")
        if os.path.exists(local_parquet_path):
            logger.info(f"ë¡œì»¬ ê°œë°œ í™˜ê²½ (DataC): {category}/{result_type}/{subcategory} â†’ {local_parquet_path}")
            return local_parquet_path
        else:
            logger.info(f"ë¡œì»¬ ê°œë°œ í™˜ê²½ (DataC): ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©")
            return "./parquet/1_safetykorea_flattened.parquet"

    if not r2_url:
        # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ë¡œì»¬ íŒŒì¼ í›„ë³´ë¡œ ë§ˆì§€ë§‰ ì‹œë„
        fallback_candidates = []
        local_candidate = local_file_mapping.get(subcategory)
        if local_candidate:
            fallback_candidates.extend([
                local_candidate,
                os.path.abspath(local_candidate),
                f"Project/{local_candidate}"
            ])

        for candidate in fallback_candidates:
            if candidate and os.path.exists(candidate):
                logger.warning(
                    f"R2 URL ëˆ„ë½: {category}/{result_type}/{subcategory} - ë¡œì»¬ íŒŒì¼ë¡œ ëŒ€ì²´ ({candidate})"
                )
                return candidate

        raise ValueError(f"R2 URL not found for {category}/{result_type}/{subcategory}. Check environment variables.")

    logger.info(f"ì „ì²´ ë°ì´í„° ëª¨ë“œ (DataC): {category}/{result_type}/{subcategory} â†’ R2")
    return r2_url

def get_data_file_path(category: str, subcategory: str, prefer_r2: bool = False) -> str:
    """ì¹´í…Œê³ ë¦¬ì™€ ì„œë¸Œì¹´í…Œê³ ë¦¬ë¡œ ë°ì´í„° íŒŒì¼ ê²½ë¡œ ìƒì„± (DATA_MODE í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ë“€ì–¼ ëª¨ë“œ)"""

    normalized_subcategory = normalize_subcategory(subcategory)
    if normalized_subcategory != subcategory:
        logger.info(
            f"ì„œë¸Œì¹´í…Œê³ ë¦¬ ì •ê·œí™”: {category}/{subcategory} â†’ {normalized_subcategory}"
        )
        subcategory = normalized_subcategory

    # ğŸ¯ DATA_MODE í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë“œ ê²°ì •
    data_mode = os.getenv("DATA_MODE", "full").lower()

    # Vercel Blob URL ë§¤í•‘ (2025ë…„ í•„í„°ë§ëœ ë°ì´í„°) - í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    blob_env_mapping = {
        # DataA ë§¤í•‘ (12ê°œ)
        "safetykorea": "BLOB_URL_DATAA_1_SAFETYKOREA",
        "efficiency-rating": "BLOB_URL_DATAA_3_EFFICIENCY",
        "high-efficiency": "BLOB_URL_DATAA_4_HIGH_EFFICIENCY",
        "standby-power": "BLOB_URL_DATAA_5_STANDBY_POWER",
        "approval": "BLOB_URL_DATAA_6_APPROVAL",
        "declaration-details": "BLOB_URL_DATAA_7_DECLARE",
        "kwtc": "BLOB_URL_DATAA_8_KWTC",
        "recall": "BLOB_URL_DATAA_9_RECALL",
        "safetykoreachild": "BLOB_URL_DATAA_10_SAFETYKOREACHILD",
        "rra-cert": "BLOB_URL_DATAA_11_RRA_CERT",
        "rra-self-cert": "BLOB_URL_DATAA_12_RRA_SELF_CERT",
        "safetykoreahome": "BLOB_URL_DATAA_13_SAFETYKOREAHOME",

        # DataB ë§¤í•‘ (1ê°œ)
        "wadiz-makers": "BLOB_URL_DATAB_2_WADIZ",
    }

    # ë¡œì»¬ parquet íŒŒì¼ ê²½ë¡œ ë§¤í•‘ (2025ë…„ í•„í„°ë§ëœ ë°ì´í„°)
    local_file_mapping = {
        # DataA ë§¤í•‘
        "safetykorea": "./parquet/1_safetykorea_flattened.parquet",
        "kwtc": "./parquet/8_kwtc_flattened.parquet",
        "rra-cert": "./parquet/11_rra_cert_flattened.parquet",     # RRA ì¸ì¦
        "rra-self-cert": "./parquet/12_rra_self_cert_flattened.parquet",  # RRA ìê¸°ì í•©ì„±
        "efficiency-rating": "./parquet/3_efficiency_flattened.parquet",
        "high-efficiency": "./parquet/4_high_efficiency_flattened.parquet",
        "standby-power": "./parquet/5_standby_power_flattened.parquet",
        "approval": "./parquet/6_approval_flattened.parquet",           # ìŠ¹ì¸ì •ë³´
        "declaration-details": "./parquet/7_declare_flattened.parquet",
        "recall": "./parquet/9_recall_flattened.parquet",
        "safetykoreachild": "./parquet/10_safetykoreachild_flattened.parquet",
        "safetykoreahome": "./parquet/13_safetykoreahome_flattened.parquet",
        # DataB ë§¤í•‘
        "wadiz-makers": "./parquet/2_wadiz_flattened.parquet",  # ì™€ë””ì¦ˆ ë©”ì´ì»¤
    }

    # ğŸŸ¢ 2025ë…„ ëª¨ë“œ: Vercel Blob URL ìš°ì„  ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
    if not prefer_r2 and data_mode == "2025":
        prefetched_path = get_prefetched_blob_path(category, subcategory)
        if prefetched_path:
            logger.info(f"2025ë…„ ëª¨ë“œ (Blob-prefetch): {category}/{subcategory} â†’ {prefetched_path}")
            return prefetched_path

        # 1. Vercel Blob URL ì‚¬ìš© (í™˜ê²½ë³€ìˆ˜ì—ì„œ)
        blob_env_var = blob_env_mapping.get(subcategory)
        if blob_env_var:
            blob_url = os.getenv(blob_env_var)
            if blob_url:
                logger.info(f"2025ë…„ ëª¨ë“œ (Blob): {category}/{subcategory} â†’ {blob_url}")
                return blob_url
            else:
                logger.warning(f"Blob í™˜ê²½ë³€ìˆ˜ ì—†ìŒ: {blob_env_var}, ë¡œì»¬ íŒŒì¼ë¡œ fallback")

        # 2. ë¡œì»¬ íŒŒì¼ fallback
        local_parquet_path = local_file_mapping.get(subcategory, "./parquet/1_safetykorea_flattened.parquet")

        # Vercel í™˜ê²½ì—ì„œ ì ˆëŒ€ê²½ë¡œë„ ì‹œë„
        if not os.path.exists(local_parquet_path):
            # ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ ì ˆëŒ€ê²½ë¡œ ì‹œë„
            abs_parquet_path = os.path.abspath(local_parquet_path)
            if os.path.exists(abs_parquet_path):
                logger.info(f"2025ë…„ ëª¨ë“œ (ì ˆëŒ€ê²½ë¡œ): {category}/{subcategory} â†’ {abs_parquet_path}")
                return abs_parquet_path

            # Project/ í•˜ìœ„ ê²½ë¡œ ì‹œë„
            project_parquet_path = f"Project/{local_parquet_path}"
            if os.path.exists(project_parquet_path):
                logger.info(f"2025ë…„ ëª¨ë“œ (Project/): {category}/{subcategory} â†’ {project_parquet_path}")
                return project_parquet_path

        if os.path.exists(local_parquet_path):
            logger.info(f"2025ë…„ ëª¨ë“œ: {category}/{subcategory} â†’ {local_parquet_path}")
            return local_parquet_path
        else:
            # 2025ë…„ ëª¨ë“œì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ R2ë¡œ fallback
            logger.warning(f"2025ë…„ ëª¨ë“œ: íŒŒì¼ ì—†ìŒ {local_parquet_path}, R2 ëª¨ë“œë¡œ fallback")
            # R2 ëª¨ë“œë¡œ ì²˜ë¦¬í•˜ë„ë¡ data_mode ë³€ê²½í•˜ì§€ ì•Šê³  ì•„ë˜ R2 ë¡œì§ìœ¼ë¡œ ì§„í–‰

    # ğŸ”µ ì „ì²´ ë°ì´í„° ëª¨ë“œ: R2 URL ì‚¬ìš© (ê¸°ë³¸ê°’, í”„ë¡œë•ì…˜)
    r2_url_mapping = {
        # DataA êµ¬ì¡° ë§¤í•‘ - ìƒˆë¡œìš´ í™˜ê²½ë³€ìˆ˜ ì´ë¦„ ì‚¬ìš©
        ("dataA", "safetykorea"): os.getenv("R2_URL_DATAA_1_SAFETYKOREA"),
        ("dataA", "kwtc"): os.getenv("R2_URL_DATAA_8_KWTC"),
        ("dataA", "rra-cert"): os.getenv("R2_URL_DATAA_11_RRA_CERT"),     # RRA ì¸ì¦
        ("dataA", "rra-self-cert"): os.getenv("R2_URL_DATAA_12_RRA_SELF_CERT"),  # RRA ìê¸°ì í•©ì„±
        ("dataA", "efficiency-rating"): os.getenv("R2_URL_DATAA_3_EFFICIENCY"), # íš¨ìœ¨ë“±ê¸‰
        ("dataA", "high-efficiency"): os.getenv("R2_URL_DATAA_4_HIGH_EFFICIENCY"),        # ê³ íš¨ìœ¨ê¸°ê¸°
        ("dataA", "standby-power"): os.getenv("R2_URL_DATAA_5_STANDBY_POWER"),      # ëŒ€ê¸°ì „ë ¥
        ("dataA", "approval"): os.getenv("R2_URL_DATAA_6_APPROVAL"),           # ìŠ¹ì¸ì •ë³´
        ("dataA", "declaration-details"): os.getenv("R2_URL_DATAA_7_DECLARE"),     # ì‹ ê³ ì •ë³´
        ("dataA", "recall"): os.getenv("R2_URL_DATAA_9_RECALL"),               # ë¦¬ì½œì •ë³´(êµ­ë‚´)
        ("dataA", "safetykoreachild"): os.getenv("R2_URL_DATAA_10_SAFETYKOREACHILD"),  # ì–´ë¦°ì´ìš©í’ˆ ì¸ì¦ì •ë³´
        ("dataA", "safetykoreahome"): os.getenv("R2_URL_DATAA_13_SAFETYKOREAHOME"),  # ìƒí™œìš©í’ˆ

        # DataB êµ¬ì¡° ë§¤í•‘ - DataAì™€ ë™ì¼í•œ íŒŒì¼ ì‚¬ìš©
        ("dataB", "wadiz-makers"): os.getenv("R2_URL_DATAB_2_WADIZ"),              # ì™€ë””ì¦ˆ ë©”ì´ì»¤

        # DataC Success êµ¬ì¡° ë§¤í•‘
        ("dataC", "success", "safetykorea"): os.getenv("R2_URL_DATAC_SUCCESS_1_SAFETYKOREA"),
        ("dataC", "success", "wadiz-makers"): os.getenv("R2_URL_DATAC_SUCCESS_2_WADIZ"),
        ("dataC", "success", "efficiency-rating"): os.getenv("R2_URL_DATAC_SUCCESS_3_EFFICIENCY"),
        ("dataC", "success", "high-efficiency"): os.getenv("R2_URL_DATAC_SUCCESS_4_HIGH_EFFICIENCY"),
        ("dataC", "success", "standby-power"): os.getenv("R2_URL_DATAC_SUCCESS_5_STANDBY_POWER"),
        ("dataC", "success", "approval"): os.getenv("R2_URL_DATAC_SUCCESS_6_APPROVAL"),
        ("dataC", "success", "declaration-details"): os.getenv("R2_URL_DATAC_SUCCESS_7_DECLARE"),
        ("dataC", "success", "kwtc"): os.getenv("R2_URL_DATAC_SUCCESS_8_KWTC"),
        ("dataC", "success", "recall"): os.getenv("R2_URL_DATAC_SUCCESS_9_RECALL"),
        ("dataC", "success", "safetykoreachild"): os.getenv("R2_URL_DATAC_SUCCESS_10_SAFETYKOREACHILD"),
        ("dataC", "success", "safetykoreahome"): os.getenv("R2_URL_DATAC_SUCCESS_13_SAFETYKOREAHOME"),
        ("dataC", "success", "rra-cert"): os.getenv("R2_URL_DATAC_SUCCESS_11_RRA_CERT"),
        ("dataC", "success", "rra-self-cert"): os.getenv("R2_URL_DATAC_SUCCESS_12_RRA_SELF_CERT"),

        # DataC Failed êµ¬ì¡° ë§¤í•‘
        ("dataC", "failed", "safetykorea"): os.getenv("R2_URL_DATAC_FAILED_1_SAFETYKOREA"),
        ("dataC", "failed", "wadiz-makers"): os.getenv("R2_URL_DATAC_FAILED_2_WADIZ"),
        ("dataC", "failed", "efficiency-rating"): os.getenv("R2_URL_DATAC_FAILED_3_EFFICIENCY"),
        ("dataC", "failed", "high-efficiency"): os.getenv("R2_URL_DATAC_FAILED_4_HIGH_EFFICIENCY"),
        ("dataC", "failed", "standby-power"): os.getenv("R2_URL_DATAC_FAILED_5_STANDBY_POWER"),
        ("dataC", "failed", "approval"): os.getenv("R2_URL_DATAC_FAILED_6_APPROVAL"),
        ("dataC", "failed", "declaration-details"): os.getenv("R2_URL_DATAC_FAILED_7_DECLARE"),
        ("dataC", "failed", "kwtc"): os.getenv("R2_URL_DATAC_FAILED_8_KWTC"),
        ("dataC", "failed", "recall"): os.getenv("R2_URL_DATAC_FAILED_9_RECALL"),
        ("dataC", "failed", "safetykoreachild"): os.getenv("R2_URL_DATAC_FAILED_10_SAFETYKOREACHILD"),
        ("dataC", "failed", "safetykoreahome"): os.getenv("R2_URL_DATAC_FAILED_13_SAFETYKOREAHOME"),
        ("dataC", "failed", "rra-cert"): os.getenv("R2_URL_DATAC_FAILED_11_RRA_CERT"),
        ("dataC", "failed", "rra-self-cert"): os.getenv("R2_URL_DATAC_FAILED_12_RRA_SELF_CERT"),
    }

    r2_url = r2_url_mapping.get((category, subcategory))

    # ë¡œì»¬ ê°œë°œ í™˜ê²½ fallback (VERCEL í™˜ê²½ë³€ìˆ˜ ì—†ì„ ë•Œ)
    if not r2_url and os.getenv("VERCEL") is None:
        local_parquet_path = local_file_mapping.get(subcategory, "./parquet/1_safetykorea_flattened.parquet")
        if os.path.exists(local_parquet_path):
            logger.info(f"ë¡œì»¬ ê°œë°œ í™˜ê²½: {category}/{subcategory} â†’ {local_parquet_path}")
            return local_parquet_path
        else:
            logger.info(f"ë¡œì»¬ ê°œë°œ í™˜ê²½: ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©")
            return "./parquet/1_safetykorea_flattened.parquet"

    if not r2_url:
        fallback_candidates = []
        local_candidate = local_file_mapping.get(subcategory)
        if local_candidate:
            fallback_candidates.extend([
                local_candidate,
                os.path.abspath(local_candidate),
                f"Project/{local_candidate}"
            ])

        for candidate in fallback_candidates:
            if candidate and os.path.exists(candidate):
                logger.warning(
                    f"R2 URL ëˆ„ë½: {category}/{subcategory} - ë¡œì»¬ íŒŒì¼ë¡œ ëŒ€ì²´ ({candidate})"
                )
                return candidate

        raise ValueError(f"R2 URL not found for {category}/{subcategory}. Check environment variables.")

    logger.info(f"ì „ì²´ ë°ì´í„° ëª¨ë“œ: {category}/{subcategory} â†’ R2")
    return r2_url

def search_in_fields(item: Dict[str, Any], keyword: str, search_field: str = "product_name") -> bool:
    """3ê°œ ê²€ìƒ‰ í•„ë“œ ì§€ì› - 'ì „ì²´' ê²€ìƒ‰ ê¸°ëŠ¥ ì™„ì „ ì œê±°"""
    keyword_lower = keyword.lower()
    
    # ë‹¤ì¸µ êµ¬ì¡° ì²˜ë¦¬ (SafetyKorea, Wadiz ë“±)
    if "resultData" in item:
        item = item["resultData"]
    elif "data" in item and isinstance(item["data"], dict):
        item = item["data"]
    
    # ê²€ìƒ‰ í•„ë“œë³„ í•„ë“œ ë³€í˜•ë“¤ (í•˜ë“œì½”ë”©)
    field_mappings = {
        "company_name": ["entrprsNm", "makerName", "importerName", "company_name", "ì œì¡°ì", "ìƒí˜¸", "ì—…ì²´ëª…", "maker_name", "ì‚¬ì—…ìëª…"],
        "model_name": ["modelName", "model_name", "ëª¨ë¸ëª…"],
        "product_name": ["prductNm", "productName", "equipment_name", "ì œí’ˆëª…", "ê¸°ìì¬ëª…ì¹­", "product_name", "í’ˆëª©ëª…"]
    }
    
    if search_field in field_mappings:
        # í•´ë‹¹ í•„ë“œì˜ ëª¨ë“  ë³€í˜•ë“¤ì—ì„œ ê²€ìƒ‰
        search_fields = field_mappings[search_field]
        
        for field in search_fields:
            if search_field_value(item, field, keyword_lower):
                return True
    
    return False

def search_nested_fields(item: Dict[str, Any], keyword: str) -> bool:
    """ì¤‘ì²©ëœ êµ¬ì¡°ì—ì„œ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰"""
    for key, value in item.items():
        if isinstance(value, str):
            if keyword in value.lower():
                return True
        elif isinstance(value, list):
            for list_item in value:
                if isinstance(list_item, str) and keyword in list_item.lower():
                    return True
                elif isinstance(list_item, dict):
                    if search_nested_fields(list_item, keyword):
                        return True
        elif isinstance(value, dict):
            if search_nested_fields(value, keyword):
                return True
        elif value is not None:
            if keyword in str(value).lower():
                return True
    return False

def search_field_value(item: Dict[str, Any], field: str, keyword: str) -> bool:
    """íŠ¹ì • í•„ë“œì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    if field in item:
        value = item[field]
        if isinstance(value, str) and keyword in value.lower():
            return True
        elif isinstance(value, list):
            for list_item in value:
                if isinstance(list_item, str) and keyword in list_item.lower():
                    return True
        elif value is not None and keyword in str(value).lower():
            return True
    return False

def generate_dynamic_summary(filtered_data: List[Dict], metadata: Dict) -> Dict[str, Any]:
    """ë™ì  ìš”ì•½ ì •ë³´ ìƒì„±"""
    if not filtered_data:
        return {}
    
    summary = {
        "total_records": len(filtered_data),
        "data_source": metadata.get("source", "unknown"),
        "last_updated": metadata.get("last_updated", "unknown")
    }
    
    # ì²« ë²ˆì§¸ ë ˆì½”ë“œì˜ í•„ë“œë¡œ ë¶„ì„ ê°€ëŠ¥í•œ í•­ëª©ë“¤ ì°¾ê¸°
    if filtered_data:
        sample_item = filtered_data[0]
        
        # ë‚ ì§œ í•„ë“œ ì°¾ê¸° (date, ë‚ ì§œ, _date, _at ë“±ì´ í¬í•¨ëœ í•„ë“œ)
        date_fields = [key for key in sample_item.keys() 
                      if any(date_keyword in key.lower() 
                            for date_keyword in ['date', 'ë‚ ì§œ', '_at', 'time'])]
        
        if date_fields:
            date_field = date_fields[0]  # ì²« ë²ˆì§¸ ë‚ ì§œ í•„ë“œ ì‚¬ìš©
            dates = [item.get(date_field, "") for item in filtered_data if item.get(date_field)]
            if dates:
                summary["date_range"] = {
                    "field": date_field,
                    "earliest": min(dates),
                    "latest": max(dates)
                }
        
        # ì¹´í…Œê³ ë¦¬ì„± í•„ë“œ ì°¾ê¸° (category, type, status, êµ¬ë¶„ ë“±)
        category_fields = [key for key in sample_item.keys() 
                          if any(cat_keyword in key.lower() 
                                for cat_keyword in ['category', 'type', 'status', 'êµ¬ë¶„', 'kind', 'class'])]
        
        for field in category_fields[:2]:  # ìµœëŒ€ 2ê°œ ì¹´í…Œê³ ë¦¬ í•„ë“œë§Œ
            field_distribution = {}
            for item in filtered_data:
                value = item.get(field, "ê¸°íƒ€")
                field_distribution[value] = field_distribution.get(value, 0) + 1
            
            if field_distribution:
                summary[f"{field}_distribution"] = field_distribution
    
    return summary

async def filter_data_by_conditions(conditions: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ê²€ìƒ‰ ì¡°ê±´ìœ¼ë¡œ ë°ì´í„° í•„í„°ë§ - ì¹´í…Œê³ ë¦¬ë³„ ì§€ì› (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§€ì›)"""
    category = conditions.get("category")
    subcategory = conditions.get("subcategory")
    file_size_mb = 0  # ê¸°ë³¸ê°’ ì„¤ì •
    
    if category and subcategory:
        # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë¡œë“œ
        data_file_path = get_data_file_path(category, subcategory)
        if data_file_path:
            data_file_str, is_r2_url, is_tabular, file_size_mb = _inspect_data_source(data_file_path)

            if is_tabular or file_size_mb > 50:
                effective_subcategory = normalize_subcategory(subcategory)
                limit = conditions.get("limit", 1000000)
                offset = conditions.get("offset", 0)
                page = (offset // limit) + 1 if limit and limit > 0 else 1

                search_result = await duckdb_search_large_file(
                    file_path=data_file_str,
                    keyword=conditions.get("keyword"),
                    search_field=conditions.get("search_field", "all"),
                    limit=limit,
                    page=page,
                    filters=conditions.get("filters"),
                    category=category,
                    subcategory=effective_subcategory
                )

                if search_result.get("error"):
                    logger.warning(f"DuckDB ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ijsonìœ¼ë¡œ fallback: {search_result.get('message')}")
                    search_result = await stream_search_large_file(
                        file_path=data_file_path,
                        keyword=conditions.get("keyword"),
                        search_field=conditions.get("search_field", "all"),
                        filters=conditions.get("filters"),
                        limit=conditions.get("limit", None),
                        offset=conditions.get("offset", 0)
                    )

                filtered_data = search_result.get("results", [])
            else:
                with open(data_file_path, 'r', encoding='utf-8') as f:
                    category_data = json.load(f)
                filtered_data = category_data.get("data", [])

                if conditions.get("keyword"):
                    keyword = conditions["keyword"].lower()
                    filtered_data = [
                        item for item in filtered_data
                        if search_in_all_fields(item, keyword)
                    ]
        else:
            filtered_data = []
    else:
        # ê¸°ë³¸ ë”ë¯¸ ë°ì´í„° ë¡œë“œ
        dummy_data = load_dummy_data()
        filtered_data = dummy_data["data"]
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        if conditions.get("keyword"):
            keyword = conditions["keyword"].lower()
            filtered_data = [
                item for item in filtered_data
                if keyword in item.get("title", "").lower() or
                   keyword in item.get("content", "").lower() or
                   any(keyword in tag.lower() for tag in item.get("tags", []))
            ]
    
    # ì¶”ê°€ í•„í„°ë“¤ (ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
    if not (category and subcategory and file_size_mb > 50):
        if conditions.get("categories"):
            filtered_data = [
                item for item in filtered_data
                if item.get("category") in conditions["categories"]
            ]
        
        # ë‚ ì§œ í•„í„°
        if conditions.get("date_from"):
            filtered_data = [
                item for item in filtered_data
                if item.get("date", "") >= conditions["date_from"]
            ]
        
        if conditions.get("date_to"):
            filtered_data = [
                item for item in filtered_data
                if item.get("date", "") <= conditions["date_to"]
            ]
    
    return filtered_data

def search_in_all_fields(item: Dict[str, Any], keyword: str) -> bool:
    """ëª¨ë“  í•„ë“œì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    # resultData êµ¬ì¡° ì²˜ë¦¬
    if "resultData" in item:
        item = item["resultData"]
    
    for key, value in item.items():
        if isinstance(value, str):
            if keyword in value.lower():
                return True
        elif isinstance(value, list):
            for list_item in value:
                if isinstance(list_item, str) and keyword in list_item.lower():
                    return True
        elif value is not None:
            if keyword in str(value).lower():
                return True
    return False

def infer_field_type(sample_data: List[Dict], field_name: str) -> str:
    """ìƒ˜í”Œ ë°ì´í„°ì—ì„œ í•„ë“œ íƒ€ì… ì¶”ë¡ """
    if not sample_data:
        return "text"
    
    sample_values = []
    for item in sample_data:
        # resultData êµ¬ì¡° ì²˜ë¦¬
        if "resultData" in item:
            item = item["resultData"]
        
        if field_name in item and item[field_name] is not None:
            sample_values.append(item[field_name])
    
    if not sample_values:
        return "text"
    
    # ë‚ ì§œ í˜•ì‹ ê²€ì‚¬
    if any(keyword in field_name.lower() for keyword in ['date', 'ë‚ ì§œ', 'time', '_at']):
        return "date"
    
    # ìˆ«ì í˜•ì‹ ê²€ì‚¬
    if all(isinstance(val, (int, float)) for val in sample_values if val is not None):
        return "number"
    
    # ë°°ì—´ í˜•ì‹ ê²€ì‚¬
    if any(isinstance(val, list) for val in sample_values):
        return "array"
    
    # URL ê²€ì‚¬
    if any(isinstance(val, str) and (val.startswith('http') or val.startswith('www')) for val in sample_values):
        return "link"
    
    # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ì ê²€ì‚¬
    if any(isinstance(val, str) and any(val.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']) for val in sample_values):
        return "image"
    
    return "text"

def get_display_config_for_rendering(category: str, subcategory: str) -> Dict[str, Any]:
    """ë Œë”ë§ìš© í‘œì‹œ ì„¤ì • ì¡°íšŒ"""
    try:
        config = display_config_manager.export_client_config(category, subcategory)
        return config
    except:
        return {}

# ì„¤ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”
def initialize_field_settings():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ field_settings.json ì´ˆê¸°í™”"""
    try:
        # field_settings.json íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ìƒì„±
        field_settings_path = Path(__file__).parent.parent / "config" / "field_settings.json"
        
        if not field_settings_path.exists() or field_settings_path.stat().st_size < 100:
            logger.info("ê¸°ë³¸ field_settings.json íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            # íŒŒì¼ì´ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì‘ì—… ì—†ìŒ
        
        # display_config_manager ìƒˆë¡œê³ ì¹¨
        display_config_manager._field_settings = display_config_manager._load_field_settings()
        display_config_manager._configs.clear()
        display_config_manager._load_all_configs()
        
        logger.info("ì„¤ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ì„¤ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")



# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ì´ˆê¸°í™”
initialize_field_settings()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
# í”„ë¦¬í˜ì¹˜ êµ¬ì„±ì„ ì¡°íšŒí•˜ëŠ” í—¬í¼ (í™•ì¥ ê°€ëŠ¥)
def get_prefetch_config() -> Dict[str, Any]:
    data_mode = os.getenv("DATA_MODE", "full").lower()
    enabled = data_mode == "2025"
    return {
        "enabled": enabled,
        "data_mode": data_mode,
        "supported_keys": list(BLOB_ENV_PREFETCH_MAPPING.keys())
    }
