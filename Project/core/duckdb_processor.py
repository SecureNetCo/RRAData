"""
DuckDB ê¸°ë°˜ ëŒ€ìš©ëŸ‰ JSON íŒŒì¼ ì²˜ë¦¬ ëª¨ë“ˆ
ë‹¤ì–‘í•œ JSON êµ¬ì¡°ì— ëŒ€ì‘í•˜ëŠ” ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì—”ì§„

ì„±ëŠ¥ ëª©í‘œ:
- ê¸°ì¡´ ijson 113ì´ˆ â†’ DuckDB 10-15ì´ˆ (8-11ë°° í–¥ìƒ)  
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
- ë‹¤ì¤‘ ì½”ì–´ ë³‘ë ¬ ì²˜ë¦¬ í™œìš©
- JSON ë°°ì—´/ê°ì²´ êµ¬ì¡° ìë™ ê°ì§€
"""

import duckdb
import asyncio
import time
import os
import hashlib
import urllib.request
import shutil
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path
import json
import logging
from functools import lru_cache
from urllib.parse import urlparse
from threading import Lock

logger = logging.getLogger(__name__)

# DuckDB httpfs ì„¤ì¹˜ ì—¬ë¶€ ìºì‹œ
HTTPFS_INSTALLED = False

# ìŠ¤í‚¤ë§ˆ ìºì‹œ (URL ë° íŒŒì¼ëª… ê¸°ì¤€) - Blob/R2 ë„ë©”ì¸ ë³€ê²½ ëŒ€ì‘
SCHEMA_CACHE_BY_URL: Dict[str, List[str]] = {}
SCHEMA_CACHE_BY_FILENAME: Dict[str, List[str]] = {}

# DuckDB ì—°ê²° ìºì‹œ (íŒŒì¼ ê²½ë¡œ/URL ê¸°ì¤€)
CONNECTION_CACHE: Dict[str, duckdb.DuckDBPyConnection] = {}
CONNECTION_LOCKS: Dict[str, Lock] = {}
CONNECTION_CACHE_LOCK = Lock()

# ì›ê²© DuckDB íŒŒì¼ ë¡œì»¬ ìºì‹œ
DUCKDB_REMOTE_CACHE: Dict[str, Path] = {}
DUCKDB_REMOTE_CACHE_LOCK = Lock()
DUCKDB_CACHE_ROOT = Path("/tmp/datapage_duckdb_cache")
DUCKDB_CACHE_ROOT.mkdir(parents=True, exist_ok=True)


def _get_search_pattern_and_operator(keyword: str, field: str) -> tuple[str, str]:
    """
    í•„ë“œë³„ ê²€ìƒ‰ íŒ¨í„´ê³¼ ì—°ì‚°ì ìƒì„± í•¨ìˆ˜
    ì¸ì¦ë²ˆí˜¸/ì‹ ê³ ë²ˆí˜¸ í•„ë“œì— ëŒ€í•´ ì •í™• ë§¤ì¹­ ì ìš©

    Returns:
        tuple: (search_pattern, operator)
        - operator: 'LIKE' ë˜ëŠ” '='
        - search_pattern: ê²€ìƒ‰ íŒ¨í„´ (LIKEìš© '%keyword%' ë˜ëŠ” ì •í™•ë§¤ì¹­ìš© 'keyword')
    """
    # ì¸ì¦ë²ˆí˜¸/ì‹ ê³ ë²ˆí˜¸ í•„ë“œëŠ” ì •í™• ë§¤ì¹­
    if field in ['cert_no', 'cert_num', 'declare_no', 'ì‹ ê³ ë²ˆí˜¸', 'ìŠ¹ì¸ë²ˆí˜¸']:
        return keyword, '='  # ì •í™• ë§¤ì¹­

    # ê¸°ë³¸: ë¶€ë¶„ ë§¤ì¹­ (LIKE '%keyword%')
    return f"%{keyword}%", 'LIKE'


def _get_search_pattern(keyword: str, field: str) -> str:
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    pattern, _ = _get_search_pattern_and_operator(keyword, field)
    return pattern


def _extract_file_name(path_like: Any) -> Optional[str]:
    """URL/ê²½ë¡œ ë¬¸ìì—´ì—ì„œ íŒŒì¼ëª…ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
    if not path_like:
        return None

    try:
        path_str = str(path_like)
        if path_str.startswith("http://") or path_str.startswith("https://"):
            parsed = urlparse(path_str)
            return Path(parsed.path).name if parsed.path else None
        return Path(path_str).name
    except Exception:
        return None

def _normalize_memory_setting(value: str, default: str) -> str:
    """Return DuckDB-friendly memory setting (accept plain numbers as MB)."""
    if not value:
        return default

    value = value.strip()
    if not value:
        return default

    lowered = value.lower()
    if lowered.endswith(("mb", "gb")):
        return value

    if lowered.isdigit():
        return f"{value}MB"

    return default


def _configure_connection(conn: duckdb.DuckDBPyConnection) -> duckdb.DuckDBPyConnection:
    """DuckDB ì—°ê²°ì— ê³µí†µ ì„¤ì • ì ìš©"""

    # **ğŸš€ ì„±ëŠ¥ ìµœì í™”: Vercel ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ ë§ì¶¤ DuckDB ì„¤ì •**
    try:
        memory_limit = _normalize_memory_setting(
            os.getenv("DUCKDB_MEMORY_LIMIT"),
            "512MB"
        )
        max_memory = _normalize_memory_setting(
            os.getenv("DUCKDB_MAX_MEMORY"),
            "640MB"
        )

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™” (Vercel 1GB ì œí•œ ê³ ë ¤)
        conn.execute(f"SET memory_limit = '{memory_limit}'")
        conn.execute(f"SET max_memory = '{max_memory}'")
        conn.execute("SET temp_directory = '/tmp'")          # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì§€ì •

        # ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”
        conn.execute("SET threads = 2")                      # ì„œë²„ë¦¬ìŠ¤ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
        conn.execute("SET enable_progress_bar = false")      # ì§„í–‰ë¥  í‘œì‹œ ë¹„í™œì„±í™”ë¡œ ì˜¤ë²„í—¤ë“œ ì œê±°
        conn.execute("SET enable_object_cache = true")       # ê°ì²´ ìºì‹± í™œì„±í™”
        conn.execute("SET preserve_insertion_order = false") # ì •ë ¬ ì„±ëŠ¥ í–¥ìƒ

        logger.info("âš¡ DuckDB ê³ ê¸‰ ìµœì í™” ì„¤ì • ì™„ë£Œ - Vercel íŠ¹í™”")
    except Exception as e:
        logger.warning(f"DuckDB ìµœì í™” ì„¤ì • ì¼ë¶€ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì„¤ì •ì´ë¼ë„ ì ìš©
        try:
            conn.execute("SET memory_limit = '256MB'")
            conn.execute("SET max_memory = '320MB'")
            conn.execute("SET threads = 2")
            logger.info("ğŸ’¡ DuckDB ê¸°ë³¸ ìµœì í™” ì„¤ì • ì ìš©")
        except:
            logger.warning("DuckDB ê¸°ë³¸ ì„¤ì • ì‹¤íŒ¨ - ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰")

    # Vercel ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì„ ìœ„í•œ ì•ˆì „í•œ httpfs ì„¤ì •
    try:
        # 1ë‹¨ê³„: home_directory ì„¤ì • (Vercel í™˜ê²½ ëŒ€ì‘)
        conn.execute("SET home_directory='/tmp'")
        logger.info("home_directory ì„¤ì • ì™„ë£Œ")

        # 2ë‹¨ê³„: httpfs ì„¤ì¹˜ ë° ë¡œë“œ (INSTALLì€ ìµœì´ˆ 1íšŒë§Œ)
        global HTTPFS_INSTALLED
        if not HTTPFS_INSTALLED:
            try:
                conn.execute("INSTALL httpfs")
                logger.info("httpfs extension ìµœì´ˆ ì„¤ì¹˜ ì™„ë£Œ")
            except Exception as install_error:
                logger.debug(f"httpfs install ìŠ¤í‚µ: {install_error}")
            finally:
                HTTPFS_INSTALLED = True

        conn.execute("LOAD httpfs")
        logger.info("httpfs extension ë¡œë“œ ì™„ë£Œ")

    except Exception as e:
        # httpfs ì‹¤íŒ¨ ì‹œ ì™„ì „ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
        logger.info(f"httpfs ì„¤ì • ìŠ¤í‚µ (ì„œë²„ë¦¬ìŠ¤ í™˜ê²½): {str(e)[:100]}...")

    return conn


def _create_optimized_connection() -> duckdb.DuckDBPyConnection:
    """ìµœì í™”ëœ DuckDB ì—°ê²° ìƒì„±"""
    conn = duckdb.connect()
    return _configure_connection(conn)


def _get_or_create_connection(connection_key: str) -> tuple[duckdb.DuckDBPyConnection, Lock]:
    """íŒŒì¼ë³„ DuckDB ì—°ê²°ì„ ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©"""
    with CONNECTION_CACHE_LOCK:
        conn = CONNECTION_CACHE.get(connection_key)
        if conn is None:
            conn = _create_optimized_connection()
            CONNECTION_CACHE[connection_key] = conn
            CONNECTION_LOCKS[connection_key] = Lock()
            logger.info(f"DuckDB ì—°ê²° ìºì‹œ ìƒì„±: {connection_key}")

        conn_lock = CONNECTION_LOCKS[connection_key]

    return conn, conn_lock

    return conn

@lru_cache(maxsize=1)
def load_case_sensitivity_config():
    """ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì„¤ì • ë¡œë“œ"""
    try:
        config_path = Path(__file__).parent.parent / "config" / "case_sensitivity_config.json"
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            logger.info("ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
            return config
    except Exception as e:
        logger.warning(f"ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
        return {
            "case_insensitive_fields": {"ì—…ì²´ëª…": True, "ì œí’ˆëª…": True, "company_name": True, "product_name": True},
            "case_sensitive_fields": {"ì¸ì¦ë²ˆí˜¸": False, "ëª¨ë¸ëª…": False, "certification_no": False, "model_name": False},
            "default_case_insensitive": False
        }

@lru_cache(maxsize=1)
def load_field_settings():
    """field_settings.json ë¡œë“œ"""
    try:
        config_path = Path(__file__).parent.parent / "config" / "field_settings.json"
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            logger.info("field_settings.json ë¡œë“œ ì™„ë£Œ")
            return config
    except Exception as e:
        logger.warning(f"field_settings.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

class DuckDBProcessor:
    """DuckDB ê¸°ë°˜ SafetyKorea ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        file_path: str,
        category: str = None,
        subcategory: str = None,
        result_type: str = None,
        required_fields: Optional[List[str]] = None,
    ):
        """
        Args:
            file_path: ì²˜ë¦¬í•  JSON/Parquet íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” URL)
            category: ë°ì´í„° ì¹´í…Œê³ ë¦¬ (dataA, dataC ë“±) - SELECT ìµœì í™”ìš©
            subcategory: ë°ì´í„° ì„œë¸Œì¹´í…Œê³ ë¦¬ (safetykoreachild ë“±) - SELECT ìµœì í™”ìš©
        """
        self.file_path_str = file_path  # ì›ë³¸ ê²½ë¡œ ë¬¸ìì—´ ì €ì¥
        self.category = category        # SELECT ìµœì í™”ìš© ì¹´í…Œê³ ë¦¬
        self.subcategory = subcategory  # SELECT ìµœì í™”ìš© ì„œë¸Œì¹´í…Œê³ ë¦¬
        self.json_structure = None      # 'array', 'object', 'nested_object'
        self.case_config = load_case_sensitivity_config()  # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì„¤ì • ë¡œë“œ
        self.field_settings = load_field_settings()        # field_settings.json ë¡œë“œ
        self.result_type = result_type  # dataC success/failed êµ¬ë¶„ìš©
        self.connection_key = file_path  # ê¸°ë³¸ì ìœ¼ë¡œ ë¬¸ìì—´ í˜•íƒœ (URL í¬í•¨)
        self.required_fields: List[str] = required_fields or []
        self.dynamic_required_fields: List[str] = []
        self.is_duckdb_storage = False
        self.duckdb_table_name: Optional[str] = None
        self._duckdb_alias: Optional[str] = None
        self._duckdb_view_name: Optional[str] = None
        self._duckdb_attached = False
        self._local_duckdb_path: Optional[str] = None
        
        # íŒŒì¼ ê²½ë¡œê°€ URLì¸ì§€ ë¡œì»¬ ê²½ë¡œì¸ì§€ í™•ì¸
        self.is_url = self.file_path_str.startswith('https://') or self.file_path_str.startswith('http://')
        
        if not self.is_url:
            # ë¡œì»¬ íŒŒì¼ì¼ ê²½ìš°ì—ë§Œ Path ê°ì²´ë¡œ ë³€í™˜í•˜ê³  ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            self.file_path = Path(file_path)
            if not self.file_path.exists():
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            self.connection_key = str(self.file_path.resolve())
        else:
            # URLì€ ë¬¸ìì—´ë¡œ ìœ ì§€ (DuckDBê°€ URLì„ ì§ì ‘ ì²˜ë¦¬ ê°€ëŠ¥)
            self.file_path = self.file_path_str
            
        logger.info(f"DuckDBProcessor ì´ˆê¸°í™”: {self.file_path} (URL: {self.is_url})")

        # DuckDB íŒŒì¼ì¼ ê²½ìš° ë©”íƒ€ë°ì´í„° ì„¤ì •
        suffix_target: Optional[Path] = None
        if self.is_url:
            try:
                parsed = urlparse(self.file_path_str)
                if parsed.path:
                    suffix_target = Path(parsed.path)
            except Exception:
                suffix_target = None
        else:
            suffix_target = self.file_path

        if suffix_target is not None and suffix_target.suffix.lower() == '.duckdb':
            self.is_duckdb_storage = True
            self.duckdb_table_name = suffix_target.stem
            digest_source = str(self.connection_key)
            digest = hashlib.md5(digest_source.encode('utf-8', errors='ignore')).hexdigest()
            identifier = digest[:12]
            self._duckdb_alias = f"db_{identifier}"
            self._duckdb_view_name = f"vw_{identifier}"
            logger.info(
                "DuckDB ìŠ¤í† ë¦¬ì§€ ê°ì§€: table=%s, alias=%s", self.duckdb_table_name, self._duckdb_alias
            )

        # ğŸš€ ì„±ëŠ¥ ìµœì í™”: R2 íŒŒì¼ ìŠ¤í‚¤ë§ˆ ìºì‹œ (ì²« ë²ˆì§¸ ë„¤íŠ¸ì›Œí¬ í†µì‹  ì œê±°)
        global SCHEMA_CACHE_BY_URL, SCHEMA_CACHE_BY_FILENAME


    def _get_connection(self) -> tuple[duckdb.DuckDBPyConnection, Lock]:
        return _get_or_create_connection(self.connection_key)

    @staticmethod
    def _escape_path(path: str) -> str:
        return path.replace("'", "''")

    def _ensure_local_duckdb_copy(self, source_url: str) -> Path:
        with DUCKDB_REMOTE_CACHE_LOCK:
            cached = DUCKDB_REMOTE_CACHE.get(source_url)
            if cached and cached.exists():
                return cached

            parsed = urlparse(source_url)
            filename = Path(parsed.path).name or f"duckdb_{hashlib.md5(source_url.encode('utf-8', errors='ignore')).hexdigest()}.duckdb"
            cache_path = DUCKDB_CACHE_ROOT / filename

            temp_path = cache_path.with_suffix('.download')
            try:
                logger.info(f"DuckDB ì›ê²© íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {source_url} â†’ {cache_path}")
                with urllib.request.urlopen(source_url) as response, open(temp_path, 'wb') as out_file:
                    shutil.copyfileobj(response, out_file)
                os.replace(temp_path, cache_path)
            except Exception as download_error:
                if temp_path.exists():
                    try:
                        temp_path.unlink()
                    except Exception:
                        pass
                raise RuntimeError(f"DuckDB íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {source_url} ({download_error})") from download_error

            DUCKDB_REMOTE_CACHE[source_url] = cache_path
            return cache_path

    def _ensure_duckdb_view(self, conn: duckdb.DuckDBPyConnection, path: str) -> str:
        """DuckDB íŒŒì¼ì„ í˜„ì¬ ì—°ê²°ì—ì„œ ë·°ë¡œ ë…¸ì¶œì‹œí‚¤ê³  ë·° ì´ë¦„ì„ ë°˜í™˜"""
        if not self.is_duckdb_storage and not path.lower().endswith('.duckdb'):
            raise ValueError("DuckDB ë·° ì¤€ë¹„ëŠ” DuckDB íŒŒì¼ì—ì„œë§Œ í˜¸ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤")

        if not self._duckdb_alias or not self._duckdb_view_name:
            digest = hashlib.md5(path.encode('utf-8', errors='ignore')).hexdigest()
            identifier = digest[:12]
            if not self._duckdb_alias:
                self._duckdb_alias = f"db_{identifier}"
            if not self._duckdb_view_name:
                self._duckdb_view_name = f"vw_{identifier}"

        alias = self._duckdb_alias
        view_name = self._duckdb_view_name
        escaped_path = self._escape_path(path)

        try:
            conn.execute(f"ATTACH '{escaped_path}' AS {alias} (READ_ONLY)")
        except (duckdb.CatalogException, duckdb.BinderException):
            # ì´ë¯¸ ATTACH ëœ ê²½ìš° ë¬´ì‹œ
            pass

        table_name = self.duckdb_table_name
        if not table_name:
            table_result = conn.execute(f"PRAGMA show_tables FROM {alias}").fetchall()
            if table_result:
                # fetchall() ê²°ê³¼ì—ì„œ ì²« ë²ˆì§¸ í–‰ì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼ ê°’ ì¶”ì¶œ
                table_name = table_result[0][0] if table_result[0] else None
                self.duckdb_table_name = table_name
            else:
                raise RuntimeError(f"DuckDB íŒŒì¼ì—ì„œ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

        table_identifier = f'"{table_name}"'
        conn.execute(
            f"CREATE OR REPLACE VIEW {view_name} AS SELECT * FROM {alias}.{table_identifier}"
        )
        self._duckdb_attached = True
        return view_name

    def _get_table_expression(self, conn: duckdb.DuckDBPyConnection, path: str) -> str:
        """ì£¼ì–´ì§„ ê²½ë¡œë¥¼ DuckDB SQL FROM ì ˆì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í‘œí˜„ì‹ìœ¼ë¡œ ë³€í™˜"""
        if path.lower().endswith('.duckdb'):
            if path.startswith(('http://', 'https://')):
                local_path = self._ensure_local_duckdb_copy(path)
                if self._local_duckdb_path is None:
                    self._local_duckdb_path = str(local_path)
                path = str(local_path)
            return self._ensure_duckdb_view(conn, path)
        escaped_path = self._escape_path(path)
        return f"read_parquet('{escaped_path}')"

    def _resolve_tabular_path(self) -> Optional[str]:
        """í˜„ì¬ íŒŒì¼ ê²½ë¡œ ì¤‘ DuckDB/Parquet í˜•íƒœë¥¼ ìš°ì„  ë°˜í™˜"""
        if self.is_url:
            if self.file_path_str.endswith('.duckdb'):
                try:
                    local_path = self._ensure_local_duckdb_copy(self.file_path_str)
                    self._local_duckdb_path = str(local_path)
                    return str(local_path)
                except Exception as download_error:
                    logger.error(download_error)
                    return None
            if self.file_path_str.endswith('.parquet'):
                return self.file_path_str
            return None

        target_path = None
        if hasattr(self.file_path, 'resolve'):
            abs_file_path = str(self.file_path.resolve())
        else:
            abs_file_path = str(Path(self.file_path).resolve())

        suffix = Path(abs_file_path).suffix.lower()
        if suffix == '.duckdb' and Path(abs_file_path).exists():
            target_path = abs_file_path
        else:
            parquet_path = abs_file_path.replace('.json', '.parquet')
            if Path(parquet_path).exists():
                target_path = parquet_path

        return target_path

    def _cache_schema(self, columns: List[str]) -> List[str]:
        """ê³„ì‚°ëœ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ URL/íŒŒì¼ëª… ìºì‹œì— ë™ì‹œ ì €ì¥ (ë™ì  ìŠ¤í‚¤ë§ˆ ëŒ€ìƒì€ ìºì‹œ ì œì™¸)"""
        if columns is None:
            return columns

        cache_key = self.file_path_str if self.is_url else str(self.file_path)
        file_name = _extract_file_name(cache_key)

        # ë™ì  ìŠ¤í‚¤ë§ˆ ë¡œë“œ ëŒ€ìƒì€ ìºì‹œì— ì €ì¥í•˜ì§€ ì•ŠìŒ
        if self._should_use_dynamic_schema(cache_key, file_name):
            logger.debug(f"ğŸ”„ ë™ì  ìŠ¤í‚¤ë§ˆ ëŒ€ìƒ: ìºì‹œ ì €ì¥ ê±´ë„ˆë›°ê¸° - {file_name or cache_key}")
            return columns

        # ì¼ë°˜ ë°ì´í„°ì…‹ì€ ìºì‹œì— ì €ì¥
        if cache_key:
            SCHEMA_CACHE_BY_URL[cache_key] = columns

        if file_name:
            SCHEMA_CACHE_BY_FILENAME[file_name] = columns

        return columns

    def _should_use_dynamic_schema(self, cache_key: str, file_name: Optional[str]) -> bool:
        """ë™ì  ìŠ¤í‚¤ë§ˆ ë¡œë“œ ëŒ€ìƒì¸ì§€ íŒë‹¨ (3,4,5ë²ˆ ë°ì´í„°ì…‹: íš¨ìœ¨ë“±ê¸‰, ê³ íš¨ìœ¨, ëŒ€ê¸°ì „ë ¥)"""
        # íŒŒì¼ëª… ê¸°ë°˜ íŒë‹¨
        if file_name:
            # 3_efficiency, 4_high_efficiency, 5_standby_power ë° success/failed ë²„ì „ í¬í•¨
            dynamic_targets = [
                "3_efficiency_flattened.parquet",
                "3_efficiency_flattened_success.parquet",
                "3_efficiency_flattened_failed.parquet",
                "4_high_efficiency_flattened.parquet",
                "4_high_efficiency_flattened_success.parquet",
                "4_high_efficiency_flattened_failed.parquet",
                "5_standby_power_flattened.parquet",
                "5_standby_power_flattened_success.parquet",
                "5_standby_power_flattened_failed.parquet"
            ]
            if file_name in dynamic_targets:
                return True

        # URL ê¸°ë°˜ íŒë‹¨ (R2 URLs)
        dynamic_url_patterns = [
            "/3_efficiency_flattened.parquet",
            "/3_efficiency_flattened_success.parquet",
            "/3_efficiency_flattened_failed.parquet",
            "/4_high_efficiency_flattened.parquet",
            "/4_high_efficiency_flattened_success.parquet",
            "/4_high_efficiency_flattened_failed.parquet",
            "/5_standby_power_flattened.parquet",
            "/5_standby_power_flattened_success.parquet",
            "/5_standby_power_flattened_failed.parquet"
        ]

        for pattern in dynamic_url_patterns:
            if pattern in cache_key:
                return True

        return False

    def _try_duckdb_blob_schema_mapping(self, cache_key: str, file_name: Optional[str]) -> Optional[List[str]]:
        """DuckDB BLOB URLì„ ìœ„í•œ ìŠ¤í‚¤ë§ˆ ë§¤í•‘ (2025ëª¨ë“œ DuckDB íŒŒì¼ ì§€ì›)"""
        if not cache_key or not file_name:
            return None

        # DuckDB BLOB URL ê°ì§€
        is_duckdb_blob = (cache_key.endswith('.duckdb') and
                         ('blob.vercel-storage.com' in cache_key or 'blob' in cache_key.lower()))

        if not is_duckdb_blob:
            return None

        logger.info(f"ğŸ”¥ DuckDB BLOB URL ê°ì§€, ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì‹œë„: {file_name}")

        # íŒŒì¼ëª…ì—ì„œ ë°ì´í„°ì…‹ íŒ¨í„´ ì¶”ì¶œí•˜ì—¬ R2 ìŠ¤í‚¤ë§ˆì™€ ë§¤í•‘
        dataset_patterns = {
            'safetykorea': ['safetykorea', 'safety_korea'],
            'wadiz': ['wadiz', 'makers'],
            'efficiency': ['efficiency', 'energy'],
            'high_efficiency': ['high_efficiency', 'high-efficiency'],
            'standby_power': ['standby_power', 'standby-power'],
            'approval': ['approval', 'approve'],
            'declare': ['declare', 'declaration'],
            'kwtc': ['kwtc'],
            'recall': ['recall'],
            'safetykoreachild': ['safetykoreachild', 'safety_korea_child', 'child'],
            'rra_cert': ['rra_cert', 'rra-cert'],
            'rra_self_cert': ['rra_self_cert', 'rra-self-cert'],
            'safetykoreahome': ['safetykoreahome', 'safety_korea_home', 'home']
        }

        # íŒŒì¼ëª…ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ íŒ¨í„´ ë§¤ì¹­
        file_name_lower = file_name.lower()

        matched_dataset = None
        for dataset, patterns in dataset_patterns.items():
            for pattern in patterns:
                if pattern in file_name_lower:
                    matched_dataset = dataset
                    break
            if matched_dataset:
                break

        if not matched_dataset:
            logger.warning(f"DuckDB BLOB URL ë°ì´í„°ì…‹ íŒ¨í„´ ë§¤í•‘ ì‹¤íŒ¨: {file_name}")
            return None

        # DataC (enhanced) ì—¬ë¶€ í™•ì¸
        is_enhanced = 'enhanced' in file_name_lower or 'success' in file_name_lower or 'failed' in file_name_lower
        is_success = 'success' in file_name_lower

        # ë§¤í•‘ëœ R2 ìŠ¤í‚¤ë§ˆ ì°¾ê¸°
        target_r2_patterns = []

        if is_enhanced:
            # DataC enhanced ìŠ¤í‚¤ë§ˆ ìš°ì„ 
            if is_success:
                target_r2_patterns.append(f"{matched_dataset}_flattened_success.parquet")
            else:
                target_r2_patterns.append(f"{matched_dataset}_flattened_failed.parquet")

        # ê¸°ë³¸ DataA ìŠ¤í‚¤ë§ˆ
        if matched_dataset == 'wadiz':
            target_r2_patterns.append('2_wadiz_flattened.parquet')
        elif matched_dataset == 'safetykorea':
            target_r2_patterns.append('1_safetykorea_flattened.parquet')
        elif matched_dataset == 'efficiency':
            target_r2_patterns.append('3_efficiency_flattened.parquet')
        elif matched_dataset == 'high_efficiency':
            target_r2_patterns.append('4_high_efficiency_flattened.parquet')
        elif matched_dataset == 'standby_power':
            target_r2_patterns.append('5_standby_power_flattened.parquet')
        elif matched_dataset == 'approval':
            target_r2_patterns.append('6_approval_flattened.parquet')
        elif matched_dataset == 'declare':
            target_r2_patterns.append('7_declare_flattened.parquet')
        elif matched_dataset == 'kwtc':
            target_r2_patterns.append('8_kwtc_flattened.parquet')
        elif matched_dataset == 'recall':
            target_r2_patterns.append('9_recall_flattened.parquet')
        elif matched_dataset == 'safetykoreachild':
            target_r2_patterns.append('10_safetykoreachild_flattened.parquet')
        elif matched_dataset == 'rra_cert':
            target_r2_patterns.append('11_rra_cert_flattened.parquet')
        elif matched_dataset == 'rra_self_cert':
            target_r2_patterns.append('12_rra_self_cert_flattened.parquet')
        elif matched_dataset == 'safetykoreahome':
            target_r2_patterns.append('13_safetykoreahome_flattened.parquet')

        # SCHEMA_CACHE_BY_FILENAMEì—ì„œ ìŠ¤í‚¤ë§ˆ ì°¾ê¸°
        for target_pattern in target_r2_patterns:
            if target_pattern in SCHEMA_CACHE_BY_FILENAME:
                schema = SCHEMA_CACHE_BY_FILENAME[target_pattern]
                logger.info(f"âœ… DuckDB BLOB ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì„±ê³µ: {file_name} â†’ {target_pattern} ({len(schema)}ê°œ ì»¬ëŸ¼)")

                # DuckDB BLOB URL ìºì‹œì— ì €ì¥
                SCHEMA_CACHE_BY_URL[cache_key] = schema
                if file_name not in SCHEMA_CACHE_BY_FILENAME:
                    SCHEMA_CACHE_BY_FILENAME[file_name] = schema

                return schema

        logger.warning(f"DuckDB BLOB URL ë§¤í•‘ ëŒ€ìƒ ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {matched_dataset} â†’ {target_r2_patterns}")
        return None


    def _detect_json_structure(self) -> str:
        """JSON íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ ê°ì§€í•©ë‹ˆë‹¤"""
        if self.json_structure:
            return self.json_structure

        # íŒŒì¼“ íŒŒì¼ì¸ ê²½ìš° JSON êµ¬ì¡° ê°ì§€ ê±´ë„ˆë›°ê¸° (UTF-8 ì˜¤ë¥˜ ë°©ì§€)
        if str(self.file_path).lower().endswith('.parquet'):
            self.json_structure = 'parquet'
            logger.info(f"íŒŒì¼“ íŒŒì¼ ê°ì§€, JSON êµ¬ì¡° ê°ì§€ ê±´ë„ˆë›°ê¸°: {self.file_path}")
            return self.json_structure

        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                # íŒŒì¼ì˜ ì²« ë¶€ë¶„ì„ ì½ì–´ì„œ êµ¬ì¡° ê°ì§€
                content = f.read(1000).strip()
                
            if content.startswith('['):
                self.json_structure = 'array'
                logger.info(f"JSON êµ¬ì¡° ê°ì§€: ë°°ì—´ í˜•íƒœ")
            elif content.startswith('{'):
                # ê°ì²´ ë‚´ì— ë°°ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
                if 'LEDë¨í”„_details' in content:
                    self.json_structure = 'nested_safetykorea'
                elif '"data"' in content and '[' in content:
                    self.json_structure = 'nested_data'
                else:
                    self.json_structure = 'object'
                logger.info(f"JSON êµ¬ì¡° ê°ì§€: ê°ì²´ í˜•íƒœ ({self.json_structure})")
            else:
                self.json_structure = 'unknown'
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” JSON êµ¬ì¡°")
                
        except Exception as e:
            logger.error(f"JSON êµ¬ì¡° ê°ì§€ ì‹¤íŒ¨: {e}")
            self.json_structure = 'unknown'
            
        return self.json_structure

    def _get_search_fields_from_config(self) -> list:
        """field_settings.jsonì—ì„œ search_fields ì¶”ì¶œ"""
        try:
            if not self.category or not self.subcategory:
                logger.info("ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—†ìŒ - ì „ì²´ ì»¬ëŸ¼ ì‚¬ìš©")
                return []

            # dataCì˜ ê²½ìš° result_type(success/failed ë“±) ê¸°ë°˜ìœ¼ë¡œ ì„¤ì • êµ¬ë¶„
            if self.category == "dataC":
                data_c_config = self.field_settings.get("dataC", {})
                result_bucket = data_c_config.get(self.result_type or "success", {})
                config_path = result_bucket.get(self.subcategory, {})
            else:
                config_path = self.field_settings.get(self.category, {}).get(self.subcategory, {})

            search_fields = config_path.get("search_fields", [])
            field_names = [field.get("field") for field in search_fields if field.get("field")]

            logger.info(f"ğŸ” ê²€ìƒ‰ í•„ë“œ ë¡œë“œ: {len(field_names)}ê°œ - {field_names[:3]}...")
            return field_names
        except Exception as e:
            logger.warning(f"search_fields ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def _get_display_fields_from_config(self) -> list:
        """field_settings.jsonì—ì„œ display_fields ì¶”ì¶œ"""
        try:
            if not self.category or not self.subcategory:
                return []

            # dataCì˜ ê²½ìš° result_type(success/failed ë“±) ê¸°ë°˜ìœ¼ë¡œ ì„¤ì • êµ¬ë¶„
            if self.category == "dataC":
                data_c_config = self.field_settings.get("dataC", {})
                result_bucket = data_c_config.get(self.result_type or "success", {})
                config_path = result_bucket.get(self.subcategory, {})
            else:
                config_path = self.field_settings.get(self.category, {}).get(self.subcategory, {})

            display_fields = config_path.get("display_fields", [])
            field_names = [field.get("field") for field in display_fields if field.get("field")]

            logger.info(f"ğŸ“Š í‘œì‹œ í•„ë“œ ë¡œë“œ: {len(field_names)}ê°œ")
            return field_names
        except Exception as e:
            logger.warning(f"display_fields ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def _get_essential_columns(self) -> str:
        """âš¡ ì„±ëŠ¥ ìµœì í™”: field_settings.json ê¸°ë°˜ ë™ì  ì»¬ëŸ¼ ì„ íƒ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)"""
        if not self.category or not self.subcategory:
            logger.info("ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—†ìŒ - SELECT * ì‚¬ìš©")
            return "*"

        try:
            # ì‹¤ì œ íŒŒì¼ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            available_fields = self._get_available_fields()
            if not available_fields:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ - SELECT * ì‚¬ìš©")
                return "*"

            # 1. search_fieldsì—ì„œ ê²€ìƒ‰ ëŒ€ìƒ ì»¬ëŸ¼ë“¤ ì¶”ì¶œ
            search_columns = self._get_search_fields_from_config()

            # 2. display_fieldsì—ì„œ í‘œì‹œ ì»¬ëŸ¼ë“¤ ì¶”ì¶œ
            display_columns = self._get_display_fields_from_config()

            # 3. ë™ì  ì •ë ¬ ì»¬ëŸ¼ ê°ì§€ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸)
            sort_columns = []
            for date_col in ["cert_date", "crawl_date", "crawled_at", "ì„¤ë¦½ì¼"]:
                if date_col in available_fields:
                    sort_columns.append(date_col)
                    break

            # 4. í•„ìˆ˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (download ë“± ì¶”ê°€ í•„ë“œ í¬í•¨)
            all_columns = search_columns + display_columns + sort_columns
            if self.required_fields:
                all_columns.extend(self.required_fields)

            # 5. ì¤‘ë³µ ì œê±° ë° None ì œê±° (ìˆœì„œ ìœ ì§€)
            essential_cols = []
            seen = set()
            for col in all_columns:
                if col and col not in seen:
                    essential_cols.append(col)
                    seen.add(col)

            # 6. ì¶”ê°€ë¡œ ë™ì ìœ¼ë¡œ í•„ìš”í•œ ì»¬ëŸ¼ í¬í•¨ (ê²€ìƒ‰ ì‹œ ë§¤í•‘ëœ í•„ë“œ ë“±)
            dynamic_extra = getattr(self, "dynamic_required_fields", [])
            if dynamic_extra:
                for col in dynamic_extra:
                    if col and col not in all_columns:
                        essential_cols.append(col)

            # 7. ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
            existing_cols = [col for col in essential_cols if col in available_fields]

            if not existing_cols:
                logger.warning("í•„ìˆ˜ ì»¬ëŸ¼ ì¤‘ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì´ ì—†ìŒ - SELECT * ì‚¬ìš©")
                return "*"

            # ëˆ„ë½ëœ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ë¡œ ì•Œë¦¼
            missing_cols = [col for col in essential_cols if col not in available_fields]
            if missing_cols:
                logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤: {missing_cols} (íŒŒì¼ì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ)")

            # ì»¬ëŸ¼ëª…ì„ ìŒë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ DuckDB ì•ˆì „ì„± í™•ë³´
            quoted_cols = [f'"{col}"' for col in existing_cols]
            columns_str = ", ".join(quoted_cols)

            logger.info(f"ğŸ“Š ì„±ëŠ¥ ìµœì í™”: {len(existing_cols)}ê°œ í•„ìˆ˜ ì»¬ëŸ¼ ì„ íƒ ({self.category}/{self.subcategory})")
            return columns_str

        except Exception as e:
            logger.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return "*"

    def _build_base_query(self, conn: duckdb.DuckDBPyConnection, file_size_mb: float) -> str:
        """Parquet ë˜ëŠ” JSON íŒŒì¼ì— ë”°ë¥¸ ê¸°ë³¸ ì¿¼ë¦¬ ìƒì„±"""
        if self.is_url:
            # URLì¸ ê²½ìš° - ì„±ëŠ¥ì„ ìœ„í•´ Parquet ìš°ì„  ì‚¬ìš©
            abs_file_path = self.file_path_str

            # JSON URLì„ Parquet URLë¡œ ë³€í™˜ ì‹œë„ (ì„±ëŠ¥ ìµœì í™”)
            if abs_file_path.endswith('.json'):
                parquet_url = abs_file_path.replace('.json', '.parquet')
                logger.info(f"âš¡ ì„±ëŠ¥ ìµœì í™”: JSON â†’ Parquet ë³€í™˜ ì‹œë„ ({parquet_url.split('/')[-1]})")
                abs_file_path = parquet_url

            if abs_file_path.endswith(('.parquet', '.duckdb')):
                logger.info(f"ğŸš€ Blob Tabular íŒŒì¼ ì‚¬ìš©: {abs_file_path.split('/')[-1]}")
                essential_cols = self._get_essential_columns()
                table_expr = self._get_table_expression(conn, abs_file_path)
                return f"SELECT {essential_cols} FROM {table_expr}"
            else:
                logger.info(f"ğŸ“„ Blob JSON íŒŒì¼ ì‚¬ìš© (Fallback): {abs_file_path.split('/')[-1]}")
                read_options = ""
        else:
            # ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš° (self.file_pathê°€ Path ê°ì²´ì—¬ì•¼ í•¨)
            if hasattr(self.file_path, 'resolve'):
                abs_file_path = str(self.file_path.resolve())
            else:
                # ë¬¸ìì—´ì¸ ê²½ìš° Path ê°ì²´ë¡œ ë³€í™˜
                abs_file_path = str(Path(self.file_path).resolve())
            tabular_path = self._resolve_tabular_path()
            if tabular_path:
                logger.info(f"Tabular íŒŒì¼ ì‚¬ìš©: {Path(tabular_path).name}")
                essential_cols = self._get_essential_columns()
                table_expr = self._get_table_expression(conn, tabular_path)
                return f"SELECT {essential_cols} FROM {table_expr}"
            
            # Parquetì´ ì—†ìœ¼ë©´ ê¸°ì¡´ JSON ë°©ì‹ ì‚¬ìš© (Fallback)
            logger.info(f"JSON íŒŒì¼ ì‚¬ìš© (Fallback): {Path(abs_file_path).name}")
            read_options = ""
        
        if file_size_mb > 10:
            read_options = ", maximum_object_size=2147483648"
            
        structure = self._detect_json_structure()
        
        if structure == 'array':
            # JSON ë°°ì—´: [{"field1": "value1"}, {"field2": "value2"}]
            essential_cols = self._get_essential_columns()
            if essential_cols == "*":
                return f"SELECT * FROM read_json_auto('{abs_file_path}'{read_options})"
            else:
                return f"SELECT {essential_cols} FROM read_json_auto('{abs_file_path}'{read_options})"
            
        elif structure == 'nested_safetykorea':
            # SafetyKorea êµ¬ì¡°: {"LEDë¨í”„_details": [...]}
            return f'SELECT UNNEST("LEDë¨í”„_details") as item FROM read_json_auto(\'{abs_file_path}\'{read_options})'
            
        elif structure == 'nested_data':
            # data í•„ë“œì— ë°°ì—´: {"data": [...]}
            return f'SELECT UNNEST("data") as item FROM read_json_auto(\'{abs_file_path}\'{read_options})'
            
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ ë°°ì—´ë¡œ ì‹œë„
            essential_cols = self._get_essential_columns()
            if essential_cols == "*":
                return f"SELECT * FROM read_json_auto('{abs_file_path}'{read_options})"
            else:
                return f"SELECT {essential_cols} FROM read_json_auto('{abs_file_path}'{read_options})"
    
    def _get_available_fields(self) -> list:
        """íŒŒì¼ì—ì„œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œëª…ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ìŠ¤í‚¤ë§ˆ ìºì‹œ ìš°ì„ )"""

        # ğŸš€ ì„±ëŠ¥ ìµœì í™”: ìºì‹œëœ ìŠ¤í‚¤ë§ˆê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (R2 í†µì‹  ì œê±°)
        cached_fields: Optional[List[str]] = None
        cache_key = self.file_path_str if self.is_url else str(self.file_path)
        file_name = _extract_file_name(cache_key)

        # ğŸ”¥ ë™ì  ìŠ¤í‚¤ë§ˆ ë¡œë“œ ëŒ€ìƒ ì²´í¬ (3,4,5ë²ˆ ë°ì´í„°ì…‹)
        is_dynamic_schema_target = self._should_use_dynamic_schema(cache_key, file_name)

        if is_dynamic_schema_target:
            # ë™ì  ìŠ¤í‚¤ë§ˆ ë¡œë“œ ëŒ€ìƒì€ ìºì‹œë¥¼ ê±´ë„ˆë›°ê³  ì§ì ‘ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
            logger.info(f"ğŸ”„ ë™ì  ìŠ¤í‚¤ë§ˆ ë¡œë“œ (ìºì‹œ ê±´ë„ˆë›°ê¸°): {file_name or cache_key}")
            cached_fields = None
        else:
            # ê¸°ì¡´ ìºì‹œ ë¡œì§ ì‚¬ìš©
            if cache_key in SCHEMA_CACHE_BY_URL:
                cached_fields = SCHEMA_CACHE_BY_URL[cache_key]
                logger.info(f"âš¡ ìŠ¤í‚¤ë§ˆ ìºì‹œ ì‚¬ìš©: {len(cached_fields)}ê°œ ì»¬ëŸ¼ (ìºì‹œ í‚¤: {cache_key})")
            elif file_name and file_name in SCHEMA_CACHE_BY_FILENAME:
                cached_fields = SCHEMA_CACHE_BY_FILENAME[file_name]
                logger.info(f"âš¡ ìŠ¤í‚¤ë§ˆ ìºì‹œ ì‚¬ìš© (íŒŒì¼ëª…): {file_name} â†’ {len(cached_fields)}ê°œ ì»¬ëŸ¼")
            else:
                # ğŸ”¥ 2025ëª¨ë“œ BLOB URL ì§€ì›: DuckDB íŒŒì¼ ìŠ¤í‚¤ë§ˆ ì¶”ë¡ 
                cached_fields = self._try_duckdb_blob_schema_mapping(cache_key, file_name)
                if cached_fields:
                    logger.info(f"âš¡ DuckDB BLOB URL ìŠ¤í‚¤ë§ˆ ë§¤í•‘: {file_name} â†’ {len(cached_fields)}ê°œ ì»¬ëŸ¼")

        if cached_fields is not None and not is_dynamic_schema_target:
            return cached_fields

        # ìºì‹œì— ì—†ëŠ” ê²½ìš°ì—ë§Œ ì›ë˜ ë¡œì§ ìˆ˜í–‰ (fallback)
        logger.debug(f"ìŠ¤í‚¤ë§ˆ ìºì‹œ ë¯¸ìŠ¤, ì›ê²© ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ìˆ˜í–‰: {cache_key}")

        try:
            # **ì„±ëŠ¥ ìµœì í™”: ìµœì í™”ëœ DuckDB ì—°ê²° ì‚¬ìš©**
            conn = _create_optimized_connection()
            try:
                if self.is_url:
                    # URLì¸ ê²½ìš° (R2 URLì€ ì´ë¯¸ parquet)
                    if self.file_path_str.endswith(('.parquet', '.duckdb')):
                        logger.info(f"Blob í•„ë“œ ì¡°íšŒ: {self.file_path_str.split('/')[-1]}")
                        table_expr = self._get_table_expression(conn, self.file_path_str)
                        base_query = f"SELECT * FROM {table_expr} LIMIT 1"
                        result = conn.execute(base_query)
                        columns = [desc[0] for desc in result.description]
                        return self._cache_schema(columns)
                    else:
                        logger.info(f"Blob JSON í•„ë“œ ì¡°íšŒ: {self.file_path_str.split('/')[-1]}")
                        # JSON URLì˜ ê²½ìš° ê¸°ë³¸ í•„ë“œ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” parquetë§Œ ì‚¬ìš©)
                        return self._cache_schema(["id", "name", "company", "date"])
                else:
                    # ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš° (self.file_pathê°€ Path ê°ì²´ì—¬ì•¼ í•¨)
                    if hasattr(self.file_path, 'resolve'):
                        abs_file_path = str(self.file_path.resolve())
                    else:
                        # ë¬¸ìì—´ì¸ ê²½ìš° Path ê°ì²´ë¡œ ë³€í™˜
                        abs_file_path = str(Path(self.file_path).resolve())
                    tabular_path = self._resolve_tabular_path()

                    if tabular_path:
                        table_expr = self._get_table_expression(conn, tabular_path)
                        base_query = f"SELECT * FROM {table_expr} LIMIT 1"
                        result = conn.execute(base_query)
                        columns = [desc[0] for desc in result.description]
                        return self._cache_schema(columns)
                    else:
                        # JSON íŒŒì¼ì˜ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
                        base_query = self._build_base_query(conn, 1.0)  # ì‘ì€ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸

                    # ì²« ë²ˆì§¸ ë ˆì½”ë“œë¡œ í•„ë“œ í™•ì¸
                    result = conn.execute(f"{base_query} LIMIT 1")
                    records = result.fetchall()

                    if not records:
                        return self._cache_schema([])

                    structure = self._detect_json_structure()
                    record = records[0]

                    if structure in ['nested_safetykorea', 'nested_data']:
                        # item í•„ë“œ ë¶„ì„
                        item = record[0]
                        if hasattr(item, '_asdict'):
                            return self._cache_schema(list(item._asdict().keys()))
                        elif isinstance(item, dict):
                            return self._cache_schema(list(item.keys()))
                        else:
                            return self._cache_schema([])
                    else:
                        # ì¼ë°˜ ë°°ì—´ êµ¬ì¡°
                        return self._cache_schema([desc[0] for desc in result.description])
            finally:
                conn.close()

        except Exception as e:
            logger.warning(f"í•„ë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _is_field_case_insensitive(self, field_name: str) -> bool:
        """í•„ë“œê°€ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨ì¸ì§€ í™•ì¸"""
        # case_insensitive_fieldsì— ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •ëœ ê²½ìš°
        if field_name in self.case_config.get("case_insensitive_fields", {}):
            return self.case_config["case_insensitive_fields"][field_name]
        
        # case_sensitive_fieldsì— ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •ëœ ê²½ìš° (ë°˜ëŒ€ë¡œ)
        if field_name in self.case_config.get("case_sensitive_fields", {}):
            return not self.case_config["case_sensitive_fields"][field_name]  # Falseë©´ case_sensitiveì´ë¯€ë¡œ case_insensitiveëŠ” False
        
        # ê¸°ë³¸ê°’ ì‚¬ìš©
        return self.case_config.get("default_case_insensitive", False)

    @staticmethod
    def _build_date_order_expression(field_name: str, table_alias: str = "") -> str:
        """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ DATEë¡œ ì •ë ¬í•˜ê¸° ìœ„í•œ í‘œí˜„ì‹ ìƒì„±"""
        prefix = f'{table_alias}' if table_alias else ''
        column_expr = f'{prefix}"{field_name}"'
        trimmed_expr = f"TRIM({column_expr})"

        return (
            "CASE "
            f"WHEN {column_expr} IS NULL OR {trimmed_expr} = '' THEN NULL "
            f"WHEN LENGTH({trimmed_expr}) = 8 THEN TRY_STRPTIME({column_expr}, '%Y%m%d') "
            f"WHEN LENGTH({trimmed_expr}) = 14 THEN TRY_STRPTIME({column_expr}, '%Y%m%d%H%M%S') "
            f"ELSE TRY_CAST({column_expr} AS DATE) "
            "END"
        )
    
    def _build_where_clause(self, keyword: str, search_field: str) -> tuple[str, list]:
        """ê²€ìƒ‰ ì¡°ê±´ SQL WHERE ì ˆ ìƒì„± (íŒŒë¼ë¯¸í„° ë°”ì¸ë”© ì‚¬ìš©)"""
        if not keyword:
            return "1=1", []  # ëª¨ë“  ê²°ê³¼ ë°˜í™˜, íŒŒë¼ë¯¸í„° ì—†ìŒ

        exact_match_fields = {"business_number", "ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸", "ftc_business_number"}
        if search_field in exact_match_fields:
            cleaned_keyword = keyword.replace('-', '').replace(' ', '')
            field_aliases = [search_field]
            if search_field == "business_number":
                field_aliases.append("ftc_business_number")
            if search_field == "ftc_business_number":
                field_aliases.append("business_number")

            conditions = []
            parameters = []
            available_fields = self._get_available_fields()

            for field in field_aliases:
                if field in available_fields:
                    conditions.append(f"REPLACE(REPLACE(CAST(\"{field}\" AS VARCHAR), '-', ''), ' ', '') = ?")
                    parameters.append(cleaned_keyword)

            if conditions:
                where_clause = " OR ".join(conditions)
                return where_clause, parameters
            else:
                return "1=0", []

        # Parquet íŒŒì¼ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        if self.is_url:
            # URLì¸ ê²½ìš° (R2 URLì€ parquet)
            using_parquet = self.file_path_str.endswith(('.parquet', '.duckdb'))
        else:
            # ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš° (self.file_pathê°€ Path ê°ì²´ì—¬ì•¼ í•¨)
            if hasattr(self.file_path, 'resolve'):
                abs_file_path = str(self.file_path.resolve())
            else:
                # ë¬¸ìì—´ì¸ ê²½ìš° Path ê°ì²´ë¡œ ë³€í™˜
                abs_file_path = str(Path(self.file_path).resolve())
            parquet_path = abs_file_path.replace('.json', '.parquet')
            suffix = Path(abs_file_path).suffix.lower()
            using_parquet = (suffix == '.duckdb' and Path(abs_file_path).exists()) or Path(parquet_path).exists()
        
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ ê°€ì ¸ì˜¤ê¸°
        available_fields = self._get_available_fields()
        
        # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ê²€ìƒ‰ ëŒ€ìƒ ê²°ì •
        if using_parquet:
            # Parquet: ì •ê·œí™”ëœ í…Œì´ë¸” êµ¬ì¡°, í…Œì´ë¸” ë³„ì¹­ ì—†ìŒ
            table_alias = ""
        else:
            # JSON: êµ¬ì¡°ì— ë”°ë¥¸ í…Œì´ë¸” ë³„ì¹­ ê²°ì •
            structure = self._detect_json_structure()
            if structure in ['nested_safetykorea', 'nested_data']:
                table_alias = "item."
            else:
                table_alias = ""
            
        
        # **ìƒˆë¡œìš´ ê²€ìƒ‰ í•„ë“œ ë§¤í•‘: ì—…ì²´ëª…, ëª¨ë¸ëª…, ì œí’ˆëª…ë§Œ ì§€ì›**
        field_mappings = {
            "company_name": ["ì—…ì²´ëª…", "maker_name", "entrprsNm", "ìƒí˜¸/ë²•ì¸ëª…", "ì‚¬ì—…ìëª…"],
            "model_name": ["ëª¨ë¸ëª…", "model_name"],
            "product_name": ["ì œí’ˆëª…", "product_name", "prductNm", "í’ˆëª©ëª…"]
        }
        
        target_fields = field_mappings.get(search_field, [search_field])

        # **ê²€ìƒ‰ í•„ë“œ ìˆ˜ì§‘ - ëª¨ë“  ë§¤ì¹­ í•„ë“œì—ì„œ ê²€ìƒ‰**
        existing_fields = []
        for field in target_fields:
            if field in available_fields:
                existing_fields.append(field)  # ëª¨ë“  ë§¤ì¹­ í•„ë“œ ìˆ˜ì§‘
        
        if not existing_fields:
            # ë§¤í•‘ëœ í•„ë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ í•„ë“œëª…ìœ¼ë¡œ ì‹œë„
            existing_fields = [search_field] if search_field in available_fields else []
        
        conditions = []
        parameters = []
        
        for field in existing_fields:
            # í•„ë“œë³„ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì„¤ì • í™•ì¸
            is_case_insensitive = self._is_field_case_insensitive(field)

            # **ê²€ìƒ‰ íŒ¨í„´ê³¼ ì—°ì‚°ì ê²°ì • - ì¸ì¦ë²ˆí˜¸/ì‹ ê³ ë²ˆí˜¸ëŠ” ì •í™• ë§¤ì¹­**
            search_pattern, operator = _get_search_pattern_and_operator(keyword, field)

            # **ì„±ëŠ¥ ìµœì í™”: ê²€ìƒ‰ì–´ ì‚¬ì „ì²˜ë¦¬ - í•„ë“œë³„ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì— ë”°ë¼ ë¯¸ë¦¬ ë³€í™˜**
            if is_case_insensitive and operator == 'LIKE':
                search_pattern = f"%{keyword.lower()}%"

            if using_parquet:
                # Parquet: ì•ˆì „í•œ CAST ì ìš©
                if is_case_insensitive and operator == 'LIKE':
                    # **ì„±ëŠ¥ ìµœì í™”: ì»¬ëŸ¼ë§Œ LOWER, ê²€ìƒ‰ì–´ëŠ” ì´ë¯¸ Pythonì—ì„œ ë³€í™˜ë¨**
                    conditions.append(f"LOWER(CAST({table_alias}\"{field}\" AS VARCHAR)) {operator} ?")
                    logger.info(f"í•„ë“œ '{field}': ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨ (ì»¬ëŸ¼ë§Œ LOWER ì ìš©), ì—°ì‚°ì: {operator}")
                else:
                    # ëŒ€ì†Œë¬¸ì êµ¬ë¶„í•¨ ë˜ëŠ” ì •í™• ë§¤ì¹­: LOWER í•¨ìˆ˜ ì‚¬ìš© ì•ˆí•¨
                    conditions.append(f"CAST({table_alias}\"{field}\" AS VARCHAR) {operator} ?")
                    logger.info(f"í•„ë“œ '{field}': ëŒ€ì†Œë¬¸ì êµ¬ë¶„í•¨ ë˜ëŠ” ì •í™•ë§¤ì¹­, ì—°ì‚°ì: {operator}")
            else:
                # JSON: ë³µí•© íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰
                if is_case_insensitive and operator == 'LIKE':
                    # **ì„±ëŠ¥ ìµœì í™”: ì»¬ëŸ¼ë§Œ LOWER, ê²€ìƒ‰ì–´ëŠ” ì´ë¯¸ Pythonì—ì„œ ë³€í™˜ë¨**
                    conditions.append(f"LOWER(CAST({table_alias}\"{field}\" AS VARCHAR)) {operator} ?")
                    logger.info(f"í•„ë“œ '{field}': ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨ (ì»¬ëŸ¼ë§Œ LOWER ì ìš©), ì—°ì‚°ì: {operator}")
                else:
                    # ëŒ€ì†Œë¬¸ì êµ¬ë¶„í•¨ ë˜ëŠ” ì •í™• ë§¤ì¹­: LOWER í•¨ìˆ˜ ì‚¬ìš© ì•ˆí•¨
                    conditions.append(f"CAST({table_alias}\"{field}\" AS VARCHAR) {operator} ?")
                    logger.info(f"í•„ë“œ '{field}': ëŒ€ì†Œë¬¸ì êµ¬ë¶„í•¨ ë˜ëŠ” ì •í™•ë§¤ì¹­, ì—°ì‚°ì: {operator}")
            parameters.append(search_pattern)
        
        where_clause = " OR ".join(conditions) if conditions else "1=1"
        logger.info(f"'{search_field}' ê²€ìƒ‰: {len(existing_fields)}ê°œ í•„ë“œ - {existing_fields}")

        # ë””ë²„ê·¸ ì •ë³´ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ì €ì¥ (API ì‘ë‹µì—ì„œ ì‚¬ìš©)
        self.debug_info = {
            "search_field": search_field,
            "existing_fields": existing_fields,
            "field_count": len(existing_fields),
            "where_clause": where_clause
        }

        current_required = getattr(self, "dynamic_required_fields", [])
        for col in existing_fields:
            if col and col not in current_required:
                current_required.append(col)
        self.dynamic_required_fields = current_required

        return where_clause, parameters

    def _build_filter_conditions(self, filters: Optional[Dict[str, Any]], table_alias: str = "") -> tuple:
        """ì¶”ê°€ í•„í„° ì¡°ê±´ ìƒì„±

        Args:
            filters: í•„í„° ì¡°ê±´ ë”•ì…”ë„ˆë¦¬
                - date_range: {'start': 'YYYY-MM-DD', 'end': 'YYYY-MM-DD'}
                - certification_type: ['KC', 'CE', 'FCC'] 
                - company_type: ['manufacturer', 'importer']
                - exclude_keywords: ['test', 'sample']
                - numeric_range: {'field': 'price', 'min': 100, 'max': 1000}
        
        Returns:
            tuple: (filter_conditions_string, parameters_list)
        """
        if not filters:
            return "1=1", []

        conditions = []
        parameters = []
        available_fields = self._get_available_fields()

        # ë‚ ì§œ ë²”ìœ„ í•„í„° - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë‚ ì§œ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        if 'date_range' in filters:
            date_range = filters['date_range']
            # ë°ì´í„°ì…‹ë³„ ë‚ ì§œ ì»¬ëŸ¼ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ìˆœ)
            date_columns = ["ì¸ì¦ì¼ì", "ì¸ì¦ë³€ê²½ì¼ì", "ì„œëª…ì¼ì", "ì¸ì¦ë§Œë£Œì¼ì", "ì™„ë£Œì¼", "ë“±ë¡ì¼ì", "date", "cert_date"]
            existing_date_column = None
            for col in date_columns:
                if col in available_fields:
                    existing_date_column = col
                    break

            if existing_date_column:
                logger.info(f"ë‚ ì§œ í•„í„° ì ìš©: {existing_date_column} ì»¬ëŸ¼ ì‚¬ìš©")
                if 'start' in date_range:
                    conditions.append(f"CAST({table_alias}\"{existing_date_column}\" AS DATE) >= ?")
                    parameters.append(date_range['start'])
                if 'end' in date_range:
                    conditions.append(f"CAST({table_alias}\"{existing_date_column}\" AS DATE) <= ?")
                    parameters.append(date_range['end'])
            else:
                logger.warning("ë‚ ì§œ í•„í„° ìŠ¤í‚µ: í•´ë‹¹ ë°ì´í„°ì…‹ì— ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì¸ì¦ íƒ€ì… í•„í„° - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì¸ì¦ë²ˆí˜¸ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        if 'certification_type' in filters:
            cert_types = filters['certification_type']
            if cert_types:
                # ë°ì´í„°ì…‹ë³„ ì¸ì¦ë²ˆí˜¸ ì»¬ëŸ¼ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ìˆœ)
                cert_columns = ["ì¸ì¦ë²ˆí˜¸", "certification_no", "license_no", "cert_no", "registration_no"]
                existing_cert_column = None
                for col in cert_columns:
                    if col in available_fields:
                        existing_cert_column = col
                        break

                if existing_cert_column:
                    logger.info(f"ì¸ì¦ íƒ€ì… í•„í„° ì ìš©: {existing_cert_column} ì»¬ëŸ¼ ì‚¬ìš©")
                    # IN ì ˆì„ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
                    placeholders = ','.join(['?' for _ in cert_types])
                    conditions.append(f"UPPER(CAST({table_alias}\"{existing_cert_column}\" AS VARCHAR)) RLIKE ANY(ARRAY[{placeholders}])")
                    parameters.extend([f".*{cert_type}.*" for cert_type in cert_types])
                else:
                    logger.warning("ì¸ì¦ íƒ€ì… í•„í„° ìŠ¤í‚µ: í•´ë‹¹ ë°ì´í„°ì…‹ì— ì¸ì¦ë²ˆí˜¸ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì—…ì²´ íƒ€ì… í•„í„° (ì œì¡°ì—…ì²´/ìˆ˜ì…ì—…ì²´) - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ìˆ˜ì…ì ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        if 'company_type' in filters:
            company_types = filters['company_type']
            # ë°ì´í„°ì…‹ë³„ ìˆ˜ì…ì ì»¬ëŸ¼ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ìˆœ)
            importer_columns = ["ìˆ˜ì…ì", "importer", "import_company", "importerName", "ìˆ˜ì…ì—…ì²´"]
            existing_importer_column = None
            for col in importer_columns:
                if col in available_fields:
                    existing_importer_column = col
                    break

            if existing_importer_column:
                logger.info(f"ì—…ì²´ íƒ€ì… í•„í„° ì ìš©: {existing_importer_column} ì»¬ëŸ¼ ì‚¬ìš©")
                ct_conditions = []
                if 'manufacturer' in company_types:
                    ct_conditions.append(f"(CAST({table_alias}\"{existing_importer_column}\" AS VARCHAR) IS NULL OR CAST({table_alias}\"{existing_importer_column}\" AS VARCHAR) = '')")
                if 'importer' in company_types:
                    ct_conditions.append(f"(CAST({table_alias}\"{existing_importer_column}\" AS VARCHAR) IS NOT NULL AND CAST({table_alias}\"{existing_importer_column}\" AS VARCHAR) != '')")

                if ct_conditions:
                    conditions.append("(" + " OR ".join(ct_conditions) + ")")
            else:
                logger.warning("ì—…ì²´ íƒ€ì… í•„í„° ìŠ¤í‚µ: í•´ë‹¹ ë°ì´í„°ì…‹ì— ìˆ˜ì…ì ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì œì™¸ í‚¤ì›Œë“œ í•„í„° - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì œí’ˆëª…, ì—…ì²´ëª… ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        if 'exclude_keywords' in filters:
            exclude_keywords = filters['exclude_keywords']
            # ë°ì´í„°ì…‹ë³„ ì œí’ˆëª…, ì—…ì²´ëª… ì»¬ëŸ¼ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ìˆœ)
            product_columns = ["ì œí’ˆëª…", "product_name", "prductNm", "í’ˆëª©ëª…", "ê¸°ìì¬ëª…ì¹­"]
            company_columns = ["ì—…ì²´ëª…", "company_name", "entrprsNm", "ìƒí˜¸/ë²•ì¸ëª…", "ì‚¬ì—…ìëª…", "maker_name"]

            existing_product_column = None
            existing_company_column = None

            for col in product_columns:
                if col in available_fields:
                    existing_product_column = col
                    break

            for col in company_columns:
                if col in available_fields:
                    existing_company_column = col
                    break

            for keyword in exclude_keywords:
                exclude_conditions = []
                if existing_product_column:
                    exclude_conditions.append(f"LOWER(CAST({table_alias}\"{existing_product_column}\" AS VARCHAR)) LIKE ?")
                    parameters.append(f"%{keyword.lower()}%")
                if existing_company_column:
                    exclude_conditions.append(f"LOWER(CAST({table_alias}\"{existing_company_column}\" AS VARCHAR)) LIKE ?")
                    parameters.append(f"%{keyword.lower()}%")

                if exclude_conditions:
                    conditions.append(f"NOT ({' OR '.join(exclude_conditions)})")
                    logger.info(f"ì œì™¸ í‚¤ì›Œë“œ í•„í„° ì ìš©: '{keyword}' - ì‚¬ìš© ì»¬ëŸ¼: {[c for c in [existing_product_column, existing_company_column] if c]}")
                else:
                    logger.warning(f"ì œì™¸ í‚¤ì›Œë“œ í•„í„° ìŠ¤í‚µ: '{keyword}' - ì œí’ˆëª…/ì—…ì²´ëª… ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ìˆ«ì ë²”ìœ„ í•„í„°
        if 'numeric_range' in filters:
            numeric_range = filters['numeric_range']
            field = numeric_range.get('field', 'price')
            if 'min' in numeric_range:
                conditions.append(f"TRY_CAST({table_alias}\"{field}\" AS DOUBLE) >= ?")
                parameters.append(numeric_range['min'])
            if 'max' in numeric_range:
                conditions.append(f"TRY_CAST({table_alias}\"{field}\" AS DOUBLE) <= ?")
                parameters.append(numeric_range['max'])
        
        # ê²°ê³¼ ì¡°í•©
        if conditions:
            return " AND ".join(conditions), parameters
        else:
            return "1=1", []

    def get_distinct_values(self, field_name: str, limit: int = 100) -> List[Any]:
        """êµ¬ì¡°í™”ëœ ë°ì´í„° íŒŒì¼ì—ì„œ íŠ¹ì • í•„ë“œì˜ DISTINCT ê°’ì„ ì¡°íšŒ"""
        tabular_path = self._resolve_tabular_path()
        if not tabular_path:
            raise ValueError("Distinct ì¡°íšŒëŠ” Parquet/DuckDB íŒŒì¼ì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤")

        conn, conn_lock = self._get_connection()
        with conn_lock:
            table_expr = self._get_table_expression(conn, tabular_path)
            query = (
                f'SELECT DISTINCT "{field_name}" FROM {table_expr} '
                f'WHERE "{field_name}" IS NOT NULL LIMIT {limit}'
            )
            rows = conn.execute(query).fetchall()
            return [row[0] for row in rows if row and row[0] is not None]

    def _get_file_size_mb(self) -> float:
        """íŒŒì¼ í¬ê¸° (MB) ë°˜í™˜"""
        if self.is_url:
            # URLì¸ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ íŒŒì¼ë¡œ ê°€ì • (Blob íŒŒì¼ì€ ì¼ë°˜ì ìœ¼ë¡œ í° íŒŒì¼)
            logger.info(f"URL íŒŒì¼ì€ í¬ê¸°ë¥¼ ì¶”ì •: 100MBë¡œ ê°€ì •")
            return 100.0
        else:
            # ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš° ì‹¤ì œ í¬ê¸° ê³„ì‚° (self.file_pathëŠ” Path ê°ì²´)
            if hasattr(self.file_path, 'stat'):
                size_bytes = self.file_path.stat().st_size
                return size_bytes / (1024 * 1024)
            else:
                # ë¬¸ìì—´ì¸ ê²½ìš° Path ê°ì²´ë¡œ ë³€í™˜
                file_path_obj = Path(self.file_path)
                size_bytes = file_path_obj.stat().st_size
                return size_bytes / (1024 * 1024)
    
    async def search_streaming(self,
                             keyword: Optional[str] = None,
                             search_field: str = "all",
                             limit: Optional[int] = 20,
                             page: int = 1,
                             filters: Optional[Dict[str, Any]] = None,
                             collect_results: bool = True,
                             chunk_callback: Optional[Callable[[List[Dict[str, Any]], int], None]] = None,
                             chunk_size: int = 1000) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ SafetyKorea ë°ì´í„° ê²€ìƒ‰

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            search_field: ê²€ìƒ‰ í•„ë“œ ("all", "product_name", "company_name", etc.)
            limit: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ (Noneì´ë©´ ì „ì²´)
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            filters: ì¶”ê°€ í•„í„° ì¡°ê±´ ë”•ì…”ë„ˆë¦¬
            collect_results: ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì—¬ë¶€ (False ë©´ chunk_callbackìœ¼ë¡œ ì „ë‹¬)
            chunk_callback: collect_results=Falseì¼ ë•Œ ê²°ê³¼ ì²­í¬ë¥¼ ì²˜ë¦¬í•  ì½œë°±
            chunk_size: chunk_callbackìœ¼ë¡œ ì „ë‹¬í•  ë°°ì¹˜ í¬ê¸°

        Returns:
            Dict: ê²€ìƒ‰ ê²°ê³¼ ë° í†µê³„ ì •ë³´
        """
        
        def _execute_query():
            start_time = time.time()
            conn, conn_lock = self._get_connection()

            with conn_lock:
                # ì„œë²„ì‚¬ì´ë“œ í˜ì´ì§€ë„¤ì´ì…˜: pageì™€ limitìœ¼ë¡œ offset ê³„ì‚°
                effective_limit = None if limit is None or limit <= 0 else limit
                offset = (page - 1) * effective_limit if effective_limit else 0
                streaming_mode = not collect_results and chunk_callback is not None
                logger.info(f"ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜: page={page}, limit={limit}, offset={offset}, streaming={streaming_mode}")

                file_size_mb = self._get_file_size_mb()
                logger.info(f"íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB")

                where_clause, where_parameters = self._build_where_clause(keyword, search_field)
                filter_clause, filter_parameters = self._build_filter_conditions(filters)

                try:
                    if file_size_mb > 1000:
                        logger.warning(f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ({file_size_mb:.1f}MB) ê°ì§€. ì™¸ë¶€ íŒŒì¼ ë¶„í•  ë˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ê¶Œì¥")

                    base_query = self._build_base_query(conn, file_size_mb)

                    tabular_path = self._resolve_tabular_path()
                    using_parquet = tabular_path is not None

                    if using_parquet:
                        # ğŸ¯ download_fields ë“± required_fields ì ìš©
                        essential_cols = self._get_essential_columns()
                        select_clause = essential_cols if essential_cols != "*" else "*"
                        available_fields = self._get_available_fields()
                        order_by = None
                        logger.info(f"ğŸ“Š ì„±ëŠ¥ ìµœì í™”: {len(essential_cols.split(',')) if essential_cols != '*' else 'ì „ì²´'}ê°œ ì»¬ëŸ¼ ì„ íƒ (dataA/{self.subcategory})")
                        logger.info("âš™ï¸ Parquet ê²°ê³¼ëŠ” íŒŒì¼ ì €ì¥ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤")

                    else:
                        structure = self._detect_json_structure()
                        available_fields = self._get_available_fields()

                        if structure in ['nested_safetykorea', 'nested_data']:
                            select_clause = "item"
                            sort_candidates = ["productName", "ì œí’ˆëª…", "ì—…ì²´ëª…", "ëª¨ë¸ëª…"]
                            order_field = next((f for f in sort_candidates if f in available_fields), None)
                            order_by = f"item.\"{order_field}\"" if order_field else "item"
                        else:
                            select_clause = "*"
                            date_candidates = [
                                "ì™„ë£Œì¼", "ì¸ì¦ì¼ì", "ì¸ì¦ë³€ê²½ì¼ì", "ì„œëª…ì¼ì", "ì¸ì¦ë§Œë£Œì¼ì",
                                "ì™„ë£Œì¼ì", "ë°œê¸‰ì¼", "ë§Œë£Œì¼", "ì„¤ë¦½ì¼",
                                "cert_date", "sign_date", "cert_chg_date",
                                "registration_date", "approval_date", "declaration_date", "recall_date",
                                "ë“±ë¡ì¼", "ìŠ¹ì¸ì¼", "ì‹ ê³ ì¼", "ë¦¬ì½œì¼", "ìƒì„±ì¼", "ìˆ˜ì •ì¼"
                            ]
                            date_candidates.extend([
                                "ì‹ ê³ ì¦ëª…ì„œ ë°œê¸‰ì¼", "ì‹œí—˜ì„±ì ì„œ ë§Œë£Œì¼", "ìœ í†µê¸°í•œ"
                            ])
                            product_candidates = ["í’ˆëª©", "ì œí’ˆëª…", "product_name", "ì—…ì²´ëª…", "company_name", "ìƒí˜¸", "ê¸°ìì¬ëª…ì¹­", "ëª¨ë¸ëª…", "model_name"]

                            date_field = next((f for f in date_candidates if f in available_fields), None)
                            product_field = next((f for f in product_candidates if f in available_fields), None)

                            order_parts = []
                            if date_field:
                                date_expr = self._build_date_order_expression(date_field)
                                order_parts.append(f'{date_expr} DESC NULLS LAST')
                            if product_field:
                                order_parts.append(f'"{product_field}" ASC')

                            if order_parts:
                                order_by = ', '.join(order_parts)
                                logger.info(f"ğŸ¯ ORDER BY ì ìš©ë¨: {order_by}")
                                logger.info(f"ğŸ“… ì„ íƒëœ date_field: {date_field}")
                                for col in [date_field, product_field]:
                                    if col and col not in self.dynamic_required_fields:
                                        self.dynamic_required_fields.append(col)
                            else:
                                first_field = available_fields[0] if available_fields else "1"
                                order_by = f'"{first_field}"' if first_field != "1" else "1"
                                logger.warning(f"âŒ ORDER BY ê¸°ë³¸ê°’ ì‚¬ìš©: {order_by} (ë‚ ì§œ í•„ë“œ ì—†ìŒ)")

                    combined_conditions = []
                    combined_parameters = []

                    if where_clause != "1=1":
                        combined_conditions.append(f"({where_clause})")
                        combined_parameters.extend(where_parameters)

                    if filter_clause != "1=1":
                        combined_conditions.append(f"({filter_clause})")
                        combined_parameters.extend(filter_parameters)

                    limit_clause = "" if effective_limit is None else f"LIMIT {effective_limit}"

                    order_clause = f"ORDER BY {order_by}" if order_by else ""

                    if combined_conditions:
                        final_where_clause = " AND ".join(combined_conditions)
                        filtered_query = f"""
                        SELECT {select_clause}, COUNT(*) OVER() as total_count
                        FROM ({base_query})
                        WHERE {final_where_clause}
                        {order_clause}
                        {limit_clause} OFFSET {offset}
                        """
                        count_query = None
                    else:
                        filtered_query = f"""
                        SELECT {select_clause}, COUNT(*) OVER() as total_count
                        FROM ({base_query})
                        {order_clause}
                        {limit_clause} OFFSET {offset}
                        """
                        count_query = None
                        combined_parameters = []

                    logger.info("DuckDB ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘...")

                    result = conn.execute(filtered_query, combined_parameters)

                    results = []
                    total_processed = 0
                    total_count_window = None
                    batch_fetch_size = chunk_size if streaming_mode else 1000
                    chunk_buffer: List[Dict[str, Any]] = [] if streaming_mode else []

                    if using_parquet:
                        structure = 'parquet'
                    else:
                        structure = self._detect_json_structure()

                    try:
                        # fetchallì„ ì‚¬ìš©í•œ ì•ˆì „í•œ ë°ì´í„° ì²˜ë¦¬ (ë°°ì¹˜ ë¡œì§ ìœ ì§€)
                        while True:
                            batch = result.fetchmany(batch_fetch_size)
                            if not batch:
                                break

                            for row in batch:
                                if using_parquet:
                                    column_names = [desc[0] for desc in result.description]
                                    record = dict(zip(column_names, row)) if row else None
                                elif structure in ['nested_safetykorea', 'nested_data']:
                                    raw_record = row[0] if row else None
                                    if raw_record:
                                        if hasattr(raw_record, '_asdict'):
                                            record = raw_record._asdict()
                                        elif isinstance(raw_record, dict):
                                            record = raw_record
                                        else:
                                            record = {"error": f"Unexpected record type: {type(raw_record)}", "content": str(raw_record)}
                                    else:
                                        record = None
                                else:
                                    column_names = [desc[0] for desc in result.description]
                                    record = dict(zip(column_names, row)) if row else None

                                if record:
                                    if isinstance(record, dict) and 'total_count' in record:
                                        total_count_window = record['total_count']
                                        del record['total_count']

                                    total_processed += 1

                                    if streaming_mode:
                                        chunk_buffer.append(record)
                                    else:
                                        results.append(record)

                                    if effective_limit is not None and total_processed >= effective_limit:
                                        break

                            if streaming_mode and chunk_callback and chunk_buffer:
                                chunk_callback(chunk_buffer.copy(), total_processed)
                                chunk_buffer.clear()

                            if effective_limit is not None and total_processed >= effective_limit:
                                break

                        # ë§ˆì§€ë§‰ ë‚¨ì€ chunk_buffer ì²˜ë¦¬
                        if streaming_mode and chunk_callback and chunk_buffer:
                            chunk_callback(chunk_buffer.copy(), total_processed)
                            chunk_buffer.clear()

                    except Exception as batch_error:
                        logger.warning(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {batch_error}")

                    if count_query is None:
                        if results:
                            if total_count_window is not None:
                                total_count = total_count_window
                            else:
                                total_count = total_processed
                        else:
                            total_count = 0
                    else:
                        try:
                            count_result = conn.execute(count_query, combined_parameters).fetchall()
                            total_count = count_result[0][0] if count_result and count_result[0] else total_processed
                        except Exception as count_error:
                            logger.warning(f"ì¹´ìš´íŠ¸ ì¿¼ë¦¬ ì˜¤ë¥˜: {count_error}")
                            total_count = total_processed

                    processing_time = time.time() - start_time

                    if effective_limit:
                        total_pages = (total_count + effective_limit - 1) // effective_limit if effective_limit > 0 else 1
                        has_next = page < total_pages
                        items_per_page = effective_limit
                    else:
                        total_pages = 1
                        has_next = False
                        items_per_page = total_count
                    has_prev = page > 1

                    return {
                        "results": results if not streaming_mode else [],
                        "pagination": {
                            "total_count": total_count,
                            "total_pages": total_pages,
                            "current_page": page,
                            "items_per_page": items_per_page,
                            "has_next": has_next,
                            "has_prev": has_prev
                        },
                        "stats": {
                            "processed_records": total_processed,
                            "processing_time": round(processing_time, 2),
                            "records_per_second": int(total_processed / processing_time) if processing_time > 0 else 0,
                            "file_size_mb": round(file_size_mb, 1),
                            "performance_gain": round(113 / processing_time, 1) if processing_time > 0 else 0
                        },
                        "debug_info": self.debug_info if hasattr(self, 'debug_info') and self.debug_info else {},
                        "query_info": {
                            "keyword": keyword,
                            "search_field": search_field,
                            "page": page,
                            "limit": limit,
                            "offset": offset
                        }
                    }

                except Exception as e:
                    processing_time = time.time() - start_time

                    if "maximum_object_size" in str(e) or (file_size_mb > 500 and "Could not read" in str(e)):
                        error_msg = f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ({file_size_mb:.1f}MB) ì²˜ë¦¬ ì‹¤íŒ¨. GitHub Releases ì™¸ë¶€ ì €ì¥ ë˜ëŠ” íŒŒì¼ ë¶„í•  í•„ìš”"
                        logger.error(f"{error_msg}: {e}")

                        return {
                            "error": "large_file_processing_failed",
                            "message": error_msg,
                            "suggestion": "íŒŒì¼ì„ GitHub Releasesì— ì—…ë¡œë“œí•˜ê³  HTTP URLë¡œ ì ‘ê·¼í•˜ê±°ë‚˜, íŒŒì¼ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì„¸ìš”",
                            "file_size_mb": round(file_size_mb, 1),
                            "processing_time": round(processing_time, 2),
                            "alternative": "ê¸°ì¡´ ijson ë°©ì‹ìœ¼ë¡œ fallback ê°€ëŠ¥"
                        }
                    else:
                        logger.error(f"DuckDB ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                        return {
                            "error": "query_execution_failed",
                            "message": str(e),
                            "processing_time": round(processing_time, 2)
                        }

        # ë¹„ë™ê¸° ì‹¤í–‰
        result = await asyncio.to_thread(_execute_query)
        return result
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ - Connection Pool ì‚¬ìš©ìœ¼ë¡œ ê°œë³„ ì—°ê²° ê´€ë¦¬ ë¶ˆí•„ìš”"""
        # **ì„±ëŠ¥ ìµœì í™”: Connection Pool ì‚¬ìš©ìœ¼ë¡œ ê°œë³„ ì—°ê²° ê´€ë¦¬ ì œê±°**
        # Connection Poolì´ ìë™ìœ¼ë¡œ ì—°ê²°ì„ ê´€ë¦¬í•˜ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”
        logger.info("DuckDB Connection Pool ì‚¬ìš© ì¤‘ - ê°œë³„ ì—°ê²° ì¢…ë£Œ ë¶ˆí•„ìš”")

# í¸ì˜ í•¨ìˆ˜
async def duckdb_search_large_file(file_path: str,
                                  keyword: Optional[str] = None,
                                  search_field: str = "all",
                                  limit: Optional[int] = 20,
                                  page: int = 1,
                                  filters: Optional[Dict[str, Any]] = None,
                                  category: str = None,
                                  subcategory: str = None,
                                  result_type: str = None,
                                  collect_results: bool = True,
                                  chunk_callback: Optional[Callable[[List[Dict[str, Any]], int], None]] = None,
                                  chunk_size: int = 1000,
                                  required_fields: Optional[List[str]] = None) -> Dict[str, Any]:
    """DuckDBë¥¼ ì‚¬ìš©í•œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ê²€ìƒ‰ (í¸ì˜ í•¨ìˆ˜)
    
    Args:
        file_path: SafetyKorea JSON íŒŒì¼ ê²½ë¡œ
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        search_field: ê²€ìƒ‰ í•„ë“œ
        limit: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜  
        offset: ê²°ê³¼ ì‹œì‘ ìœ„ì¹˜
        filters: ì¶”ê°€ í•„í„° ì¡°ê±´
        
    Returns:
        Dict: ê²€ìƒ‰ ê²°ê³¼
    """
    processor = DuckDBProcessor(
        file_path,
        category=category,
        subcategory=subcategory,
        result_type=result_type,
        required_fields=required_fields,
    )
    try:
        return await processor.search_streaming(
            keyword,
            search_field,
            limit,
            page,
            filters,
            collect_results,
            chunk_callback,
            chunk_size
        )
    finally:
        processor.close()
