#!/usr/bin/env python3
"""
ê³ ë„í™”ëœ í•„ë“œ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- DataA, DataB, DataC ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì§€ì›
- field_settings.json vs ì‹¤ì œ parquet íŒŒì¼ í•„ë“œ ë¹„êµ
- ëˆ„ë½ëœ í•„ë“œ ë° ì¶”ê°€ í•„ë“œ ê°ì§€
- ìƒì„¸í•œ ê°­ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
"""

import json
import os
from pathlib import Path
from collections import defaultdict

# í•„ë“œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•œ DuckDB ì‚¬ìš© (pyarrow ëŒ€ì‹ )
try:
    import duckdb
    DUCKDB_AVAILABLE = True
except ImportError:
    DUCKDB_AVAILABLE = False
    print("âš ï¸ DuckDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ íŒŒì¼ ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

def extract_field_settings():
    """field_settings.jsonì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ í•„ë“œ ì •ë³´ ì¶”ì¶œ"""
    
    field_settings_path = Path("/Users/jws/cursorPrj/DataPagePrj/Project/config/field_settings.json")
    
    with open(field_settings_path, 'r', encoding='utf-8') as f:
        field_settings = json.load(f)
    
    # íŒŒì¼ëª… ë§¤í•‘ (ëª¨ë“  ì¹´í…Œê³ ë¦¬)
    file_mapping = {
        # DataA Base Files
        "safetykorea": "1_safetykorea_flattened.parquet",
        "wadiz-makers": "2_wadiz_flattened.parquet",
        "efficiency-rating": "3_efficiency_flattened.parquet",
        "high-efficiency": "4_high_efficiency_flattened.parquet",
        "standby-power": "5_standby_power_flattened.parquet",
        "approval": "6_approval_flattened.parquet",
        "declaration-details": "7_declare_flattened.parquet",
        "kwtc": "8_kwtc_flattened.parquet",
        "recall": "9_recall_flattened.parquet",
        "safetykoreachild": "10_safetykoreachild_flattened.parquet",
        "rra-cert": "11_rra_cert_flattened.parquet",
        "rra-self-cert": "12_rra_self_cert_flattened.parquet",
        "safetykoreahome": "13_safetykoreahome_flattened.parquet"
    }
    
    results = []
    
    # DataA ì²˜ë¦¬
    if "dataA" in field_settings:
        for subcategory, config in field_settings["dataA"].items():
            if subcategory in file_mapping:
                result = process_category_config(
                    "DataA", subcategory, config, file_mapping[subcategory]
                )
                results.append(result)
    
    # DataB ì²˜ë¦¬
    if "dataB" in field_settings:
        for subcategory, config in field_settings["dataB"].items():
            if subcategory in file_mapping:
                result = process_category_config(
                    "DataB", subcategory, config, file_mapping[subcategory]
                )
                results.append(result)
    
    # DataC ì²˜ë¦¬
    if "dataC" in field_settings:
        for subcategory, config in field_settings["dataC"].items():
            if subcategory in file_mapping:
                # DataC Success íŒŒì¼
                success_file = f"enhanced/success/{file_mapping[subcategory].replace('.parquet', '_success.parquet')}"
                result_success = process_category_config(
                    "DataC_Success", subcategory, config, success_file
                )
                results.append(result_success)
                
                # DataC Failed íŒŒì¼
                failed_file = f"enhanced/failed/{file_mapping[subcategory].replace('.parquet', '_failed.parquet')}"
                result_failed = process_category_config(
                    "DataC_Failed", subcategory, config, failed_file
                )
                results.append(result_failed)
    
    return results

def process_category_config(category, subcategory, config, file_path):
    """ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ ì„¤ì • ì²˜ë¦¬"""
    
    category_name = config.get("ì¹´í…Œê³ ë¦¬ëª…", config.get("category_info", {}).get("display_name", subcategory))
    
    # display_fieldsì—ì„œ ì»¬ëŸ¼ëª… ì¶”ì¶œ
    display_fields = config.get("display_fields", [])
    display_columns = []
    for field in display_fields:
        if isinstance(field, dict) and "field" in field:
            display_columns.append({
                "field": field["field"],
                "name": field.get("name", field["field"]),
                "width": field.get("width", "auto")
            })
    
    # download_fieldsì—ì„œ ë‹¤ìš´ë¡œë“œ ì»¬ëŸ¼ ì¶”ì¶œ
    download_fields = config.get("download_fields", [])
    
    # search_fieldsì—ì„œ ê²€ìƒ‰ í•„ë“œ ì¶”ì¶œ
    search_fields = config.get("search_fields", [])
    search_field_names = [sf.get("field") for sf in search_fields if isinstance(sf, dict) and sf.get("field") != "all"]
    
    return {
        "category": category,
        "subcategory": subcategory,
        "file_path": file_path,
        "category_name": category_name,
        "display_columns": display_columns,
        "download_fields": download_fields,
        "search_fields": search_field_names,
        "total_display_fields": len(display_columns),
        "total_download_fields": len(download_fields),
        "total_search_fields": len(search_field_names)
    }

def get_actual_parquet_columns(file_path):
    """ì‹¤ì œ parquet íŒŒì¼ì˜ ì»¬ëŸ¼ëª… ì¡°íšŒ (DuckDB ì‚¬ìš©)"""
    
    if not DUCKDB_AVAILABLE:
        return None, "DuckDB not available"
    
    full_path = Path("/Users/jws/cursorPrj/DataPagePrj/Project/parquet") / file_path
    
    if not full_path.exists():
        return None, f"File not found: {full_path}"
    
    try:
        # DuckDBë¡œ íŒŒì¼ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
        conn = duckdb.connect()
        result = conn.execute(f"DESCRIBE SELECT * FROM read_parquet('{full_path}') LIMIT 1").fetchall()
        conn.close()
        
        # ì»¬ëŸ¼ëª…ë§Œ ì¶”ì¶œ
        columns = [row[0] for row in result]
        return columns, None
        
    except Exception as e:
        return None, str(e)

def analyze_field_gaps(settings_results):
    """ì„¤ì •ê³¼ ì‹¤ì œ íŒŒì¼ ê°„ì˜ í•„ë“œ ê°­ ë¶„ì„"""
    
    gap_analysis = []
    
    for result in settings_results:
        file_path = result['file_path']
        
        # ì‹¤ì œ íŒŒì¼ì˜ ì»¬ëŸ¼ ì¡°íšŒ
        actual_columns, error = get_actual_parquet_columns(file_path)
        
        if error:
            gap_info = {
                **result,
                "actual_columns": None,
                "actual_column_count": 0,
                "error": error,
                "missing_display_fields": [],
                "missing_download_fields": [],
                "extra_columns": [],
                "coverage_ratio": 0.0
            }
        else:
            # ì„¤ì •ëœ í•„ë“œë“¤
            configured_display = set([col['field'] for col in result['display_columns']])
            configured_download = set(result['download_fields'])
            configured_search = set(result['search_fields'])
            all_configured = configured_display | configured_download | configured_search
            
            # ì‹¤ì œ íŒŒì¼ ì»¬ëŸ¼
            actual_set = set(actual_columns)
            
            # ê°­ ë¶„ì„
            missing_display = configured_display - actual_set
            missing_download = configured_download - actual_set
            missing_search = configured_search - actual_set
            extra_columns = actual_set - all_configured
            
            # ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
            coverage_ratio = len(all_configured & actual_set) / len(actual_set) if actual_set else 0
            
            gap_info = {
                **result,
                "actual_columns": actual_columns,
                "actual_column_count": len(actual_columns),
                "error": None,
                "missing_display_fields": list(missing_display),
                "missing_download_fields": list(missing_download),
                "missing_search_fields": list(missing_search),
                "extra_columns": list(extra_columns),
                "coverage_ratio": coverage_ratio,
                "total_configured_fields": len(all_configured),
                "matched_fields": len(all_configured & actual_set)
            }
        
        gap_analysis.append(gap_info)
    
    return gap_analysis

def write_comprehensive_field_report(settings_results, gap_analysis):
    """ì¢…í•©ì ì¸ í•„ë“œ ë¶„ì„ ë³´ê³ ì„œ ì‘ì„±"""
    
    output_path = "/Users/jws/cursorPrj/DataPagePrj/Project/enhanced_field_analysis_report.txt"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("=" * 120 + "\n")
        f.write("ğŸ”¬ ì¢…í•© í•„ë“œ ë¶„ì„ ë³´ê³ ì„œ (DataA + DataB + DataC í¬í•¨)\n")
        f.write("field_settings.json vs ì‹¤ì œ parquet íŒŒì¼ ë¹„êµ\n")
        f.write("=" * 120 + "\n\n")
        
        # 1. ì „ì²´ í˜„í™© ìš”ì•½
        write_overall_summary(f, settings_results, gap_analysis)
        
        # 2. ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„
        write_category_analysis(f, gap_analysis)
        
        # 3. ê°­ ë¶„ì„ ìš”ì•½
        write_gap_summary(f, gap_analysis)
        
        # 4. ëˆ„ë½ í•„ë“œ ìƒì„¸ ëª©ë¡
        write_missing_fields_detail(f, gap_analysis)
        
        # 5. ì¶”ê°€ í•„ë“œ ëª©ë¡ (ì„¤ì •ì— ì—†ëŠ” ì‹¤ì œ í•„ë“œ)
        write_extra_fields_detail(f, gap_analysis)
        
        # 6. ê¶Œì¥ì‚¬í•­
        write_recommendations(f, gap_analysis)
    
    print(f"\nğŸ“„ ì¢…í•© í•„ë“œ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {output_path}")

def write_overall_summary(f, settings_results, gap_analysis):
    """ì „ì²´ í˜„í™© ìš”ì•½ ì‘ì„±"""
    
    f.write("ğŸ“Š ì „ì²´ í˜„í™© ìš”ì•½\n")
    f.write("-" * 60 + "\n")
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    categories = defaultdict(int)
    for result in settings_results:
        categories[result['category']] += 1
    
    total_files = len(settings_results)
    analyzable_files = len([g for g in gap_analysis if g['error'] is None])
    error_files = total_files - analyzable_files
    
    f.write(f"ì´ ì„¤ì • íŒŒì¼: {total_files}ê°œ\n")
    for category, count in sorted(categories.items()):
        f.write(f"  - {category}: {count}ê°œ\n")
    
    f.write(f"\në¶„ì„ ê°€ëŠ¥í•œ íŒŒì¼: {analyzable_files}ê°œ\n")
    f.write(f"ë¶„ì„ ë¶ˆê°€ íŒŒì¼: {error_files}ê°œ\n")
    
    if analyzable_files > 0:
        # ì „ì²´ í†µê³„
        total_display = sum([g['total_display_fields'] for g in gap_analysis if g['error'] is None])
        total_download = sum([g['total_download_fields'] for g in gap_analysis if g['error'] is None])
        total_actual = sum([g['actual_column_count'] for g in gap_analysis if g['error'] is None])
        avg_coverage = sum([g['coverage_ratio'] for g in gap_analysis if g['error'] is None]) / analyzable_files
        
        f.write(f"\nğŸ“ˆ í•„ë“œ í†µê³„:\n")
        f.write(f"  ì „ì²´ ì„¤ì •ëœ í‘œì‹œ í•„ë“œ: {total_display}ê°œ\n")
        f.write(f"  ì „ì²´ ì„¤ì •ëœ ë‹¤ìš´ë¡œë“œ í•„ë“œ: {total_download}ê°œ\n")
        f.write(f"  ì „ì²´ ì‹¤ì œ íŒŒì¼ ì»¬ëŸ¼: {total_actual}ê°œ\n")
        f.write(f"  í‰ê·  ì»¤ë²„ë¦¬ì§€: {avg_coverage*100:.1f}%\n")
    
    f.write("\n")

def write_category_analysis(f, gap_analysis):
    """ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„ ì‘ì„±"""
    
    categories = ['DataA', 'DataB', 'DataC_Success', 'DataC_Failed']
    
    for category in categories:
        category_results = [g for g in gap_analysis if g['category'] == category]
        
        if not category_results:
            continue
        
        f.write("\n" + "=" * 120 + "\n")
        f.write(f"ğŸ“‚ {category} ì¹´í…Œê³ ë¦¬ ìƒì„¸ ë¶„ì„\n")
        f.write("-" * 60 + "\n")
        
        for result in category_results:
            f.write(f"\nğŸ—‚ï¸ {result['subcategory']} ({result['category_name']})\n")
            f.write(f"   íŒŒì¼: {result['file_path']}\n")
            
            if result['error']:
                f.write(f"   âŒ ë¶„ì„ ì‹¤íŒ¨: {result['error']}\n")
                continue
            
            f.write(f"   ğŸ“Š í‘œì‹œ í•„ë“œ: {result['total_display_fields']}ê°œ\n")
            f.write(f"   ğŸ“¥ ë‹¤ìš´ë¡œë“œ í•„ë“œ: {result['total_download_fields']}ê°œ\n")
            f.write(f"   ğŸ” ê²€ìƒ‰ í•„ë“œ: {result['total_search_fields']}ê°œ\n")
            f.write(f"   ğŸ“ ì‹¤ì œ ì»¬ëŸ¼: {result['actual_column_count']}ê°œ\n")
            f.write(f"   ğŸ“ˆ ì»¤ë²„ë¦¬ì§€: {result['coverage_ratio']*100:.1f}%\n")
            
            # ê°­ ì •ë³´
            if result['missing_display_fields']:
                f.write(f"   âš ï¸ ëˆ„ë½ëœ í‘œì‹œ í•„ë“œ: {len(result['missing_display_fields'])}ê°œ\n")
            
            if result['missing_download_fields']:
                f.write(f"   âš ï¸ ëˆ„ë½ëœ ë‹¤ìš´ë¡œë“œ í•„ë“œ: {len(result['missing_download_fields'])}ê°œ\n")
            
            if result['extra_columns']:
                f.write(f"   â• ì¶”ê°€ ì»¬ëŸ¼: {len(result['extra_columns'])}ê°œ\n")

def write_gap_summary(f, gap_analysis):
    """ê°­ ë¶„ì„ ìš”ì•½ ì‘ì„±"""
    
    f.write("\n" + "=" * 120 + "\n")
    f.write("ğŸš¨ ê°­ ë¶„ì„ ìš”ì•½\n")
    f.write("-" * 60 + "\n")
    
    analyzable = [g for g in gap_analysis if g['error'] is None]
    
    if not analyzable:
        f.write("ë¶„ì„ ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\n")
        return
    
    # ë¬¸ì œê°€ ìˆëŠ” íŒŒì¼ë“¤
    files_with_missing_display = [g for g in analyzable if g['missing_display_fields']]
    files_with_missing_download = [g for g in analyzable if g['missing_download_fields']]
    files_with_extra = [g for g in analyzable if g['extra_columns']]
    low_coverage_files = [g for g in analyzable if g['coverage_ratio'] < 0.8]
    
    f.write(f"ğŸ“Š ë¬¸ì œ í˜„í™©:\n")
    f.write(f"  ëˆ„ë½ëœ í‘œì‹œ í•„ë“œê°€ ìˆëŠ” íŒŒì¼: {len(files_with_missing_display)}ê°œ\n")
    f.write(f"  ëˆ„ë½ëœ ë‹¤ìš´ë¡œë“œ í•„ë“œê°€ ìˆëŠ” íŒŒì¼: {len(files_with_missing_download)}ê°œ\n")
    f.write(f"  ì¶”ê°€ ì»¬ëŸ¼ì´ ìˆëŠ” íŒŒì¼: {len(files_with_extra)}ê°œ\n")
    f.write(f"  ë‚®ì€ ì»¤ë²„ë¦¬ì§€(<80%) íŒŒì¼: {len(low_coverage_files)}ê°œ\n")
    
    # TOP ë¬¸ì œ íŒŒì¼ë“¤
    f.write(f"\nğŸ”¥ ì£¼ìš” ë¬¸ì œ íŒŒì¼ TOP 5:\n")
    problem_files = sorted(analyzable, 
                          key=lambda x: len(x['missing_display_fields']) + len(x['missing_download_fields']), 
                          reverse=True)[:5]
    
    for i, pf in enumerate(problem_files, 1):
        total_missing = len(pf['missing_display_fields']) + len(pf['missing_download_fields'])
        if total_missing > 0:
            f.write(f"  {i}. {pf['file_path']} - {total_missing}ê°œ í•„ë“œ ëˆ„ë½\n")

def write_missing_fields_detail(f, gap_analysis):
    """ëˆ„ë½ í•„ë“œ ìƒì„¸ ëª©ë¡ ì‘ì„±"""
    
    f.write("\n" + "=" * 120 + "\n")
    f.write("âŒ ëˆ„ë½ëœ í•„ë“œ ìƒì„¸ ëª©ë¡\n")
    f.write("-" * 60 + "\n")
    
    analyzable = [g for g in gap_analysis if g['error'] is None]
    files_with_missing = [g for g in analyzable if g['missing_display_fields'] or g['missing_download_fields']]
    
    if not files_with_missing:
        f.write("ëˆ„ë½ëœ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì„¤ì •ì´ ì˜¬ë°”ë¦…ë‹ˆë‹¤! âœ…\n")
        return
    
    for result in files_with_missing:
        f.write(f"\nğŸ“ {result['file_path']} ({result['category']})\n")
        
        if result['missing_display_fields']:
            f.write(f"  ğŸ”¸ ëˆ„ë½ëœ í‘œì‹œ í•„ë“œ ({len(result['missing_display_fields'])}ê°œ):\n")
            for field in sorted(result['missing_display_fields']):
                f.write(f"    - {field}\n")
        
        if result['missing_download_fields']:
            f.write(f"  ğŸ”¸ ëˆ„ë½ëœ ë‹¤ìš´ë¡œë“œ í•„ë“œ ({len(result['missing_download_fields'])}ê°œ):\n")
            for field in sorted(result['missing_download_fields']):
                f.write(f"    - {field}\n")

def write_extra_fields_detail(f, gap_analysis):
    """ì¶”ê°€ í•„ë“œ ìƒì„¸ ëª©ë¡ ì‘ì„±"""
    
    f.write("\n" + "=" * 120 + "\n")
    f.write("â• ì¶”ê°€ í•„ë“œ ëª©ë¡ (ì„¤ì •ì— ì—†ëŠ” ì‹¤ì œ ì»¬ëŸ¼)\n")
    f.write("-" * 60 + "\n")
    
    analyzable = [g for g in gap_analysis if g['error'] is None]
    files_with_extra = [g for g in analyzable if g['extra_columns']]
    
    if not files_with_extra:
        f.write("ëª¨ë“  ì‹¤ì œ ì»¬ëŸ¼ì´ ì„¤ì •ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! âœ…\n")
        return
    
    for result in files_with_extra:
        f.write(f"\nğŸ“ {result['file_path']} ({result['category']})\n")
        f.write(f"  ğŸ”¸ ì¶”ê°€ ì»¬ëŸ¼ ({len(result['extra_columns'])}ê°œ):\n")
        for field in sorted(result['extra_columns']):
            f.write(f"    - {field}\n")

def write_recommendations(f, gap_analysis):
    """ê¶Œì¥ì‚¬í•­ ì‘ì„±"""
    
    f.write("\n" + "=" * 120 + "\n")
    f.write("ğŸ’¡ ê¶Œì¥ì‚¬í•­ ë° ê°œì„  ë°©ì•ˆ\n")
    f.write("-" * 60 + "\n")
    
    analyzable = [g for g in gap_analysis if g['error'] is None]
    
    f.write("ğŸ¯ ì¦‰ì‹œ í•´ê²° í•„ìš”:\n")
    
    # ëˆ„ë½ í•„ë“œ í•´ê²°
    missing_count = len([g for g in analyzable if g['missing_display_fields'] or g['missing_download_fields']])
    if missing_count > 0:
        f.write(f"  1. {missing_count}ê°œ íŒŒì¼ì˜ ëˆ„ë½ëœ í•„ë“œ ì„¤ì • ì¶”ê°€\n")
        f.write("     - field_settings.jsonì— ëˆ„ë½ëœ í•„ë“œë“¤ì„ ì¶”ê°€í•˜ê±°ë‚˜\n")
        f.write("     - ì‹¤ì œ íŒŒì¼ì—ì„œ í•´ë‹¹ í•„ë“œê°€ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸\n")
    
    # ì¶”ê°€ í•„ë“œ í™œìš©
    extra_count = len([g for g in analyzable if g['extra_columns']])
    if extra_count > 0:
        f.write(f"  2. {extra_count}ê°œ íŒŒì¼ì˜ ì¶”ê°€ ì»¬ëŸ¼ í™œìš© ê²€í† \n")
        f.write("     - ìœ ìš©í•œ ì¶”ê°€ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ display_fieldsë‚˜ download_fieldsì— ì¶”ê°€\n")
        f.write("     - ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ì´ë¼ë©´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ì œê±° ê²€í† \n")
    
    f.write("\nğŸ”„ ì§€ì†ì ì¸ ê°œì„ :\n")
    f.write("  3. ìë™ í•„ë“œ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬ì¶•\n")
    f.write("     - CI/CD íŒŒì´í”„ë¼ì¸ì— í•„ë“œ ì¼ì¹˜ì„± ê²€ì‚¬ ì¶”ê°€\n")
    f.write("     - ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€ ì‹œ ìë™ ì•Œë¦¼\n")
    
    f.write("  4. DataC Success/Failed íŒŒì¼ ë³„ë„ ì„¤ì • ê³ ë ¤\n")
    f.write("     - Enhanced í•„ë“œì— ëŒ€í•œ ë³„ë„ UI í‘œì‹œ ë°©ì•ˆ\n")
    f.write("     - Successì™€ Failed ë°ì´í„°ì˜ ì°¨ë³„í™”ëœ í™œìš©\n")
    
    f.write("\nğŸ“Š ì„±ëŠ¥ ìµœì í™”:\n")
    f.write("  5. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°ë¡œ íŒŒì¼ í¬ê¸° ìµœì í™”\n")
    f.write("  6. ìì£¼ ì‚¬ìš©ë˜ëŠ” í•„ë“œ ìš°ì„ ìˆœìœ„ ì¡°ì •\n")
    f.write("  7. ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì¸ë±ì‹± ì»¬ëŸ¼ ì„ ë³„\n")

if __name__ == "__main__":
    print("ğŸš€ ê³ ë„í™”ëœ í•„ë“œ ë¶„ì„ ì‹œì‘")
    print("DataA + DataB + DataC í¬í•¨, ì‹¤ì œ íŒŒì¼ê³¼ì˜ ê°­ ë¶„ì„")
    
    # 1. field_settings.jsonì—ì„œ ì„¤ì • ì •ë³´ ì¶”ì¶œ
    print("\nğŸ“– field_settings.json ë¶„ì„ ì¤‘...")
    settings_results = extract_field_settings()
    print(f"âœ… {len(settings_results)}ê°œ ì„¤ì • ì¶”ì¶œ ì™„ë£Œ")
    
    # 2. ì‹¤ì œ íŒŒì¼ê³¼ì˜ ê°­ ë¶„ì„
    print("\nğŸ” ì‹¤ì œ parquet íŒŒì¼ê³¼ì˜ ê°­ ë¶„ì„ ì¤‘...")
    gap_analysis = analyze_field_gaps(settings_results)
    
    analyzable_count = len([g for g in gap_analysis if g['error'] is None])
    error_count = len(gap_analysis) - analyzable_count
    print(f"âœ… ë¶„ì„ ì™„ë£Œ: {analyzable_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
    
    # 3. ì¢…í•© ë³´ê³ ì„œ ì‘ì„±
    print("\nğŸ“ ì¢…í•© ë³´ê³ ì„œ ì‘ì„± ì¤‘...")
    write_comprehensive_field_report(settings_results, gap_analysis)
    
    print("\nğŸ‰ ê³ ë„í™”ëœ í•„ë“œ ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“„ ë³´ê³ ì„œ íŒŒì¼: enhanced_field_analysis_report.txt")
